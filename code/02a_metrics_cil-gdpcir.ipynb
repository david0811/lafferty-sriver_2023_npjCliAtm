{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61813786-4f8a-457c-8e4d-e66e517651f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: may need to run the following command in terminal to install regionmask:\n",
    "# mamba install -c conda-forge regionmask cartopy pygeos\n",
    "# mamba install -c anaconda cryptography==38.0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35e6a91-5f45-445a-8e78-53fa1dbfa401",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "### TO RUN ON MICROSOFT PLANETARY COMPUTER ####\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8b0cb3-6059-43c0-87ee-cb343c13e526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import getpass\n",
    "import io\n",
    "\n",
    "import azure.storage.blob\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import planetary_computer\n",
    "import pystac\n",
    "import pystac_client\n",
    "import requests\n",
    "import xarray as xr\n",
    "import zarr\n",
    "\n",
    "# import regionmask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230b7c1-1a5c-44ef-be0b-92a672666307",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9e1ed9-efe8-4743-b2f5-7e56b75fcca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# Azure blob storage\n",
    "######################\n",
    "# connection string (from azure web login, select your storage account, then \"Access keys\")\n",
    "connection_string = getpass.getpass()\n",
    "\n",
    "\n",
    "# format storage\n",
    "container_client = azure.storage.blob.ContainerClient.from_connection_string(\n",
    "    connection_string, container_name=\"mpctransfer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b16c220d-323f-4b12-954f-acfe608b96a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Models\n",
    "###################\n",
    "from utils import cil_ssp_dict\n",
    "\n",
    "models = list(cil_ssp_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27086cd8-9a0c-49bd-871c-8150e4831f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Data access\n",
    "#################\n",
    "\n",
    "# Complete catalog\n",
    "catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "\n",
    "\n",
    "# function to grab variables and SSPs for singe model\n",
    "def grab_model(model_id, vars_to_grab):\n",
    "    # Search across all licences in CIL-GDPCIR\n",
    "    search = catalog.search(\n",
    "        collections=[\"cil-gdpcir-cc0\", \"cil-gdpcir-cc-by\", \"cil-gdpcir-cc-by-sa\"],\n",
    "        query={\"cmip6:source_id\": {\"eq\": model_id}, \"cmip6:experiment_id\": {\"neq\": \"historical\"}},  # omit historical\n",
    "    )\n",
    "    ensemble = search.get_all_items()\n",
    "\n",
    "    # grab all into one dataset\n",
    "    ds_ssp = []\n",
    "\n",
    "    for item in ensemble:\n",
    "        signed = planetary_computer.sign(item)\n",
    "        ds_vars = []\n",
    "        for variable_id in vars_to_grab:\n",
    "            asset = signed.assets[variable_id]\n",
    "            ds_tmp = xr.open_dataset(asset.href, **asset.extra_fields[\"xarray:open_kwargs\"])\n",
    "            ds_tmp = ds_tmp.assign_coords(ssp=ds_tmp.attrs[\"experiment_id\"])\n",
    "            ds_vars.append(ds_tmp)\n",
    "        ds_ssp.append(xr.merge(ds_vars))\n",
    "\n",
    "    ds_out = xr.concat(ds_ssp, dim=\"ssp\")\n",
    "\n",
    "    return ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d05e9339-31c0-4a3c-bc93-81708fad6d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for longest consecutive spell if needed\n",
    "def n_longest_consecutive(ds, dim=\"time\"):\n",
    "    ds = ds.cumsum(dim=dim) - ds.cumsum(dim=dim).where(ds == 0).ffill(dim=dim).fillna(0)\n",
    "    return ds.max(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d575492-be02-48a1-b258-30d854165429",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Set paths\n",
    "# UPDATE THIS FOR REPRODUCTION\n",
    "###############################\n",
    "quantile_path = \"/home/jovyan/PlanetaryComputerExamples/DownscalingComparison/lafferty-sriver_inprep_tbd/data/quantiles/\"  # location of GMFD, ERA5 quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32e5f78d-502d-4d43-bd80-01802ed831c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pccompute.westeurope.cloudapp.azure.com/compute/services/dask-gateway/clusters/prod.0d55d66efa764a92a6ae63f74f2cff69/status\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# Dask\n",
    "#########\n",
    "import dask_gateway\n",
    "\n",
    "gateway = dask_gateway.Gateway()\n",
    "\n",
    "# cluster options\n",
    "cluster_options = gateway.cluster_options()\n",
    "cluster_options[\"worker_memory\"] = 30\n",
    "cluster_options[\"worker_cores\"] = 1\n",
    "\n",
    "# start cluster\n",
    "cluster = gateway.new_cluster(cluster_options)\n",
    "client = cluster.get_client()\n",
    "cluster.scale(45)\n",
    "\n",
    "# dashboard link\n",
    "print(cluster.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8acc31e-89a3-4d4b-b51e-4e12d5a8e6d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Simple metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c0a62c-d15c-490c-ba23-1f2ac34842a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Annual averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf3ec95-02ea-434e-ae27-cf1555f61f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# loop through models: RUNTIME IS AROUND 15 MINS PER MODEL WITH 40 DASK WORKERS\n",
    "for model in models:\n",
    "    # load data (lazy)\n",
    "    ds = grab_model(model, [\"tasmin\", \"tasmax\", \"pr\"])\n",
    "\n",
    "    # compute\n",
    "    ds[\"tas\"] = (ds[\"tasmax\"] + ds[\"tasmin\"]) / 2.0\n",
    "    ds_final = ds.resample(time=\"1Y\").mean()\n",
    "\n",
    "    # unit conversions\n",
    "    ds_final[\"tas\"] = ds_final[\"tas\"] - 273.15  # K -> C\n",
    "    ds_final[\"tasmax\"] = ds_final[\"tasmax\"] - 273.15  # K -> C\n",
    "    ds_final[\"tasmin\"] = ds_final[\"tasmin\"] - 273.15  # K -> C\n",
    "\n",
    "    # storage options\n",
    "    ds_final = ds_final.chunk({\"ssp\": 1, \"time\": 10, \"lat\": 720, \"lon\": 1440})\n",
    "\n",
    "    compressor = zarr.Blosc(cname=\"zstd\", clevel=3)\n",
    "    encoding = {vname: {\"compressor\": compressor} for vname in ds_final.data_vars}\n",
    "\n",
    "    azure_prefix = \"cil-gdpcir/avg/\" + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode=\"w\")\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb97d939-4949-4517-bb29-e3577dd27ff9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1-day max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a70a36-32c4-4e23-b557-64609833edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# loop through models: RUNTIME IS AROUND 15 MINS PER MODEL WITH 40 DASK WORKERS\n",
    "for model in models:\n",
    "    # load data (lazy)\n",
    "    ds = grab_model(model, [\"tasmin\", \"tasmax\", \"pr\"])\n",
    "\n",
    "    # compute\n",
    "    ds[\"tas\"] = (ds[\"tasmax\"] + ds[\"tasmin\"]) / 2.0\n",
    "    ds_final = ds.resample(time=\"1Y\").max()\n",
    "\n",
    "    # unit conversions\n",
    "    ds_final[\"tas\"] = ds_final[\"tas\"] - 273.15  # K -> C\n",
    "    ds_final[\"tasmax\"] = ds_final[\"tasmax\"] - 273.15  # K -> C\n",
    "    ds_final[\"tasmin\"] = ds_final[\"tasmin\"] - 273.15  # K -> C\n",
    "\n",
    "    # storage options\n",
    "    ds_final = ds_final.chunk({\"ssp\": 1, \"time\": 10, \"lat\": 720, \"lon\": 1440})\n",
    "\n",
    "    compressor = zarr.Blosc(cname=\"zstd\", clevel=3)\n",
    "    encoding = {vname: {\"compressor\": compressor} for vname in ds_final.data_vars}\n",
    "\n",
    "    azure_prefix = \"cil-gdpcir/max/\" + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode=\"w\")\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e2edac-d9e8-41e3-8a32-f4a4ebf5ffec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 5-day max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17dc9f7b-6413-45aa-9632-f678ff931fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xclim\n",
    "\n",
    "xclim.set_options(cf_compliance=\"log\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5781fbee-6740-492d-a963-1085fc928c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# loop through models: RUNTIME IS AROUND 20 MINS PER MODEL WITH 40 DASK WORKERS\n",
    "for model in models:\n",
    "    # load data (lazy)\n",
    "    ds = grab_model(model, [\"tasmin\", \"tasmax\", \"pr\"])\n",
    "    ds[\"tas\"] = (ds[\"tasmax\"] + ds[\"tasmin\"]) / 2.0\n",
    "\n",
    "    # Compute\n",
    "    ds_RX5day = xclim.indicators.icclim.RX5day(ds=ds[[\"pr\"]], freq=\"Y\")\n",
    "\n",
    "    ds_temp5day = ds[[\"tas\", \"tasmin\", \"tasmax\"]].rolling(time=5).mean().resample(time=\"1Y\").max()\n",
    "    ds_temp5day -= 273.15  # K -> C\n",
    "\n",
    "    # Storage options\n",
    "    ds_final = xr.merge([ds_temp5day, ds_RX5day])\n",
    "    ds_final = ds_final.chunk({\"ssp\": 1, \"time\": 30, \"lat\": 720, \"lon\": 1440})\n",
    "\n",
    "    compressor = zarr.Blosc(cname=\"zstd\", clevel=3)\n",
    "    encoding = {vname: {\"compressor\": compressor} for vname in ds_final.data_vars}\n",
    "\n",
    "    azure_prefix = \"cil-gdpcir/max5d/\" + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode=\"w\")\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780643e4-79e9-4ea3-92dc-27abab924dc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Dry days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f305273-59aa-4e0f-b0d3-eb92bcd0b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# NOTE: for CIL, I believe that all days with <1mm precipitation\n",
    "# are set to zero, so the distinctions here are redundant\n",
    "# but I did not know this until after having done the calculations\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "873331c7-4a6a-4466-8377-21b69ec498e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for longest consecutive spell if needed\n",
    "def n_longest_consecutive(ds, dim=\"time\"):\n",
    "    ds = ds.cumsum(dim=dim) - ds.cumsum(dim=dim).where(ds == 0).ffill(dim=dim).fillna(0)\n",
    "    return ds.max(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49044ccf-dbcb-47f0-93b1-6ec6f660a434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-ESM1-5\n",
      "BCC-CSM2-MR\n",
      "CanESM5\n",
      "CMCC-ESM2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 23:41:11,403 - distributed.client - WARNING - Couldn't gather 1 keys, rescheduling {\"('store-map-4fdc02a2ef25c5a64311bfc931f50efb', 2, 1, 0, 0)\": ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EC-Earth3\n",
      "EC-Earth3-Veg-LR\n",
      "GFDL-ESM4\n",
      "HadGEM3-GC31-LL\n",
      "INM-CM4-8\n",
      "INM-CM5-0\n",
      "MIROC-ES2L\n",
      "MIROC6\n",
      "MPI-ESM1-2-LR\n",
      "NESM3\n",
      "NorESM2-LM\n",
      "NorESM2-MM\n",
      "UKESM1-0-LL\n",
      "CPU times: user 12min 50s, sys: 13.7 s, total: 13min 3s\n",
      "Wall time: 3h 6min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# loop through models: RUNTIME IS AROUND 10 MINS PER MODEL WITH 40 DASK WORKERS\n",
    "for model in models:\n",
    "    # load data (lazy)\n",
    "    ds = grab_model(model, [\"pr\"])\n",
    "\n",
    "    # Compute\n",
    "    # Number of dry days\n",
    "    ds_tmp_0 = (ds == 0.0).resample(time=\"1Y\").sum()  # 0mm\n",
    "    ds_tmp_1 = (ds < 1.0).resample(time=\"1Y\").sum()  # less than 1mm\n",
    "    # Longest consecutive dry day streak\n",
    "    ds_tmp_0c = (ds == 0.0).resample(time=\"1Y\").apply(n_longest_consecutive)  # 0mm longest consecutive\n",
    "    ds_tmp_1c = (ds < 1.0).resample(time=\"1Y\").apply(n_longest_consecutive)  # less than 1mm longest consecutive\n",
    "    # Merge\n",
    "    ds_final = xr.merge(\n",
    "        [\n",
    "            ds_tmp_0.rename({\"pr\": \"count_eq_0\"}),\n",
    "            ds_tmp_0c.rename({\"pr\": \"streak_eq_0\"}),\n",
    "            ds_tmp_1.rename({\"pr\": \"count_lt_1\"}),\n",
    "            ds_tmp_1c.rename({\"pr\": \"streak_lt_1\"}),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # storage options\n",
    "    ds_final = ds_final.chunk({\"ssp\": 1, \"time\": 10, \"lat\": 720, \"lon\": 1440})\n",
    "\n",
    "    compressor = zarr.Blosc(cname=\"zstd\", clevel=3)\n",
    "    encoding = {vname: {\"compressor\": compressor} for vname in ds_final.data_vars}\n",
    "\n",
    "    azure_prefix = \"cil-gdpcir/dry/\" + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode=\"w\")\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf0585-27e6-4923-9337-2eacaa5bdfa2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Less simple metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03338ede-8320-4f94-868a-b3372f78b1b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Wet days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "905e8674-d51e-4b48-ad5c-b4ebd3896dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-ESM1-5\n",
      "BCC-CSM2-MR\n",
      "CanESM5\n",
      "CMCC-ESM2\n",
      "EC-Earth3\n",
      "EC-Earth3-Veg-LR\n",
      "GFDL-ESM4\n",
      "HadGEM3-GC31-LL\n",
      "INM-CM4-8\n",
      "INM-CM5-0\n",
      "MIROC-ES2L\n",
      "MIROC6\n",
      "MPI-ESM1-2-LR\n",
      "NESM3\n",
      "NorESM2-LM\n",
      "NorESM2-MM\n",
      "UKESM1-0-LL\n",
      "CPU times: user 22min 10s, sys: 8.22 s, total: 22min 18s\n",
      "Wall time: 6h 24min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load quantiles\n",
    "ds_q_era5 = xr.open_dataset(quantile_path + \"era5_precip_quantiles_nex-cil-deepsd.nc\")\n",
    "ds_q_era5[\"lon\"] = np.where(ds_q_era5[\"lon\"] > 180, ds_q_era5[\"lon\"] - 360, ds_q_era5[\"lon\"])\n",
    "ds_q_era5 = ds_q_era5.sortby(\"lon\")\n",
    "\n",
    "ds_q_gmfd = xr.open_dataset(quantile_path + \"gmfd_precip_quantiles_nex-cil-deepsd.nc\")\n",
    "ds_q_gmfd[\"lon\"] = np.where(ds_q_gmfd[\"lon\"] > 180, ds_q_gmfd[\"lon\"] - 360, ds_q_gmfd[\"lon\"])\n",
    "ds_q_gmfd = ds_q_gmfd.sortby(\"lon\")\n",
    "\n",
    "# Loop through models: RUNTIME IS AROUND 10 MINS PER MODEL WITH 40 DASK WORKERS\n",
    "for model in models:\n",
    "    # Load data (lazy)\n",
    "    ds = grab_model(model, [\"pr\"])\n",
    "\n",
    "    ## Calculate metrics\n",
    "    var_id = \"pr\"\n",
    "    ds_tmp_out = []\n",
    "    for rp in [\"q99\", \"rp10\"]:\n",
    "        # Get above/below binary\n",
    "        ds_tmp_q_era5 = ds[var_id] > ds_q_era5[var_id + \"_\" + rp]\n",
    "        ds_tmp_q_gmfd = ds[var_id] > ds_q_gmfd[var_id + \"_\" + rp]\n",
    "\n",
    "        # Count of hot days\n",
    "        ds_tmp_q_era5_count = ds_tmp_q_era5.resample(time=\"1Y\").sum()\n",
    "        ds_tmp_out.append(xr.Dataset({var_id + \"_\" + rp + \"era5_count\": ds_tmp_q_era5_count}))\n",
    "        ds_tmp_q_gmfd_count = ds_tmp_q_gmfd.resample(time=\"1Y\").sum()\n",
    "        ds_tmp_out.append(xr.Dataset({var_id + \"_\" + rp + \"gmfd_count\": ds_tmp_q_gmfd_count}))\n",
    "\n",
    "        # Longest consecutive hot day streak\n",
    "        ds_tmp_q_era5_streak = ds_tmp_q_era5.resample(time=\"1Y\").apply(n_longest_consecutive)\n",
    "        ds_tmp_out.append(xr.Dataset({var_id + \"_\" + rp + \"era5_streak\": ds_tmp_q_era5_streak}))\n",
    "        ds_tmp_q_gmfd_streak = ds_tmp_q_gmfd.resample(time=\"1Y\").apply(n_longest_consecutive)\n",
    "        ds_tmp_out.append(xr.Dataset({var_id + \"_\" + rp + \"gmfd_streak\": ds_tmp_q_gmfd_streak}))\n",
    "\n",
    "    # Merge metrics and append\n",
    "    ds_final = xr.merge(ds_tmp_out)\n",
    "\n",
    "    # storage options\n",
    "    ds_final = ds_final.chunk({\"ssp\": 1, \"time\": 10, \"lat\": 720, \"lon\": 1440})\n",
    "\n",
    "    compressor = zarr.Blosc(cname=\"zstd\", clevel=3)\n",
    "    encoding = {vname: {\"compressor\": compressor} for vname in ds_final.data_vars}\n",
    "\n",
    "    azure_prefix = \"cil-gdpcir/wet/\" + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode=\"w\")\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10658cc1-ca2b-40e9-b52e-1bae96472af0",
   "metadata": {},
   "source": [
    "## Hot days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62574f0a-3fc3-43a6-b734-fb68844fe07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load quantiles\n",
    "ds_q_gmfd = xr.open_dataset(quantile_path + \"gmfd_temperature_quantiles_nex-cil-deepsd.nc\")\n",
    "ds_q_gmfd[\"lon\"] = np.where(ds_q_gmfd[\"lon\"] > 180, ds_q_gmfd[\"lon\"] - 360, ds_q_gmfd[\"lon\"])\n",
    "ds_q_gmfd = ds_q_gmfd.sortby(\"lon\")\n",
    "\n",
    "ds_q_era5 = xr.open_dataset(quantile_path + \"era5_temperature_quantiles_nex-cil-deepsd.nc\")\n",
    "ds_q_era5[\"lon\"] = np.where(ds_q_era5[\"lon\"] > 180, ds_q_era5[\"lon\"] - 360, ds_q_era5[\"lon\"])\n",
    "ds_q_era5 = ds_q_era5.sortby(\"lon\")\n",
    "\n",
    "# Loop through models\n",
    "for model in models:\n",
    "    # Load data (lazy)\n",
    "    ds = grab_model(model, [\"tasmin\", \"tasmax\"])\n",
    "    ds[\"tas\"] = (ds[\"tasmax\"] + ds[\"tasmin\"]) / 2.0\n",
    "    ds -= 273.15  # K -> C\n",
    "\n",
    "    # Calculate metrics\n",
    "    ds_tmp_final = []\n",
    "    for var_id in [\"tasmin\", \"tasmax\", \"tas\"]:\n",
    "        ds_tmp_out = []\n",
    "        for rp in [\"q99\", \"rp10\"]:\n",
    "            # Get above/below binary\n",
    "            ds_tmp_q_era5 = ds[var_id] > ds_q_era5[var_id + \"_\" + rp]\n",
    "            ds_tmp_q_gmfd = ds[var_id] > ds_q_gmfd[var_id + \"_\" + rp]\n",
    "\n",
    "            # Count of hot days\n",
    "            ds_tmp_q_era5_count = ds_tmp_q_era5.resample(time=\"1Y\").sum()\n",
    "            ds_tmp_out.append(xr.Dataset({var_id + \"_\" + rp + \"era5_count\": ds_tmp_q_era5_count}))\n",
    "            ds_tmp_q_gmfd_count = ds_tmp_q_gmfd.resample(time=\"1Y\").sum()\n",
    "            ds_tmp_out.append(xr.Dataset({var_id + \"_\" + rp + \"gmfd_count\": ds_tmp_q_gmfd_count}))\n",
    "\n",
    "            # Longest consecutive hot day streak\n",
    "            ds_tmp_q_era5_streak = ds_tmp_q_era5.resample(time=\"1Y\").apply(n_longest_consecutive)\n",
    "            ds_tmp_out.append(xr.Dataset({var_id + \"_\" + rp + \"era5_streak\": ds_tmp_q_era5_streak}))\n",
    "            ds_tmp_q_gmfd_streak = ds_tmp_q_gmfd.resample(time=\"1Y\").apply(n_longest_consecutive)\n",
    "            ds_tmp_out.append(xr.Dataset({var_id + \"_\" + rp + \"gmfd_streak\": ds_tmp_q_gmfd_streak}))\n",
    "\n",
    "        # Merge RPs and append\n",
    "        ds_out = xr.merge(ds_tmp_out)\n",
    "        ds_tmp_final.append(ds_out)\n",
    "\n",
    "    # Merge variables\n",
    "    ds_final = xr.merge(ds_tmp_final)\n",
    "\n",
    "    # storage options\n",
    "    ds_final = ds_final.chunk({\"ssp\": 1, \"time\": 20, \"lat\": 600, \"lon\": 1440})\n",
    "\n",
    "    compressor = zarr.Blosc(cname=\"zstd\", clevel=3)\n",
    "    encoding = {vname: {\"compressor\": compressor} for vname in ds_final.data_vars}\n",
    "\n",
    "    azure_prefix = \"cil-gdpcir/hot/\" + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode=\"w\")\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb0ae49-c9f1-41b5-9883-1bc96c3a19c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Multivariate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f710c-bb00-4a34-96c6-398f3f4e16a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hot and dry days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb364641-5529-4819-ad8d-15f97c5a8d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCC-CSM2-MR\n",
      "CanESM5\n",
      "CMCC-ESM2\n",
      "EC-Earth3\n",
      "EC-Earth3-Veg-LR\n",
      "GFDL-ESM4\n",
      "HadGEM3-GC31-LL\n",
      "INM-CM4-8\n",
      "INM-CM5-0\n",
      "MIROC-ES2L\n",
      "MIROC6\n",
      "MPI-ESM1-2-LR\n",
      "NESM3\n",
      "NorESM2-LM\n",
      "NorESM2-MM\n",
      "UKESM1-0-LL\n",
      "CPU times: user 21min 30s, sys: 11.9 s, total: 21min 42s\n",
      "Wall time: 6h 36min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load quantiles\n",
    "ds_q_gmfd = xr.open_dataset(quantile_path + \"gmfd_temperature_quantiles_nex-cil-deepsd.nc\")\n",
    "ds_q_gmfd[\"lon\"] = np.where(ds_q_gmfd[\"lon\"] > 180, ds_q_gmfd[\"lon\"] - 360, ds_q_gmfd[\"lon\"])\n",
    "ds_q_gmfd = ds_q_gmfd.sortby(\"lon\")\n",
    "\n",
    "ds_q_era5 = xr.open_dataset(quantile_path + \"era5_temperature_quantiles_nex-cil-deepsd.nc\")\n",
    "ds_q_era5[\"lon\"] = np.where(ds_q_era5[\"lon\"] > 180, ds_q_era5[\"lon\"] - 360, ds_q_era5[\"lon\"])\n",
    "ds_q_era5 = ds_q_era5.sortby(\"lon\")\n",
    "\n",
    "# Loop through models: RUNTIME IS AROUND 20 MINS PER MODEL WITH 50 DASK WORKERS\n",
    "for model in models[1:]:\n",
    "    # Load data (lazy)\n",
    "    ds = grab_model(model, [\"tasmax\", \"pr\"])\n",
    "    ds[\"tasmax\"] = ds[\"tasmax\"] - 273.15  # K -> C\n",
    "\n",
    "    ## Calculate metrics\n",
    "    ds_tmp_out = []\n",
    "    for rp in [\"q99\", \"rp10\"]:\n",
    "        # Get above/below binary\n",
    "        ds_tmp_q_gmfd = (ds[\"tasmax\"] > ds_q_gmfd[\"tasmax_\" + rp]) & (ds[\"pr\"] < 1.0)\n",
    "        ds_tmp_q_era5 = (ds[\"tasmax\"] > ds_q_era5[\"tasmax_\" + rp]) & (ds[\"pr\"] < 1.0)\n",
    "\n",
    "        # Count of hot+dry days\n",
    "        ds_tmp_q_era5_count = ds_tmp_q_era5.resample(time=\"1Y\").sum()\n",
    "        ds_tmp_out.append(xr.Dataset({\"hotdry_\" + rp + \"era5_count\": ds_tmp_q_era5_count}))\n",
    "        ds_tmp_q_gmfd_count = ds_tmp_q_gmfd.resample(time=\"1Y\").sum()\n",
    "        ds_tmp_out.append(xr.Dataset({\"hotdry_\" + rp + \"gmfd_count\": ds_tmp_q_gmfd_count}))\n",
    "\n",
    "        # Longest consecutive hot+dry day streak\n",
    "        ds_tmp_q_era5_streak = ds_tmp_q_era5.resample(time=\"1Y\").apply(n_longest_consecutive)\n",
    "        ds_tmp_out.append(xr.Dataset({\"hotdry_\" + rp + \"era5_streak\": ds_tmp_q_era5_streak}))\n",
    "        ds_tmp_q_gmfd_streak = ds_tmp_q_gmfd.resample(time=\"1Y\").apply(n_longest_consecutive)\n",
    "        ds_tmp_out.append(xr.Dataset({\"hotdry_\" + rp + \"gmfd_streak\": ds_tmp_q_gmfd_streak}))\n",
    "\n",
    "    # Merge metrics and append\n",
    "    ds_final = xr.merge(ds_tmp_out)\n",
    "\n",
    "    # storage options\n",
    "    ds_final = ds_final.chunk({\"ssp\": 1, \"time\": 10, \"lat\": 720, \"lon\": 1440})\n",
    "\n",
    "    compressor = zarr.Blosc(cname=\"zstd\", clevel=3)\n",
    "    encoding = {vname: {\"compressor\": compressor} for vname in ds_final.data_vars}\n",
    "\n",
    "    azure_prefix = \"cil-gdpcir/hotdry/\" + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode=\"w\")\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07752d5-fcc8-415b-bc4f-ef3da06e1c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
