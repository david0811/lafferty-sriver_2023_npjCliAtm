{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35e6a91-5f45-445a-8e78-53fa1dbfa401",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "### TO RUN ON MICROSOFT PLANETARY COMPUTER ####\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8b0cb3-6059-43c0-87ee-cb343c13e526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import planetary_computer\n",
    "import pystac_client\n",
    "import pystac\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "import collections\n",
    "import fsspec\n",
    "import requests\n",
    "\n",
    "import getpass\n",
    "import azure.storage.blob\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230b7c1-1a5c-44ef-be0b-92a672666307",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9e1ed9-efe8-4743-b2f5-7e56b75fcca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# Azure blob storage\n",
    "######################\n",
    "# connection string (from azure web login, select your storage account, then \"Access keys\")\n",
    "connection_string = getpass.getpass()\n",
    "\n",
    "    \n",
    "# format storage\n",
    "container_client = azure.storage.blob.ContainerClient.from_connection_string(\n",
    "    connection_string, container_name=\"mpctransfer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b16c220d-323f-4b12-954f-acfe608b96a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Models\n",
    "###################\n",
    "from utils import cil_ssp_dict\n",
    "\n",
    "models = list(cil_ssp_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27086cd8-9a0c-49bd-871c-8150e4831f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Data access\n",
    "#################\n",
    "\n",
    "# Complete catalog\n",
    "catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "\n",
    "# function to grab variables and SSPs for singe model\n",
    "def grab_model(model_id, vars_to_grab):\n",
    "    # Search across all licences in CIL-GDPCIR\n",
    "    search = catalog.search(\n",
    "        collections=[\"cil-gdpcir-cc0\", \"cil-gdpcir-cc-by\", \"cil-gdpcir-cc-by-sa\"],\n",
    "        query={\"cmip6:source_id\" : {\"eq\": model_id},\n",
    "               \"cmip6:experiment_id\": {\"neq\": \"historical\"}} # omit historical\n",
    "    )\n",
    "    ensemble = search.get_all_items()\n",
    "    \n",
    "    # grab all into one dataset\n",
    "    ds_ssp = []\n",
    "\n",
    "    for item in ensemble:\n",
    "        signed = planetary_computer.sign(item)\n",
    "        ds_vars = []\n",
    "        for variable_id in vars_to_grab:\n",
    "            asset = signed.assets[variable_id]\n",
    "            ds_tmp = xr.open_dataset(asset.href, **asset.extra_fields[\"xarray:open_kwargs\"])\n",
    "            ds_tmp = ds_tmp.assign_coords(ssp = ds_tmp.attrs['experiment_id'])\n",
    "            ds_vars.append(ds_tmp)\n",
    "        ds_ssp.append(xr.merge(ds_vars))\n",
    "\n",
    "    ds_out = xr.concat(ds_ssp, dim='ssp')\n",
    "    \n",
    "    return ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e5f78d-502d-4d43-bd80-01802ed831c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pccompute.westeurope.cloudapp.azure.com/compute/services/dask-gateway/clusters/prod.9137e99390d54e7eb85502a9fd7c7196/status\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# Dask\n",
    "#########\n",
    "import dask_gateway\n",
    "gateway = dask_gateway.Gateway()\n",
    "\n",
    "# cluster options\n",
    "cluster_options = gateway.cluster_options()\n",
    "cluster_options[\"worker_memory\"] = 16\n",
    "cluster_options[\"worker_cores\"] = 1\n",
    "\n",
    "# start cluster\n",
    "cluster = gateway.new_cluster(cluster_options)\n",
    "client = cluster.get_client()\n",
    "cluster.scale(50)\n",
    "\n",
    "# dashboard link\n",
    "print(cluster.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8acc31e-89a3-4d4b-b51e-4e12d5a8e6d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Simple metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c0a62c-d15c-490c-ba23-1f2ac34842a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Annual averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf3ec95-02ea-434e-ae27-cf1555f61f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# loop through models: RUNTIME IS AROUND 15 MINS PER MODEL WITH 40 DASK WORKERS\n",
    "for model in models:\n",
    "    # load data (lazy)\n",
    "    ds = grab_model(model, ['tasmin', 'tasmax', 'pr'])\n",
    "    \n",
    "    # compute\n",
    "    ds['tas'] = (ds['tasmax'] + ds['tasmin']) / 2.\n",
    "    ds_final = ds.resample(time='1Y').mean()\n",
    "    \n",
    "    # unit conversions\n",
    "    ds_final['tas'] = ds_final['tas'] - 273.15 # K -> C\n",
    "    ds_final['tasmax'] = ds_final['tasmax'] - 273.15 # K -> C\n",
    "    ds_final['tasmin'] = ds_final['tasmin'] - 273.15 # K -> C\n",
    "        \n",
    "    # storage options    \n",
    "    ds_final = ds_final.chunk({'ssp':1, 'time':10, 'lat':720, 'lon':1440})\n",
    "    \n",
    "    compressor = zarr.Blosc(cname='zstd', clevel=3)\n",
    "    encoding = {vname: {'compressor': compressor} for vname in ds_final.data_vars}\n",
    "    \n",
    "    azure_prefix = 'cil-gdpcir/avg/' + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode='w')\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb97d939-4949-4517-bb29-e3577dd27ff9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Annual maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c8579e6-6721-4267-bd37-844331af59c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INM-CM5-0\n",
      "MIROC-ES2L\n",
      "MIROC6\n",
      "MPI-ESM1-2-LR\n",
      "NESM3\n",
      "NorESM2-LM\n",
      "NorESM2-MM\n",
      "UKESM1-0-LL\n",
      "CPU times: user 3min 33s, sys: 6.36 s, total: 3min 39s\n",
      "Wall time: 1h 46min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# loop through models: RUNTIME IS AROUND 15 MINS PER MODEL WITH 40 DASK WORKERS\n",
    "for model in models[9:]:\n",
    "    # load data (lazy)\n",
    "    ds = grab_model(model, ['tasmin', 'tasmax', 'pr'])\n",
    "    \n",
    "    # compute\n",
    "    ds['tas'] = (ds['tasmax'] + ds['tasmin']) / 2.\n",
    "    ds_final = ds.resample(time='1Y').max()\n",
    "    \n",
    "    # unit conversions\n",
    "    ds_final['tas'] = ds_final['tas'] - 273.15 # K -> C\n",
    "    ds_final['tasmax'] = ds_final['tasmax'] - 273.15 # K -> C\n",
    "    ds_final['tasmin'] = ds_final['tasmin'] - 273.15 # K -> C\n",
    "        \n",
    "    # storage options    \n",
    "    ds_final = ds_final.chunk({'ssp':1, 'time':10, 'lat':720, 'lon':1440})\n",
    "    \n",
    "    compressor = zarr.Blosc(cname='zstd', clevel=3)\n",
    "    encoding = {vname: {'compressor': compressor} for vname in ds_final.data_vars}\n",
    "    \n",
    "    azure_prefix = 'cil-gdpcir/max/' + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode='w')\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780643e4-79e9-4ea3-92dc-27abab924dc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Dry days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "873331c7-4a6a-4466-8377-21b69ec498e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for longest consecutive spell if needed\n",
    "def n_longest_consecutive(ds, dim='time'):\n",
    "    ds = ds.cumsum(dim=dim) - ds.cumsum(dim=dim).where(ds == 0).ffill(dim=dim).fillna(0)\n",
    "    return ds.max(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49044ccf-dbcb-47f0-93b1-6ec6f660a434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-ESM1-5\n",
      "BCC-CSM2-MR\n",
      "CanESM5\n",
      "CMCC-ESM2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 23:41:11,403 - distributed.client - WARNING - Couldn't gather 1 keys, rescheduling {\"('store-map-4fdc02a2ef25c5a64311bfc931f50efb', 2, 1, 0, 0)\": ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EC-Earth3\n",
      "EC-Earth3-Veg-LR\n",
      "GFDL-ESM4\n",
      "HadGEM3-GC31-LL\n",
      "INM-CM4-8\n",
      "INM-CM5-0\n",
      "MIROC-ES2L\n",
      "MIROC6\n",
      "MPI-ESM1-2-LR\n",
      "NESM3\n",
      "NorESM2-LM\n",
      "NorESM2-MM\n",
      "UKESM1-0-LL\n",
      "CPU times: user 12min 50s, sys: 13.7 s, total: 13min 3s\n",
      "Wall time: 3h 6min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# loop through models: RUNTIME IS AROUND 10 MINS PER MODEL WITH 40 DASK WORKERS\n",
    "for model in models:\n",
    "    # load data (lazy)\n",
    "    ds = grab_model(model, ['pr'])\n",
    "    \n",
    "    # Compute\n",
    "    # Number of dry days\n",
    "    ds_tmp_0 = (ds == 0.).resample(time='1Y').sum() # 0mm\n",
    "    ds_tmp_1 = (ds < 1.).resample(time='1Y').sum() # less than 1mm\n",
    "    # Longest sonsecutive dry day streak\n",
    "    ds_tmp_0c = (ds == 0.).resample(time='1Y').apply(n_longest_consecutive) # 0mm longest consecutive\n",
    "    ds_tmp_1c = (ds < 1.).resample(time='1Y').apply(n_longest_consecutive) # less than 1mm longest consecutive\n",
    "    # Merge\n",
    "    ds_final = xr.merge([ds_tmp_0.rename({'pr':'count_eq_0'}),\n",
    "                       ds_tmp_0c.rename({'pr':'streak_eq_0'}),\n",
    "                       ds_tmp_1.rename({'pr':'count_lt_1'}),\n",
    "                       ds_tmp_1c.rename({'pr':'streak_lt_1'})])\n",
    "        \n",
    "    # storage options    \n",
    "    ds_final = ds_final.chunk({'ssp':1, 'time':10, 'lat':720, 'lon':1440})\n",
    "    \n",
    "    compressor = zarr.Blosc(cname='zstd', clevel=3)\n",
    "    encoding = {vname: {'compressor': compressor} for vname in ds_final.data_vars}\n",
    "    \n",
    "    azure_prefix = 'cil-gdpcir/dry/' + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode='w')\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf0585-27e6-4923-9337-2eacaa5bdfa2",
   "metadata": {},
   "source": [
    "# Less simple metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fb822cd-537e-4273-9742-d694ce85a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for longest consecutive spell if needed\n",
    "def n_longest_consecutive(ds, dim='time'):\n",
    "    ds = ds.cumsum(dim=dim) - ds.cumsum(dim=dim).where(ds == 0).ffill(dim=dim).fillna(0)\n",
    "    return ds.max(dim=dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03338ede-8320-4f94-868a-b3372f78b1b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Wet days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "905e8674-d51e-4b48-ad5c-b4ebd3896dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-ESM1-5\n",
      "BCC-CSM2-MR\n",
      "CanESM5\n",
      "CMCC-ESM2\n",
      "EC-Earth3\n",
      "EC-Earth3-Veg-LR\n",
      "GFDL-ESM4\n",
      "HadGEM3-GC31-LL\n",
      "INM-CM4-8\n",
      "INM-CM5-0\n",
      "MIROC-ES2L\n",
      "MIROC6\n",
      "MPI-ESM1-2-LR\n",
      "NESM3\n",
      "NorESM2-LM\n",
      "NorESM2-MM\n",
      "UKESM1-0-LL\n",
      "CPU times: user 1h 3min 51s, sys: 32.4 s, total: 1h 4min 24s\n",
      "Wall time: 9h 9min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "    \n",
    "# Load quantiles\n",
    "ds_q_era5 = xr.open_dataset('../data/era5_precip_quantiles_nex-cil-deepsd.nc')\n",
    "ds_q_era5['lon'] = np.where(ds_q_era5['lon'] > 180, ds_q_era5['lon'] - 360, ds_q_era5['lon'])\n",
    "ds_q_era5 = ds_q_era5.sortby('lon')\n",
    "\n",
    "ds_q_gmfd = xr.open_dataset('../data/gmfd_precip_quantiles_nex-cil-deepsd.nc')\n",
    "ds_q_gmfd['lon'] = np.where(ds_q_gmfd['lon'] > 180, ds_q_gmfd['lon'] - 360, ds_q_gmfd['lon'])\n",
    "ds_q_gmfd = ds_q_gmfd.sortby('lon')\n",
    "    \n",
    "# Loop through models: RUNTIME IS AROUND 10 MINS PER MODEL WITH 40 DASK WORKERS\n",
    "for model in models:\n",
    "    # Load data (lazy)\n",
    "    ds = grab_model(model, ['pr'])\n",
    "    \n",
    "    ## Calculate metrics\n",
    "    var_id = 'pr'\n",
    "    ds_tmp_out = []\n",
    "    for rp in ['rp5', 'rp10', 'rp20']:\n",
    "        # Get above/below binary\n",
    "        ds_tmp_q_era5 = ds > ds_q_era5[var_id + '_' + rp]\n",
    "        ds_tmp_q_gmfd = ds > ds_q_gmfd[var_id + '_' + rp]\n",
    "        \n",
    "        # Count of hot/wet days\n",
    "        ds_tmp_q_era5_count = ds_tmp_q_era5.resample(time='1Y').sum()\n",
    "        ds_tmp_out.append(ds_tmp_q_era5_count.rename({var_id : var_id + '_' + rp + 'era5_count'}))\n",
    "        ds_tmp_q_gmfd_count = ds_tmp_q_gmfd.resample(time='1Y').sum()\n",
    "        ds_tmp_out.append(ds_tmp_q_gmfd_count.rename({var_id : var_id + '_' + rp + 'gmfd_count'}))\n",
    "        \n",
    "        # Longest consecutive hot/wet day streak\n",
    "        ds_tmp_q_era5_streak = ds_tmp_q_era5.resample(time='1Y').apply(n_longest_consecutive)\n",
    "        ds_tmp_out.append(ds_tmp_q_era5_streak.rename({var_id : var_id + '_' + rp + 'era5_streak'}))\n",
    "        ds_tmp_q_gmfd_streak = ds_tmp_q_gmfd.resample(time='1Y').apply(n_longest_consecutive)\n",
    "        ds_tmp_out.append(ds_tmp_q_gmfd_streak.rename({var_id : var_id + '_' + rp + 'gmfd_streak'}))\n",
    "        \n",
    "    # Merge metrics and append\n",
    "    ds_final = xr.merge(ds_tmp_out)\n",
    "        \n",
    "    # storage options    \n",
    "    ds_final = ds_final.chunk({'ssp':1, 'time':10, 'lat':720, 'lon':1440})\n",
    "    \n",
    "    compressor = zarr.Blosc(cname='zstd', clevel=3)\n",
    "    encoding = {vname: {'compressor': compressor} for vname in ds_final.data_vars}\n",
    "    \n",
    "    azure_prefix = 'cil-gdpcir/wet/' + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode='w')\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10658cc1-ca2b-40e9-b52e-1bae96472af0",
   "metadata": {},
   "source": [
    "## Hot days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cdc55e9-ea3b-404f-99a9-b30f66b25110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to speed up the calculation, we calculate hot days for tasmin, tasmax only\n",
    "# and only use the GMFD quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d56592-9395-4f7c-a00f-a40956b31033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-ESM1-5\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "    \n",
    "# Load quantiles\n",
    "ds_q_gmfd = xr.open_dataset('../data/gmfd_temperature_quantiles_nex-cil-deepsd.nc')\n",
    "ds_q_gmfd['lon'] = np.where(ds_q_gmfd['lon'] > 180, ds_q_gmfd['lon'] - 360, ds_q_gmfd['lon'])\n",
    "ds_q_gmfd = ds_q_gmfd.sortby('lon')\n",
    "    \n",
    "# Loop through models\n",
    "for model in models:\n",
    "    # Load data (lazy)\n",
    "    ds = grab_model(model, ['tasmin','tasmax'])\n",
    "    ds -= 273.15 # K -> C\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ds_tmp_final = []\n",
    "    for var_id in ['tasmin','tasmax']:\n",
    "        ds_tmp_out = []\n",
    "        for rp in ['rp5', 'rp10', 'rp20']:\n",
    "            # Get above/below binary\n",
    "            ds_tmp_q_gmfd = ds[var_id] > ds_q_gmfd[var_id + '_' + rp]\n",
    "            \n",
    "            # Count of hot days\n",
    "            ds_tmp_q_gmfd_count = ds_tmp_q_gmfd.resample(time='1Y').sum()\n",
    "            ds_tmp_out.append(xr.Dataset({var_id + '_' + rp + 'gmfd_count': ds_tmp_q_gmfd_count}))\n",
    "            \n",
    "            # Longest consecutive hot day streak\n",
    "            ds_tmp_q_gmfd_streak = ds_tmp_q_gmfd.resample(time='1Y').apply(n_longest_consecutive)\n",
    "            ds_tmp_out.append(xr.Dataset({var_id + '_' + rp + 'gmfd_streak': ds_tmp_q_gmfd_streak}))\n",
    "        \n",
    "        # Merge RPs and append\n",
    "        ds_out = xr.merge(ds_tmp_out)\n",
    "        ds_tmp_final.append(ds_out)\n",
    "    \n",
    "    # Merge variables\n",
    "    ds_final = xr.merge(ds_tmp_final)\n",
    "    \n",
    "    # storage options    \n",
    "    ds_final = ds_final.chunk({'ssp':1, 'time':20, 'lat':600, 'lon':1440})\n",
    "    \n",
    "    compressor = zarr.Blosc(cname='zstd', clevel=3)\n",
    "    encoding = {vname: {'compressor': compressor} for vname in ds_final.data_vars}\n",
    "    \n",
    "    azure_prefix = 'cil-gdpcir/hot/' + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode='w')\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f657669-8631-49c9-a3bd-56268f77f329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
