{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35e6a91-5f45-445a-8e78-53fa1dbfa401",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "### TO RUN ON MICROSOFT PLANETARY COMPUTER ####\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8b0cb3-6059-43c0-87ee-cb343c13e526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import planetary_computer\n",
    "import pystac_client\n",
    "import pystac\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "import collections\n",
    "import fsspec\n",
    "import requests\n",
    "\n",
    "import getpass\n",
    "import azure.storage.blob\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230b7c1-1a5c-44ef-be0b-92a672666307",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9e1ed9-efe8-4743-b2f5-7e56b75fcca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# Azure blob storage\n",
    "######################\n",
    "# connection string (from azure web login, select your storage account, then \"Access keys\")\n",
    "connection_string = getpass.getpass()\n",
    "\n",
    "    \n",
    "# format storage\n",
    "container_client = azure.storage.blob.ContainerClient.from_connection_string(\n",
    "    connection_string, container_name=\"mpctransfer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b16c220d-323f-4b12-954f-acfe608b96a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Models\n",
    "###################\n",
    "from utils import cil_ssp_dict\n",
    "\n",
    "models = list(cil_ssp_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27086cd8-9a0c-49bd-871c-8150e4831f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Data access\n",
    "#################\n",
    "\n",
    "# Complete catalog\n",
    "catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "\n",
    "# function to grab variables and SSPs for singe model\n",
    "def grab_model(model_id, vars_to_grab):\n",
    "    # Search across all licences in CIL-GDPCIR\n",
    "    search = catalog.search(\n",
    "        collections=[\"cil-gdpcir-cc0\", \"cil-gdpcir-cc-by\", \"cil-gdpcir-cc-by-sa\"],\n",
    "        query={\"cmip6:source_id\" : {\"eq\": model_id},\n",
    "               \"cmip6:experiment_id\": {\"neq\": \"historical\"}} # omit historical\n",
    "    )\n",
    "    ensemble = search.get_all_items()\n",
    "    \n",
    "    # grab all into one dataset\n",
    "    ds_ssp = []\n",
    "\n",
    "    for item in ensemble:\n",
    "        signed = planetary_computer.sign(item)\n",
    "        ds_vars = []\n",
    "        for variable_id in vars_to_grab:\n",
    "            asset = signed.assets[variable_id]\n",
    "            ds_tmp = xr.open_dataset(asset.href, **asset.extra_fields[\"xarray:open_kwargs\"])\n",
    "            ds_tmp = ds_tmp.assign_coords(ssp = ds_tmp.attrs['experiment_id'])\n",
    "            ds_vars.append(ds_tmp)\n",
    "        ds_ssp.append(xr.merge(ds_vars))\n",
    "\n",
    "    ds_out = xr.concat(ds_ssp, dim='ssp')\n",
    "    \n",
    "    return ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32e5f78d-502d-4d43-bd80-01802ed831c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pccompute.westeurope.cloudapp.azure.com/compute/services/dask-gateway/clusters/prod.89abceba62a04dd0b89baf0d6ba012fb/status\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# Dask\n",
    "#########\n",
    "import dask_gateway\n",
    "gateway = dask_gateway.Gateway()\n",
    "\n",
    "# cluster options\n",
    "cluster_options = gateway.cluster_options()\n",
    "cluster_options[\"worker_memory\"] = 16\n",
    "cluster_options[\"worker_cores\"] = 1\n",
    "\n",
    "# start cluster\n",
    "cluster = gateway.new_cluster(cluster_options)\n",
    "client = cluster.get_client()\n",
    "cluster.scale(40)\n",
    "\n",
    "# dashboard link\n",
    "print(cluster.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8acc31e-89a3-4d4b-b51e-4e12d5a8e6d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Simple metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c0a62c-d15c-490c-ba23-1f2ac34842a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Annual averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf3ec95-02ea-434e-ae27-cf1555f61f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# loop through models: RUNTIME IS AROUND 15 MINS PER MODEL WITH 40 DASK WORKERS\n",
    "for model in models:\n",
    "    # load data (lazy)\n",
    "    ds = grab_model(model, ['tasmin', 'tasmax', 'pr'])\n",
    "    \n",
    "    # compute\n",
    "    ds['tas'] = (ds['tasmax'] + ds['tasmin']) / 2.\n",
    "    ds_final = ds.resample(time='1Y').mean()\n",
    "    \n",
    "    # unit conversions\n",
    "    ds_final['tas'] = ds_final['tas'] - 273.15 # K -> C\n",
    "    ds_final['tasmax'] = ds_final['tasmax'] - 273.15 # K -> C\n",
    "    ds_final['tasmin'] = ds_final['tasmin'] - 273.15 # K -> C\n",
    "        \n",
    "    # storage options    \n",
    "    ds_final = ds_final.chunk({'ssp':1, 'time':10, 'lat':720, 'lon':1440})\n",
    "    \n",
    "    compressor = zarr.Blosc(cname='zstd', clevel=3)\n",
    "    encoding = {vname: {'compressor': compressor} for vname in ds_final.data_vars}\n",
    "    \n",
    "    azure_prefix = 'cil-gdpcir/avg/' + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode='w')\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb97d939-4949-4517-bb29-e3577dd27ff9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1-day max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c8579e6-6721-4267-bd37-844331af59c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INM-CM5-0\n",
      "MIROC-ES2L\n",
      "MIROC6\n",
      "MPI-ESM1-2-LR\n",
      "NESM3\n",
      "NorESM2-LM\n",
      "NorESM2-MM\n",
      "UKESM1-0-LL\n",
      "CPU times: user 3min 33s, sys: 6.36 s, total: 3min 39s\n",
      "Wall time: 1h 46min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# loop through models: RUNTIME IS AROUND 15 MINS PER MODEL WITH 40 DASK WORKERS\n",
    "for model in models[9:]:\n",
    "    # load data (lazy)\n",
    "    ds = grab_model(model, ['tasmin', 'tasmax', 'pr'])\n",
    "    \n",
    "    # compute\n",
    "    ds['tas'] = (ds['tasmax'] + ds['tasmin']) / 2.\n",
    "    ds_final = ds.resample(time='1Y').max()\n",
    "    \n",
    "    # unit conversions\n",
    "    ds_final['tas'] = ds_final['tas'] - 273.15 # K -> C\n",
    "    ds_final['tasmax'] = ds_final['tasmax'] - 273.15 # K -> C\n",
    "    ds_final['tasmin'] = ds_final['tasmin'] - 273.15 # K -> C\n",
    "        \n",
    "    # storage options    \n",
    "    ds_final = ds_final.chunk({'ssp':1, 'time':10, 'lat':720, 'lon':1440})\n",
    "    \n",
    "    compressor = zarr.Blosc(cname='zstd', clevel=3)\n",
    "    encoding = {vname: {'compressor': compressor} for vname in ds_final.data_vars}\n",
    "    \n",
    "    azure_prefix = 'cil-gdpcir/max/' + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode='w')\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e2edac-d9e8-41e3-8a32-f4a4ebf5ffec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 5-day max (pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17dc9f7b-6413-45aa-9632-f678ff931fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xclim\n",
    "xclim.set_options(cf_compliance=\"log\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97848ea0-56f2-48e9-b5e6-16a100a906a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-ESM1-5\n",
      "BCC-CSM2-MR\n",
      "CanESM5\n",
      "CMCC-ESM2\n",
      "EC-Earth3\n",
      "EC-Earth3-Veg-LR\n",
      "GFDL-ESM4\n",
      "HadGEM3-GC31-LL\n",
      "INM-CM4-8\n",
      "INM-CM5-0\n",
      "MIROC-ES2L\n",
      "MIROC6\n",
      "MPI-ESM1-2-LR\n",
      "NESM3\n",
      "NorESM2-LM\n",
      "NorESM2-MM\n",
      "UKESM1-0-LL\n",
      "CPU times: user 15min 25s, sys: 8.66 s, total: 15min 34s\n",
      "Wall time: 7h 22min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# loop through models: RUNTIME IS AROUND 20 MINS PER MODEL WITH 40 DASK WORKERS\n",
    "for model in models:\n",
    "    # load data (lazy)\n",
    "    ds = grab_model(model, ['pr'])\n",
    "    \n",
    "    # Compute\n",
    "    ds_final = xclim.indicators.icclim.RX5day(ds=ds, freq='Y')\n",
    "        \n",
    "    # Storage options\n",
    "    ds_final = xr.Dataset({'RX5day':ds_final})\n",
    "    ds_final = ds_final.chunk({'ssp':1, 'time':10, 'lat':720, 'lon':1440})\n",
    "    \n",
    "    compressor = zarr.Blosc(cname='zstd', clevel=3)\n",
    "    encoding = {vname: {'compressor': compressor} for vname in ds_final.data_vars}\n",
    "    \n",
    "    azure_prefix = 'cil-gdpcir/max5d/' + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode='w')\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780643e4-79e9-4ea3-92dc-27abab924dc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Dry days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f305273-59aa-4e0f-b0d3-eb92bcd0b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# NOTE: for CIL, I believe that all days with <1mm precipitation \n",
    "# are set to zero, so the distinctions here are redundant\n",
    "# but I did not know this until after having done the calculations\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "873331c7-4a6a-4466-8377-21b69ec498e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for longest consecutive spell if needed\n",
    "def n_longest_consecutive(ds, dim='time'):\n",
    "    ds = ds.cumsum(dim=dim) - ds.cumsum(dim=dim).where(ds == 0).ffill(dim=dim).fillna(0)\n",
    "    return ds.max(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49044ccf-dbcb-47f0-93b1-6ec6f660a434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-ESM1-5\n",
      "BCC-CSM2-MR\n",
      "CanESM5\n",
      "CMCC-ESM2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 23:41:11,403 - distributed.client - WARNING - Couldn't gather 1 keys, rescheduling {\"('store-map-4fdc02a2ef25c5a64311bfc931f50efb', 2, 1, 0, 0)\": ()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EC-Earth3\n",
      "EC-Earth3-Veg-LR\n",
      "GFDL-ESM4\n",
      "HadGEM3-GC31-LL\n",
      "INM-CM4-8\n",
      "INM-CM5-0\n",
      "MIROC-ES2L\n",
      "MIROC6\n",
      "MPI-ESM1-2-LR\n",
      "NESM3\n",
      "NorESM2-LM\n",
      "NorESM2-MM\n",
      "UKESM1-0-LL\n",
      "CPU times: user 12min 50s, sys: 13.7 s, total: 13min 3s\n",
      "Wall time: 3h 6min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# loop through models: RUNTIME IS AROUND 10 MINS PER MODEL WITH 40 DASK WORKERS\n",
    "for model in models:\n",
    "    # load data (lazy)\n",
    "    ds = grab_model(model, ['pr'])\n",
    "    \n",
    "    # Compute\n",
    "    # Number of dry days\n",
    "    ds_tmp_0 = (ds == 0.).resample(time='1Y').sum() # 0mm\n",
    "    ds_tmp_1 = (ds < 1.).resample(time='1Y').sum() # less than 1mm\n",
    "    # Longest consecutive dry day streak\n",
    "    ds_tmp_0c = (ds == 0.).resample(time='1Y').apply(n_longest_consecutive) # 0mm longest consecutive\n",
    "    ds_tmp_1c = (ds < 1.).resample(time='1Y').apply(n_longest_consecutive) # less than 1mm longest consecutive\n",
    "    # Merge\n",
    "    ds_final = xr.merge([ds_tmp_0.rename({'pr':'count_eq_0'}),\n",
    "                       ds_tmp_0c.rename({'pr':'streak_eq_0'}),\n",
    "                       ds_tmp_1.rename({'pr':'count_lt_1'}),\n",
    "                       ds_tmp_1c.rename({'pr':'streak_lt_1'})])\n",
    "        \n",
    "    # storage options    \n",
    "    ds_final = ds_final.chunk({'ssp':1, 'time':10, 'lat':720, 'lon':1440})\n",
    "    \n",
    "    compressor = zarr.Blosc(cname='zstd', clevel=3)\n",
    "    encoding = {vname: {'compressor': compressor} for vname in ds_final.data_vars}\n",
    "    \n",
    "    azure_prefix = 'cil-gdpcir/dry/' + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode='w')\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf0585-27e6-4923-9337-2eacaa5bdfa2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Less simple metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fb822cd-537e-4273-9742-d694ce85a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for longest consecutive spell if needed\n",
    "def n_longest_consecutive(ds, dim='time'):\n",
    "    ds = ds.cumsum(dim=dim) - ds.cumsum(dim=dim).where(ds == 0).ffill(dim=dim).fillna(0)\n",
    "    return ds.max(dim=dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03338ede-8320-4f94-868a-b3372f78b1b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Wet days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "905e8674-d51e-4b48-ad5c-b4ebd3896dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-ESM1-5\n",
      "BCC-CSM2-MR\n",
      "CanESM5\n",
      "CMCC-ESM2\n",
      "EC-Earth3\n",
      "EC-Earth3-Veg-LR\n",
      "GFDL-ESM4\n",
      "HadGEM3-GC31-LL\n",
      "INM-CM4-8\n",
      "INM-CM5-0\n",
      "MIROC-ES2L\n",
      "MIROC6\n",
      "MPI-ESM1-2-LR\n",
      "NESM3\n",
      "NorESM2-LM\n",
      "NorESM2-MM\n",
      "UKESM1-0-LL\n",
      "CPU times: user 22min 10s, sys: 8.22 s, total: 22min 18s\n",
      "Wall time: 6h 24min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "    \n",
    "# Load quantiles\n",
    "ds_q_era5 = xr.open_dataset('../data/quantiles/era5_precip_quantiles_nex-cil-deepsd.nc')\n",
    "ds_q_era5['lon'] = np.where(ds_q_era5['lon'] > 180, ds_q_era5['lon'] - 360, ds_q_era5['lon'])\n",
    "ds_q_era5 = ds_q_era5.sortby('lon')\n",
    "\n",
    "ds_q_gmfd = xr.open_dataset('../data/quantiles/gmfd_precip_quantiles_nex-cil-deepsd.nc')\n",
    "ds_q_gmfd['lon'] = np.where(ds_q_gmfd['lon'] > 180, ds_q_gmfd['lon'] - 360, ds_q_gmfd['lon'])\n",
    "ds_q_gmfd = ds_q_gmfd.sortby('lon')\n",
    "    \n",
    "# Loop through models: RUNTIME IS AROUND 10 MINS PER MODEL WITH 40 DASK WORKERS\n",
    "for model in models:\n",
    "    # Load data (lazy)\n",
    "    ds = grab_model(model, ['pr'])\n",
    "    \n",
    "    ## Calculate metrics\n",
    "    var_id = 'pr'\n",
    "    ds_tmp_out = []\n",
    "    for rp in ['q99', 'rp10']:\n",
    "        # Get above/below binary\n",
    "        ds_tmp_q_era5 = ds[var_id] > ds_q_era5[var_id + '_' + rp]\n",
    "        ds_tmp_q_gmfd = ds[var_id] > ds_q_gmfd[var_id + '_' + rp]\n",
    "        \n",
    "        # Count of hot days\n",
    "        ds_tmp_q_era5_count = ds_tmp_q_era5.resample(time='1Y').sum()\n",
    "        ds_tmp_out.append(xr.Dataset({var_id + '_' + rp + 'era5_count': ds_tmp_q_era5_count}))\n",
    "        ds_tmp_q_gmfd_count = ds_tmp_q_gmfd.resample(time='1Y').sum()\n",
    "        ds_tmp_out.append(xr.Dataset({var_id + '_' + rp + 'gmfd_count': ds_tmp_q_gmfd_count}))\n",
    "        \n",
    "        # Longest consecutive hot day streak\n",
    "        ds_tmp_q_era5_streak = ds_tmp_q_era5.resample(time='1Y').apply(n_longest_consecutive)\n",
    "        ds_tmp_out.append(xr.Dataset({var_id + '_' + rp + 'era5_streak': ds_tmp_q_era5_streak}))\n",
    "        ds_tmp_q_gmfd_streak = ds_tmp_q_gmfd.resample(time='1Y').apply(n_longest_consecutive)\n",
    "        ds_tmp_out.append(xr.Dataset({var_id + '_' + rp + 'gmfd_streak': ds_tmp_q_gmfd_streak}))\n",
    "        \n",
    "    # Merge metrics and append\n",
    "    ds_final = xr.merge(ds_tmp_out)\n",
    "        \n",
    "    # storage options    \n",
    "    ds_final = ds_final.chunk({'ssp':1, 'time':10, 'lat':720, 'lon':1440})\n",
    "    \n",
    "    compressor = zarr.Blosc(cname='zstd', clevel=3)\n",
    "    encoding = {vname: {'compressor': compressor} for vname in ds_final.data_vars}\n",
    "    \n",
    "    azure_prefix = 'cil-gdpcir/wet/' + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode='w')\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10658cc1-ca2b-40e9-b52e-1bae96472af0",
   "metadata": {},
   "source": [
    "## Hot days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d56592-9395-4f7c-a00f-a40956b31033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-ESM1-5\n"
     ]
    },
    {
     "ename": "KilledWorker",
     "evalue": "(\"('gt-481c11cc6d0a8d9a2dc84a5204cdc76f', 0, 61, 1, 3)\", <WorkerState 'tls://10.244.119.118:46151', name: dask-worker-89abceba62a04dd0b89baf0d6ba012fb-h72vz, status: closed, memory: 0, processing: 657>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKilledWorker\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:55\u001b[0m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/core/dataset.py:2060\u001b[0m, in \u001b[0;36mDataset.to_zarr\u001b[0;34m(self, store, chunk_store, mode, synchronizer, group, encoding, compute, consolidated, append_dim, region, safe_chunks, storage_options)\u001b[0m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;124;03m\"\"\"Write dataset contents to a zarr group.\u001b[39;00m\n\u001b[1;32m   1951\u001b[0m \n\u001b[1;32m   1952\u001b[0m \u001b[38;5;124;03mZarr chunks are determined in the following way:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;124;03m    The I/O user guide, with more details and examples.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2058\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_zarr\n\u001b[0;32m-> 2060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_zarr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   2061\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mappend_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mappend_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2073\u001b[0m \u001b[43m    \u001b[49m\u001b[43msafe_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2074\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/api.py:1638\u001b[0m, in \u001b[0;36mto_zarr\u001b[0;34m(dataset, store, chunk_store, mode, synchronizer, group, encoding, compute, consolidated, append_dim, region, safe_chunks, storage_options)\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;66;03m# TODO: figure out how to properly handle unlimited_dims\u001b[39;00m\n\u001b[1;32m   1637\u001b[0m dump_to_store(dataset, zstore, writer, encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[0;32m-> 1638\u001b[0m writes \u001b[38;5;241m=\u001b[39m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[1;32m   1641\u001b[0m     _finalize_store(writes, zstore)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/common.py:168\u001b[0m, in \u001b[0;36mArrayWriter.sync\u001b[0;34m(self, compute)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mda\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# TODO: consider wrapping targets with dask.delayed, if this makes\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# for any discernible difference in perforance, e.g.,\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# targets = [dask.delayed(t) for t in self.targets]\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m delayed_store \u001b[38;5;241m=\u001b[39m \u001b[43mda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflush\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msources \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/dask/array/core.py:1230\u001b[0m, in \u001b[0;36mstore\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compute:\n\u001b[1;32m   1229\u001b[0m     store_dsk \u001b[38;5;241m=\u001b[39m HighLevelGraph(layers, dependencies)\n\u001b[0;32m-> 1230\u001b[0m     \u001b[43mcompute_as_if_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mArray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_dsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/dask/base.py:342\u001b[0m, in \u001b[0;36mcompute_as_if_collection\u001b[0;34m(cls, dsk, keys, scheduler, get, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m schedule \u001b[38;5;241m=\u001b[39m get_scheduler(scheduler\u001b[38;5;241m=\u001b[39mscheduler, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, get\u001b[38;5;241m=\u001b[39mget)\n\u001b[1;32m    341\u001b[0m dsk2 \u001b[38;5;241m=\u001b[39m optimization_function(\u001b[38;5;28mcls\u001b[39m)(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3036\u001b[0m, in \u001b[0;36mClient.get\u001b[0;34m(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   3034\u001b[0m         should_rejoin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3036\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirect\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3037\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   3038\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m futures\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:2210\u001b[0m, in \u001b[0;36mClient.gather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2209\u001b[0m     local_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gather\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2213\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m    \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/utils.py:338\u001b[0m, in \u001b[0;36mSyncMethodMixin.sync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/utils.py:405\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[1;32m    404\u001b[0m     typ, exc, tb \u001b[38;5;241m=\u001b[39m error\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/utils.py:378\u001b[0m, in \u001b[0;36msync.<locals>.f\u001b[0;34m()\u001b[0m\n\u001b[1;32m    376\u001b[0m         future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(future, callback_timeout)\n\u001b[1;32m    377\u001b[0m     future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(future)\n\u001b[0;32m--> 378\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m future\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     error \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/tornado/gen.py:762\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 762\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     exc_info \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:2073\u001b[0m, in \u001b[0;36mClient._gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         exc \u001b[38;5;241m=\u001b[39m CancelledError(key)\n\u001b[1;32m   2072\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2073\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   2075\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKilledWorker\u001b[0m: (\"('gt-481c11cc6d0a8d9a2dc84a5204cdc76f', 0, 61, 1, 3)\", <WorkerState 'tls://10.244.119.118:46151', name: dask-worker-89abceba62a04dd0b89baf0d6ba012fb-h72vz, status: closed, memory: 0, processing: 657>)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "    \n",
    "# Load quantiles\n",
    "ds_q_gmfd = xr.open_dataset('../data/quantiles/gmfd_temperature_quantiles_nex-cil-deepsd.nc')\n",
    "ds_q_gmfd['lon'] = np.where(ds_q_gmfd['lon'] > 180, ds_q_gmfd['lon'] - 360, ds_q_gmfd['lon'])\n",
    "ds_q_gmfd = ds_q_gmfd.sortby('lon')\n",
    "\n",
    "ds_q_era5 = xr.open_dataset('../data/quantiles/era5_temperature_quantiles_nex-cil-deepsd.nc')\n",
    "ds_q_era5['lon'] = np.where(ds_q_era5['lon'] > 180, ds_q_era5['lon'] - 360, ds_q_era5['lon'])\n",
    "ds_q_era5 = ds_q_era5.sortby('lon')\n",
    "    \n",
    "# Loop through models\n",
    "for model in models[1:]:\n",
    "    # Load data (lazy)\n",
    "    ds = grab_model(model, ['tasmin','tasmax'])\n",
    "    ds['tas'] = (ds['tasmax'] + ds['tasmin']) / 2.\n",
    "    ds -= 273.15 # K -> C\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ds_tmp_final = []\n",
    "    for var_id in ['tasmin','tasmax', 'tas']:\n",
    "        ds_tmp_out = []\n",
    "        for rp in ['q99', 'rp10']:\n",
    "            # Get above/below binary\n",
    "            ds_tmp_q_era5 = ds[var_id] > ds_q_era5[var_id + '_' + rp]\n",
    "            ds_tmp_q_gmfd = ds[var_id] > ds_q_gmfd[var_id + '_' + rp]\n",
    "        \n",
    "            # Count of hot days\n",
    "            ds_tmp_q_era5_count = ds_tmp_q_era5.resample(time='1Y').sum()\n",
    "            ds_tmp_out.append(xr.Dataset({var_id + '_' + rp + 'era5_count': ds_tmp_q_era5_count}))\n",
    "            ds_tmp_q_gmfd_count = ds_tmp_q_gmfd.resample(time='1Y').sum()\n",
    "            ds_tmp_out.append(xr.Dataset({var_id + '_' + rp + 'gmfd_count': ds_tmp_q_gmfd_count}))\n",
    "        \n",
    "            # Longest consecutive hot day streak\n",
    "            ds_tmp_q_era5_streak = ds_tmp_q_era5.resample(time='1Y').apply(n_longest_consecutive)\n",
    "            ds_tmp_out.append(xr.Dataset({var_id + '_' + rp + 'era5_streak': ds_tmp_q_era5_streak}))\n",
    "            ds_tmp_q_gmfd_streak = ds_tmp_q_gmfd.resample(time='1Y').apply(n_longest_consecutive)\n",
    "            ds_tmp_out.append(xr.Dataset({var_id + '_' + rp + 'gmfd_streak': ds_tmp_q_gmfd_streak}))\n",
    "        \n",
    "        # Merge RPs and append\n",
    "        ds_out = xr.merge(ds_tmp_out)\n",
    "        ds_tmp_final.append(ds_out)\n",
    "    \n",
    "    # Merge variables\n",
    "    ds_final = xr.merge(ds_tmp_final)\n",
    "    \n",
    "    # storage options    \n",
    "    ds_final = ds_final.chunk({'ssp':1, 'time':20, 'lat':600, 'lon':1440})\n",
    "    \n",
    "    compressor = zarr.Blosc(cname='zstd', clevel=3)\n",
    "    encoding = {vname: {'compressor': compressor} for vname in ds_final.data_vars}\n",
    "    \n",
    "    azure_prefix = 'cil-gdpcir/hot/' + model\n",
    "    store = zarr.ABSStore(client=container_client, prefix=azure_prefix)\n",
    "\n",
    "    # store\n",
    "    ds_final.to_zarr(store=store, encoding=encoding, consolidated=True, mode='w')\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f657669-8631-49c9-a3bd-56268f77f329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
