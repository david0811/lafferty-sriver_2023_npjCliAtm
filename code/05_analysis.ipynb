{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57743a7e-8b68-4544-8378-ac3e453f96bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f3946-bb89-4535-a1e8-ae9b6ccaed61",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "662dfbf5-61c7-48f6-884d-ce058d6a7512",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Set paths\n",
    "# UPDATE THIS FOR REPRODUCTION\n",
    "###############################\n",
    "nex_in = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/nex-gddp/'\n",
    "cil_in = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/cil-gdpcir/'\n",
    "isi_in = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/isimip3b/regridded/conservative/'\n",
    "cbp_in = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/carbonplan/'\n",
    "\n",
    "out_path = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/uc_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c50de8a-ea1e-4a9d-ad37-c522111ff76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Models\n",
    "###################\n",
    "from utils import nex_ssp_dict, cil_ssp_dict, isimip_ssp_dict, gardsv_ssp_dict, gardsv_var_dict, deepsdbc_dict\n",
    "\n",
    "nex_models = list(nex_ssp_dict.keys())\n",
    "cil_models = list(cil_ssp_dict.keys())\n",
    "isi_models = list(isimip_ssp_dict.keys())\n",
    "cbp_gard_models = list(gardsv_ssp_dict.keys())\n",
    "cbp_gard_precip_models = [model for model in cbp_gard_models if 'pr' in gardsv_var_dict[model]]\n",
    "cbp_deep_models = list(deepsdbc_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1c47d5-5fb5-4648-aac2-a86cbb62fdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-fe6f75b0-6494-11ed-9ef9-34e6d79eac77</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_jobqueue.PBSCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">PBSCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">efd2b5b2</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-c6b421d9-9946-4240-bd99-b5e343da86e2</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://10.102.201.239:40469\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.102.201.239:40469' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############\n",
    "# Dask\n",
    "############\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "cluster = PBSCluster(cores=1, memory='45GB', resource_spec='pmem=45GB',\n",
    "                     # account='open',\n",
    "                     worker_extra_args=['#PBS -l feature=rhel7'], \n",
    "                     walltime='00:30:00')\n",
    "\n",
    "cluster.scale(jobs=10)  # ask for jobs\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f20699-35c7-4dbf-a8ad-ad73115785df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Total uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6099faaa-43cd-4e3b-a541-8cf3dfb66379",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a2c6626-3157-4eee-9847-5df9e74f459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Total uncertainty: variance across all models, scenarios, ensembles \n",
    "#######################################################################\n",
    "def uc_total(nex_in, nex_models, \n",
    "             cil_in, cil_models, \n",
    "             isi_in, isi_models, \n",
    "             cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "             metric, submetric,\n",
    "             year):\n",
    "    \"\"\"\n",
    "    Reads in all models, ssps, and calculates the total uncertainty (variance across\n",
    "    all model, ssp, ensemble dimensions) for a given year (and possibly DataArray).\n",
    "    For metrics like 'hot' where there are several sub-metrics based on different \n",
    "    thresholds and/or observational data, we need to select a specific DataArray\n",
    "    to keep the memory manageable.\n",
    "    \"\"\"\n",
    "    # Subfunction for general preprocessing of each model/ensemble\n",
    "    def read_and_process(ensemble, path_in, model, year, metric, submetric):\n",
    "        # Read netcdf or zarr\n",
    "        if ensemble in ['NEX', 'ISIMIP', 'GARD-SV']:\n",
    "            ds = xr.open_dataset(path_in + metric + '/' + model + '.nc')\n",
    "        elif ensemble in ['CIL', 'DeepSD-BC']:\n",
    "            ds = xr.open_dataset(path_in + metric + '/' + model, engine='zarr')\n",
    "            \n",
    "        \n",
    "        # Select submetric if chosen\n",
    "        if submetric:\n",
    "            ds = ds[submetric]\n",
    "    \n",
    "        # Common preprocessing\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = ensemble)\n",
    "        ds = ds.assign_coords(model = model)\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "    \n",
    "        # Fix lon to [-180,180]\n",
    "        if ensemble in ['NEX', 'ISIMIP']:\n",
    "            ds['lon'] = np.where(ds['lon'] > 180, ds['lon'] - 360, ds['lon'])\n",
    "            ds = ds.sortby('lon')\n",
    "    \n",
    "        # Some models/methods are missing precip so fill with NaNs\n",
    "        if (metric in ['max', 'avg']) and ('pr' not in ds.data_vars):\n",
    "            ds['pr'] = xr.full_like(ds[list(ds.data_vars)[0]], np.nan)\n",
    "    \n",
    "        # Return\n",
    "        return ds\n",
    "\n",
    "    ######################\n",
    "    # Read all ensembles\n",
    "    ######################\n",
    "    # NEX-GDDP \n",
    "    ds_out = []\n",
    "    for model in nex_models:\n",
    "        ds_out.append(read_and_process('NEX', nex_in, model, year, metric, submetric))\n",
    "    ds_nex = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "\n",
    "    # CIL-GDPCIR\n",
    "    ds_out = []\n",
    "    for model in cil_models:\n",
    "        ds_out.append(read_and_process('CIL', cil_in, model, year, metric, submetric))\n",
    "    ds_cil = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "\n",
    "    # ISIMIP\n",
    "    ds_out = []\n",
    "    for model in isi_models:\n",
    "        ds_out.append(read_and_process('ISIMIP', isi_in, model, year, metric, submetric))\n",
    "    ds_isi = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "\n",
    "    # carbonplan: GARD-SV\n",
    "    ds_out = []\n",
    "    for model in cbp_gard_models:\n",
    "        ds_out.append(read_and_process('GARD-SV', cbp_in + '/regridded/conservative/GARD-SV/', model, year, metric, submetric))\n",
    "    ds_cbp_gard = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "    \n",
    "    # carbonplan: DeepSD-BC\n",
    "    ds_out = []\n",
    "    for model in cbp_deep_models:\n",
    "        ds_out.append(read_and_process('DeepSD-BC', cbp_in + 'native_grid/DeepSD-BC/', model, year, metric, submetric))\n",
    "    ds_cbp_deep = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "\n",
    "    ###########################\n",
    "    # Merge all and mask ocean\n",
    "    ###########################\n",
    "    ds = xr.concat([ds_nex, ds_cil, ds_isi, ds_cbp_gard, ds_cbp_deep],\n",
    "                       dim='ensemble', fill_value=np.nan)\n",
    "    \n",
    "    # Mask out ocean points (NEX is only available over land)\n",
    "    ds_mask = ds.sel(ensemble='NEX').isel(ssp=0, model=0)[list(ds.keys())[0]].isnull()\n",
    "    ds = xr.where(ds_mask, np.nan, ds)\n",
    "    \n",
    "    ##########################\n",
    "    # Uncertainty calculation\n",
    "    ##########################\n",
    "    ## Total uncertainty\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        U_total_true = ds.var(dim=['ensemble', 'ssp', 'model']) # throws warning when all NaNs\n",
    "\n",
    "    U_total_true = U_total_true.assign_coords(uncertainty = 'total_true')\n",
    "    \n",
    "    return U_total_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae5b42a-fc3e-49d4-8763-7a7eb1840181",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Annual averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb9a5cf-fef0-4b13-a893-c3abb8f8869f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.5 s, sys: 4.94 s, total: 1min 4s\n",
      "Wall time: 7min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "metric = 'avg'\n",
    "\n",
    "# Dask delayed over years\n",
    "delayed_res = []\n",
    "for year in range(2015, 2100):\n",
    "    # Read all ensembles and compute total uncertainty\n",
    "    tmp_res = dask.delayed(uc_total)(nex_in, nex_models, \n",
    "                                     cil_in, cil_models, \n",
    "                                     isi_in, isi_models, \n",
    "                                     cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                     metric, False,\n",
    "                                     year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'total_uncertainty/' + metric + '.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e3cdc-5cc0-4784-8f7e-afd5f64d8f3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1-day maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a42be14-76c5-45c6-af3e-b3c3ac35d50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.6 s, sys: 5.52 s, total: 1min 1s\n",
      "Wall time: 6min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "metric = 'max'\n",
    "\n",
    "# Dask delayed over years\n",
    "delayed_res = []\n",
    "for year in range(2015, 2100):\n",
    "    # Read all ensembles and compute total uncertainty\n",
    "    tmp_res = dask.delayed(uc_total)(nex_in, nex_models, \n",
    "                                     cil_in, cil_models, \n",
    "                                     isi_in, isi_models, \n",
    "                                     cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                     metric, False,\n",
    "                                     year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'total_uncertainty/' + metric +'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6a732-375d-4abd-aefe-810b447ab92c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Dry days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef513330-ecba-4154-bfc2-33daaefaeed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 16.9 s, total: 1min 53s\n",
      "Wall time: 11min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "metric = 'dry'\n",
    "\n",
    "# Dask delayed over years\n",
    "delayed_res = []\n",
    "for year in range(2015, 2100):\n",
    "    # Read all ensembles and compute total uncertainty\n",
    "    tmp_res = dask.delayed(uc_total)(nex_in, nex_models, \n",
    "                                     cil_in, cil_models, \n",
    "                                     isi_in, isi_models, \n",
    "                                     cbp_in, cbp_gard_precip_models, cbp_deep_models,\n",
    "                                     metric, False,\n",
    "                                     year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'total_uncertainty/' + metric +'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4014b48-519c-4546-be70-9f01b3401784",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Wet days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2abd3c6-a8e8-42d4-9570-d731da1df337",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/carbonplan/native_grid/DeepSD-BC/wet/MRI-ESM2-0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:21\u001b[0m\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/dask/base.py:600\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[1;32m    598\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 600\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/distributed/client.py:3096\u001b[0m, in \u001b[0;36mClient.get\u001b[0;34m(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   3094\u001b[0m         should_rejoin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3095\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3096\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirect\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3097\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   3098\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m futures\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/distributed/client.py:2265\u001b[0m, in \u001b[0;36mClient.gather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   2263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2264\u001b[0m     local_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2266\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gather\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2268\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2271\u001b[0m \u001b[43m    \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/distributed/utils.py:339\u001b[0m, in \u001b[0;36mSyncMethodMixin.sync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/distributed/utils.py:406\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[1;32m    405\u001b[0m     typ, exc, tb \u001b[38;5;241m=\u001b[39m error\n\u001b[0;32m--> 406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/distributed/utils.py:379\u001b[0m, in \u001b[0;36msync.<locals>.f\u001b[0;34m()\u001b[0m\n\u001b[1;32m    377\u001b[0m         future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(future, callback_timeout)\n\u001b[1;32m    378\u001b[0m     future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(future)\n\u001b[0;32m--> 379\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m future\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     error \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/tornado/gen.py:762\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 762\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     exc_info \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/distributed/client.py:2128\u001b[0m, in \u001b[0;36mClient._gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   2126\u001b[0m         exc \u001b[38;5;241m=\u001b[39m CancelledError(key)\n\u001b[1;32m   2127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2128\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m   2129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   2130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn [5], line 85\u001b[0m, in \u001b[0;36muc_total\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m ds_out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m cbp_deep_models:\n\u001b[0;32m---> 85\u001b[0m     ds_out\u001b[38;5;241m.\u001b[39mappend(read_and_process(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeepSD-BC\u001b[39m\u001b[38;5;124m'\u001b[39m, cbp_in \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnative_grid/DeepSD-BC/\u001b[39m\u001b[38;5;124m'\u001b[39m, model, year, metric, submetric))\n\u001b[1;32m     86\u001b[0m ds_cbp_deep \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mconcat(ds_out, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, fill_value\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m###########################\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Merge all and mask ocean\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m###########################\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [5], line 23\u001b[0m, in \u001b[0;36mread_and_process\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     ds \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(path_in \u001b[38;5;241m+\u001b[39m metric \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m model \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ensemble \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCIL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeepSD-BC\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 23\u001b[0m     ds \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(path_in \u001b[38;5;241m+\u001b[39m metric \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m model, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzarr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Select submetric if chosen\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m submetric:\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/xarray/backends/api.py:539\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    527\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    528\u001b[0m     decode_cf,\n\u001b[1;32m    529\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    535\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    536\u001b[0m )\n\u001b[1;32m    538\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 539\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mopen_dataset(\n\u001b[1;32m    540\u001b[0m     filename_or_obj,\n\u001b[1;32m    541\u001b[0m     drop_variables\u001b[38;5;241m=\u001b[39mdrop_variables,\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoders,\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    544\u001b[0m )\n\u001b[1;32m    545\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    546\u001b[0m     backend_ds,\n\u001b[1;32m    547\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    556\u001b[0m )\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/xarray/backends/zarr.py:840\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    822\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    836\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    837\u001b[0m ):\n\u001b[1;32m    839\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 840\u001b[0m     store \u001b[38;5;241m=\u001b[39m ZarrStore\u001b[38;5;241m.\u001b[39mopen_group(\n\u001b[1;32m    841\u001b[0m         filename_or_obj,\n\u001b[1;32m    842\u001b[0m         group\u001b[38;5;241m=\u001b[39mgroup,\n\u001b[1;32m    843\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    844\u001b[0m         synchronizer\u001b[38;5;241m=\u001b[39msynchronizer,\n\u001b[1;32m    845\u001b[0m         consolidated\u001b[38;5;241m=\u001b[39mconsolidated,\n\u001b[1;32m    846\u001b[0m         consolidate_on_close\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    847\u001b[0m         chunk_store\u001b[38;5;241m=\u001b[39mchunk_store,\n\u001b[1;32m    848\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    849\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    850\u001b[0m     )\n\u001b[1;32m    852\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/xarray/backends/zarr.py:404\u001b[0m, in \u001b[0;36mopen_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    388\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    389\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to open Zarr store with consolidated metadata, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    390\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut successfully read with non-consolidated metadata. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    402\u001b[0m             )\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m zarr\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mGroupNotFoundError:\n\u001b[0;32m--> 404\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m consolidated:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# TODO: an option to pass the metadata_key keyword\u001b[39;00m\n\u001b[1;32m    407\u001b[0m     zarr_group \u001b[38;5;241m=\u001b[39m zarr\u001b[38;5;241m.\u001b[39mopen_consolidated(store, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/carbonplan/native_grid/DeepSD-BC/wet/MRI-ESM2-0'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "metric = 'wet'\n",
    "submetric = ['pr_rp5gmfd_count', 'pr_rp5gmfd_streak']\n",
    "\n",
    "# Dask delayed over years\n",
    "delayed_res = []\n",
    "for year in range(2015, 2100):\n",
    "    # Read all ensembles and compute total uncertainty\n",
    "    tmp_res = dask.delayed(uc_total)(nex_in, nex_models, \n",
    "                                     cil_in, cil_models, \n",
    "                                     isi_in, isi_models, \n",
    "                                     cbp_in, cbp_gard_precip_models, cbp_deep_models,\n",
    "                                     metric, submetric,\n",
    "                                     year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'total_uncertainty/' + metric + '_gmfd_rp5.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1909cad3-30ab-440c-b89a-9ea138eae528",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hot days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fa52c48-19f3-466d-ab61-9b0b47f5691b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 45s, sys: 6.89 s, total: 1min 52s\n",
      "Wall time: 12min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hot days: tasmin\n",
    "metric = 'hot'\n",
    "submetric = ['tasmin_rp5gmfd_count', 'tasmin_rp5gmfd_streak']\n",
    "\n",
    "# Dask delayed over years\n",
    "delayed_res = []\n",
    "for year in range(2015, 2100):\n",
    "    # Read all ensembles and compute total uncertainty\n",
    "    tmp_res = dask.delayed(uc_total)(nex_in, [model + '_tasmin' for model in nex_models], \n",
    "                                     cil_in, cil_models, \n",
    "                                     isi_in, [model + '_tasmin' for model in isi_models], \n",
    "                                     cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                     metric, submetric, \n",
    "                                     year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'total_uncertainty/' + metric + '_tasmin.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e731cea0-36b1-4a66-9061-6a9a9c4e2023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 6.94 s, total: 1min 53s\n",
      "Wall time: 10min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hot days: tasmin\n",
    "metric = 'hot'\n",
    "submetric = ['tasmax_rp5gmfd_count', 'tasmax_rp5gmfd_streak']\n",
    "\n",
    "# Dask delayed over years\n",
    "delayed_res = []\n",
    "for year in range(2015, 2100):\n",
    "    # Read all ensembles and compute total uncertainty\n",
    "    tmp_res = dask.delayed(uc_total)(nex_in, [model + '_tasmax' for model in nex_models], \n",
    "                                     cil_in, cil_models, \n",
    "                                     isi_in, [model + '_tasmax' for model in isi_models], \n",
    "                                     cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                     metric, submetric, \n",
    "                                     year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'total_uncertainty/' + metric + '_tasmax.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb443a5-6422-4597-b3b7-3d83e4e857a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# No interannual variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "830e1b1a-ee79-417b-b048-a08c0cc43ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Uncertainty characterization following Hawkins & Sutton 2009 \n",
    "# No consideration of internal variability!\n",
    "################################################################\n",
    "def uc_hs09(nex_in, nex_models, \n",
    "            cil_in, cil_models, \n",
    "            isi_in, isi_models, \n",
    "            cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "            metric, year):\n",
    "    ##################################\n",
    "    # Read and format all ensembles\n",
    "    ##################################\n",
    "    # NEX-GDDP \n",
    "    ds_out = []\n",
    "    for model in nex_models:\n",
    "        ds = xr.open_dataset(nex_in + metric + '/' + model + '.nc')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        ds['lon'] = np.where(ds['lon'] > 180, ds['lon'] - 360, ds['lon'])\n",
    "        ds = ds.sortby('lon')\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = 'NEX')\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(nex_in, '').split('/')[-1][:-3])\n",
    "        ds_out.append(ds)\n",
    "    ds_nex = xr.concat(ds_out, dim='model', compat='identical')\n",
    "\n",
    "    # CIL-GDPCIR\n",
    "    ds_out = []\n",
    "    for model in cil_models:\n",
    "        ds = xr.open_dataset(cil_in + metric + '/' + model, engine='zarr')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "        ds = ds.assign_coords(ensemble = 'CIL')\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(cil_in, '').split('/')[-1])\n",
    "        ds_out.append(ds)\n",
    "    ds_cil = xr.concat(ds_out, dim='model', compat='identical')\n",
    "\n",
    "    # ISIMIP\n",
    "    ds_out = []\n",
    "    for model in isi_models:\n",
    "        ds = xr.open_dataset(isi_in + metric + '/' + model + '.nc')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        ds['lon'] = np.where(ds['lon'] > 180, ds['lon'] - 360, ds['lon'])\n",
    "        ds = ds.sortby('lon')\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = 'ISIMIP')\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(isi_in, '').split('/')[-1][:-3])\n",
    "        ds_out.append(ds)\n",
    "    ds_isi = xr.concat(ds_out, dim='model', compat='identical')\n",
    "\n",
    "    # carbonplan: GARD-SV\n",
    "    ds_out = []\n",
    "    for model in cbp_gard_models:\n",
    "        ds = xr.open_dataset(cbp_in + 'GARD-SV/' + metric + '/' + model + '.nc')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = 'GARD-SV')\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(cbp_in, '').split('/')[-1][:-3])\n",
    "        # for some models/methods we are missing \n",
    "        # precip so need to fill with NaNs\n",
    "        if 'pr' not in ds.data_vars:\n",
    "            ds['pr'] = xr.full_like(ds['tas'], np.NaN)\n",
    "        ds_out.append(ds)\n",
    "    ds_cbp_gard = xr.concat(ds_out, dim='model', compat='identical')\n",
    "    \n",
    "    # carbonplan: DeepSD-BC\n",
    "    ds_out = []\n",
    "    for model in cbp_deep_models:\n",
    "        ds = xr.open_dataset(cbp_in + 'DeepSD-BC/' + metric + '/' + model + '.nc')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = 'DeepSD-BC')\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(cbp_in, '').split('/')[-1][:-3])\n",
    "        # for some models/methods we are missing \n",
    "        # precip so need to fill with NaNs\n",
    "        if 'pr' not in ds.data_vars:\n",
    "            ds['pr'] = xr.full_like(ds['tas'], np.NaN)\n",
    "        ds_out.append(ds)\n",
    "    ds_cbp_deep = xr.concat(ds_out, dim='model', compat='identical')\n",
    "\n",
    "    ###########################\n",
    "    # Merge all and mask ocean\n",
    "    ###########################\n",
    "    ds = xr.concat([ds_nex, ds_cil, ds_isi, ds_cbp_gard, ds_cbp_deep],\n",
    "                       dim='ensemble', fill_value=np.nan)\n",
    "    \n",
    "    # mask out ocean points (NEX is only available over land)\n",
    "    ds_mask = ds.sel(ensemble='NEX').isel(ssp=0, model=0)[list(ds.keys())[0]].isnull()\n",
    "    ds = xr.where(ds_mask, np.nan, ds)\n",
    "    \n",
    "    ##########################\n",
    "    # Uncertainty calculation\n",
    "    ##########################\n",
    "    ##  Model uncertainty\n",
    "    # Variance across models, averaged over scenarios and ensembles\n",
    "    U_model = ds.var(dim='model')\n",
    "    weights = ds.isel(lat=300, lon=800)[list(ds.data_vars)[0]].count(dim='model').rename('weights')     # weights (choose point over land)\n",
    "    weights = xr.where(weights == 1, 0, weights) # remove combinations where variance was calculated over 1 entry\n",
    "    U_model = U_model.weighted(weights).mean(dim=['ssp', 'ensemble']) # weighted average\n",
    "\n",
    "    ## Scenario uncertainty\n",
    "    # Variance across multi-model means (HS09 approach)\n",
    "    U_scen = ds.mean(dim=['model', 'ensemble']).var(dim='ssp')\n",
    "\n",
    "    ## Downscaling uncertainy\n",
    "    # Variance across ensembles, averaged over models and scenarios\n",
    "    U_ens = ds.var(dim='ensemble')\n",
    "    weights = ds.isel(lat=300, lon=800)[list(ds.data_vars)[0]].count(dim='ensemble').rename('weights') # weights\n",
    "    weights = xr.where(weights == 1, 0, weights) # remove combinations where variance was calculated over 1 entry\n",
    "    U_ens = U_ens.weighted(weights).mean(dim=['ssp', 'model'])\n",
    "\n",
    "    ## Total uncertainty    \n",
    "    # Our 'simulated' total uncertainty\n",
    "    # This will in general not equal true total\n",
    "    U_total = U_model + U_scen + U_ens\n",
    "\n",
    "    ## Merge and return\n",
    "    U_model = U_model.assign_coords(uncertainty = 'model')\n",
    "    U_scen = U_scen.assign_coords(uncertainty = 'scenario')\n",
    "    U_ens = U_ens.assign_coords(uncertainty = 'ensemble')\n",
    "    U_total = U_total_sim.assign_coords(uncertainty = 'total')\n",
    "    \n",
    "    return xr.concat([U_model, U_scen, U_ens, U_total, U_total], dim='uncertainty')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b61a306-665b-40c4-8024-57c20a3364ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Annual averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbeead59-5a74-4f7b-89c3-f0e819715d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'avg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84b78327-13ae-472f-93a7-4b2c28ec4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "################################\n",
    "# UC on raw outputs (no iav)\n",
    "################################\n",
    "delayed_res = []\n",
    "for year in range(2015, 2101):\n",
    "    # Read all ensembles and compute UC\n",
    "    tmp_res = dask.delayed(uc_hs09)(nex_in, nex_models, \n",
    "                                    cil_in, cil_models, \n",
    "                                    isi_in, isi_models, \n",
    "                                    cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                    metric, year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'hs09_no_iav/' + metric +'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25831023-fd72-43ee-b37e-004bcf01683f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Annual maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56c04bef-2ae7-480f-abcd-b82482b2d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff0ebc7a-482d-482f-8ff4-fbd96058a869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 s, sys: 9.44 s, total: 24.8 s\n",
      "Wall time: 7min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################################\n",
    "# UC on raw outputs (no iav)\n",
    "################################\n",
    "delayed_res = []\n",
    "for year in range(2015, 2101):\n",
    "    # Read all ensembles and compute UC\n",
    "    tmp_res = dask.delayed(uc_hs09)(nex_in, nex_models, \n",
    "                                    cil_in, cil_models, \n",
    "                                    isi_in, isi_models, \n",
    "                                    cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                    metric, year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'hs09_no_iav/' + metric +'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0791ed2a-a415-44a6-ba57-6ec6589e454c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Interannual variability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f835b0-9cf4-46f0-8167-189063ca1d97",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f612e0f3-8e82-4d6d-847b-19eae4bac017",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Uncertainty characterization following Hawkins & Sutton 2009 \n",
    "# 'Forced response' = 10 year rolling mean\n",
    "################################################################\n",
    "def uc_hs09_forced(nex_in, nex_models, \n",
    "                   cil_in, cil_models, \n",
    "                   isi_in, isi_models, \n",
    "                   cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                   metric, submetric, \n",
    "                   year):\n",
    "    \"\"\"\n",
    "    Reads in all models, ssps, and calculates the uncertainty in the 'forced response'\n",
    "    (10 year rolling mean) along each dimension for a given year (and possibly DataArray).\n",
    "    For metrics like 'hot' where there are several sub-metrics based on different \n",
    "    thresholds and/or observational data, we need to select a specific DataArray\n",
    "    to keep the memory manageable.\n",
    "    \"\"\"\n",
    "    # Subfunction for general preprocessing of each model/ensemble\n",
    "    def read_and_process(ensemble, path_in, model, year, metric, submetric):\n",
    "        # Read netcdf or zarr\n",
    "        if ensemble in ['NEX', 'ISIMIP', 'GARD-SV']:\n",
    "            ds = xr.open_dataset(path_in + metric + '/' + model + '.nc')\n",
    "        elif ensemble in ['CIL', 'DeepSD-BC']:\n",
    "            ds = xr.open_dataset(path_in + metric + '/' + model, engine='zarr')\n",
    "            \n",
    "        \n",
    "        # Select submetric if chosen\n",
    "        if submetric:\n",
    "            ds = ds[submetric]\n",
    "    \n",
    "        # Common preprocessing\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=slice(year-6, year+6)) # faster rolling mean\n",
    "        ds = ds.rolling(time=10, center=True).mean().sel(time=year)\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = ensemble)\n",
    "        ds = ds.assign_coords(model = model)\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "    \n",
    "        # Fix lon to [-180,180]\n",
    "        if ensemble in ['NEX', 'ISIMIP']:\n",
    "            ds['lon'] = np.where(ds['lon'] > 180, ds['lon'] - 360, ds['lon'])\n",
    "            ds = ds.sortby('lon')\n",
    "    \n",
    "        # Some models/methods are missing precip so fill with NaNs\n",
    "        if (metric in ['max', 'avg']) and ('pr' not in ds.data_vars):\n",
    "            ds['pr'] = xr.full_like(ds[list(ds.data_vars)[0]], np.nan)\n",
    "    \n",
    "        # Return\n",
    "        return ds\n",
    "\n",
    "    ######################\n",
    "    # Read all ensembles\n",
    "    ######################\n",
    "    # NEX-GDDP \n",
    "    ds_out = []\n",
    "    for model in nex_models:\n",
    "        ds_out.append(read_and_process('NEX', nex_in, model, year, metric, submetric))\n",
    "    ds_nex = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "\n",
    "    # CIL-GDPCIR\n",
    "    ds_out = []\n",
    "    for model in cil_models:\n",
    "        ds_out.append(read_and_process('CIL', cil_in, model, year, metric, submetric))\n",
    "    ds_cil = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "\n",
    "    # ISIMIP\n",
    "    ds_out = []\n",
    "    for model in isi_models:\n",
    "        ds_out.append(read_and_process('ISIMIP', isi_in, model, year, metric, submetric))\n",
    "    ds_isi = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "\n",
    "    # carbonplan: GARD-SV\n",
    "    ds_out = []\n",
    "    for model in cbp_gard_models:\n",
    "        ds_out.append(read_and_process('GARD-SV', cbp_in + '/regridded/conservative/GARD-SV/', model, year, metric, submetric))\n",
    "    ds_cbp_gard = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "    \n",
    "    # carbonplan: DeepSD-BC\n",
    "    ds_out = []\n",
    "    for model in cbp_deep_models:\n",
    "        ds_out.append(read_and_process('DeepSD-BC', cbp_in + 'native_grid/DeepSD-BC/', model, year, metric, submetric))\n",
    "    ds_cbp_deep = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "\n",
    "    ###########################\n",
    "    # Merge all and mask ocean\n",
    "    ###########################\n",
    "    ds = xr.concat([ds_nex, ds_cil, ds_isi, ds_cbp_gard, ds_cbp_deep],\n",
    "                       dim='ensemble', fill_value=np.nan)\n",
    "    \n",
    "    # mask out ocean points (NEX is only available over land)\n",
    "    ds_mask = ds.sel(ensemble='NEX').isel(ssp=0, model=0)[list(ds.keys())[0]].isnull()\n",
    "    ds = xr.where(ds_mask, np.nan, ds)\n",
    "    \n",
    "    ##########################\n",
    "    # Uncertainty calculation\n",
    "    ##########################\n",
    "    ##  Model uncertainty\n",
    "    # Variance across models, averaged over scenarios and ensembles\n",
    "    U_model = ds.var(dim='model')\n",
    "    weights = ds.isel(lat=300, lon=800)[list(ds.data_vars)[0]].count(dim='model').rename('weights') # weights (choose point over land)\n",
    "    weights = xr.where(weights == 1, 0, weights) # remove combinations where variance was calculated over 1 entry\n",
    "    U_model = U_model.weighted(weights).mean(dim=['ssp', 'ensemble']) # weighted average\n",
    "\n",
    "    ## Scenario uncertainty\n",
    "    # Variance across multi-model means (HS09 approach)\n",
    "    U_scen = ds.mean(dim=['model', 'ensemble']).var(dim='ssp')\n",
    "\n",
    "    ## Downscaling uncertainy\n",
    "    # Variance across ensembles, averaged over models and scenarios\n",
    "    U_ens = ds.var(dim='ensemble')\n",
    "    weights = ds.isel(lat=300, lon=800)[list(ds.data_vars)[0]].count(dim='ensemble').rename('weights') # weights\n",
    "    weights = xr.where(weights == 1, 0, weights) # remove combinations where variance was calculated over 1 entry\n",
    "    U_ens = U_ens.weighted(weights).mean(dim=['ssp', 'model'])\n",
    "\n",
    "    ## Merge and return\n",
    "    U_model = U_model.assign_coords(uncertainty = 'model')\n",
    "    U_scen = U_scen.assign_coords(uncertainty = 'scenario')\n",
    "    U_ens = U_ens.assign_coords(uncertainty = 'ensemble')\n",
    "    \n",
    "    return xr.concat([U_model, U_scen, U_ens], dim='uncertainty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b210d705-e5cf-4ff4-9a47-fcc171bc699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Uncertainty characterization following Hawkins & Sutton 2009 \n",
    "# Interannual variability (single value for all years)\n",
    "################################################################\n",
    "def uc_hs09_iav(path_in, ensemble, model, metric, submetric):\n",
    "    \"\"\"\n",
    "    Calculates the internal variability (variance over all years\n",
    "    of residuals from rolling mean) for a given model-ssp-ensemble\n",
    "    \"\"\"\n",
    "    # Subfunction for general preprocessing of each model/ensemble\n",
    "    def read_and_process(ensemble, path_in, model, metric, submetric):\n",
    "        # Read netcdf or zarr\n",
    "        if ensemble in ['NEX', 'ISIMIP', 'GARD-SV']:\n",
    "            ds = xr.open_dataset(path_in + metric + '/' + model + '.nc')\n",
    "        elif ensemble in ['CIL', 'DeepSD-BC']:\n",
    "            ds = xr.open_dataset(path_in + metric + '/' + model, engine='zarr')\n",
    "            \n",
    "        # Select submetric if chosen\n",
    "        if submetric:\n",
    "            ds = ds[submetric]\n",
    "    \n",
    "        # Common preprocessing\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensmod = ensemble + '__' + model)\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "    \n",
    "        # Fix lon to [-180,180]\n",
    "        if ensemble in ['NEX', 'ISIMIP']:\n",
    "            ds['lon'] = np.where(ds['lon'] > 180, ds['lon'] - 360, ds['lon'])\n",
    "            ds = ds.sortby('lon')\n",
    "    \n",
    "        # Some models/methods are missing precip so fill with NaNs\n",
    "        if (metric in ['max', 'avg']) and ('pr' not in ds.data_vars):\n",
    "            ds['pr'] = xr.full_like(ds[list(ds.data_vars)[0]], np.nan)\n",
    "    \n",
    "        # Return\n",
    "        return ds\n",
    "\n",
    "    ###############\n",
    "    # Read model\n",
    "    ###############\n",
    "    ds = read_and_process(ensemble, path_in, model, metric, submetric)\n",
    "            \n",
    "    #####################################\n",
    "    # Get IAV estimate\n",
    "    # Variance of rolling mean residuals\n",
    "    #####################################\n",
    "    ds_rolling = ds.rolling(time=10, center=True).mean().sel(time=slice(2020,2096))\n",
    "    return (ds - ds_rolling).var(dim='time')\n",
    "\n",
    "\n",
    "\n",
    "def make_delayed_list_iav(metric, submetric, submetric_var):\n",
    "    \"\"\"\n",
    "    Make a delayed list with IAV of all models-ssps-ensembles which \n",
    "    can then be combined into one dataset and averaged for best estimate.\n",
    "    \"\"\"\n",
    "    # Parallelize with dask over models\n",
    "    delayed_res = []\n",
    "    \n",
    "    # NEX\n",
    "    if submetric_var:\n",
    "        models = [model + '_' + submetric_var for model in nex_models]\n",
    "    for model in models:\n",
    "        tmp_res = dask.delayed(uc_hs09_iav)(nex_in, 'NEX', model, metric, submetric)\n",
    "        delayed_res.append(tmp_res)\n",
    "        \n",
    "    # CIL\n",
    "    for model in cil_models:\n",
    "        tmp_res = dask.delayed(uc_hs09_iav)(cil_in, 'CIL', model, metric, submetric)\n",
    "        delayed_res.append(tmp_res)\n",
    "        \n",
    "    # ISIMIP\n",
    "    if submetric_var:\n",
    "        models = [model + '_' + submetric_var for model in isi_models]\n",
    "    for model in models:\n",
    "        tmp_res = dask.delayed(uc_hs09_iav)(isi_in, 'ISIMIP', model, metric, submetric)\n",
    "        delayed_res.append(tmp_res)\n",
    "        \n",
    "    # carbonplan GARD-SV\n",
    "    if metric in ['wet', 'dry']:\n",
    "        models = cbp_gard_precip_models\n",
    "    else:\n",
    "        models = cbp_gard_models\n",
    "        \n",
    "    for model in models:\n",
    "        tmp_res = dask.delayed(uc_hs09_iav)(cbp_in + '/regridded/conservative/GARD-SV/', 'GARD-SV', model, metric, submetric)\n",
    "        delayed_res.append(tmp_res)\n",
    "        \n",
    "    # carbonplan DeepSD-BC\n",
    "    for model in cbp_deep_models:\n",
    "        tmp_res = dask.delayed(uc_hs09_iav)(cbp_in + 'native_grid/DeepSD-BC/', 'DeepSD-BC', model, metric, submetric)\n",
    "        delayed_res.append(tmp_res)\n",
    "        \n",
    "    # return\n",
    "    return delayed_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a15c7-7b8c-4005-8855-a771f4649fc3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Annual averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79c75f14-322b-4e1d-b642-e429e7fbdaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'avg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39e8e9-7b05-40ad-b5eb-56e945cffd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "################################\n",
    "# Interannual variability\n",
    "################################\n",
    "delayed_res = make_delayed_list_iav(metric, False)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and average over ensemble + model (ensmod) and ssp\n",
    "ds_out = xr.concat(res, dim='ensmod').mean(dim=['ensmod', 'ssp'])\n",
    "ds_out.to_netcdf(out_path + 'hs09_iav/' + metric +'_iav.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750aae98-913c-48e6-9868-201d7bd01b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "################################\n",
    "# UC on forced response\n",
    "################################\n",
    "delayed_res = []\n",
    "for year in range(2020, 2097):\n",
    "    # Read all ensembles and compute UC\n",
    "    tmp_res = dask.delayed(uc_hs09_forced)(nex_in, nex_models, \n",
    "                                           cil_in, cil_models, \n",
    "                                           isi_in, isi_models, \n",
    "                                           cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                           metric, False,\n",
    "                                           year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'hs09_iav/' + metric +'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175dd321-2169-466e-8cef-ed20d0ede997",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1-day maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a260c81-0ea8-4510-84cc-8fd7f0490d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05c78b45-049e-4665-becf-f294c8a96bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.1 s, sys: 15 s, total: 1min 4s\n",
      "Wall time: 4min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################################\n",
    "# Interannual variability\n",
    "################################\n",
    "delayed_res = make_delayed_list_iav(metric, False)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and average over ensemble + model (ensmod) and ssp\n",
    "ds_out = xr.concat(res, dim='ensmod').mean(dim=['ensmod', 'ssp'])\n",
    "ds_out.to_netcdf(out_path + 'hs09_iav/' + metric +'_iav.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "702d89a9-23cd-4d42-84e8-fd039d6e2c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 43s, sys: 26.2 s, total: 3min 9s\n",
      "Wall time: 17min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################################\n",
    "# UC on forced response\n",
    "################################\n",
    "delayed_res = []\n",
    "for year in range(2020, 2097):\n",
    "    # Read all ensembles and compute UC\n",
    "    tmp_res = dask.delayed(uc_hs09_forced)(nex_in, nex_models, \n",
    "                                           cil_in, cil_models, \n",
    "                                           isi_in, isi_models, \n",
    "                                           cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                           metric, False,\n",
    "                                           year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'hs09_iav/' + metric +'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a85a3-825c-4964-87e4-9387da6ed0c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dry days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef59571a-64f4-4247-b3ae-1219c9d0fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'dry'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fdbe6a-51d4-499e-bdb0-da7a1ace75c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "################################\n",
    "# Interannual variability\n",
    "################################\n",
    "delayed_res = make_delayed_list_iav(metric, False)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and average over ensemble + model (ensmod) and ssp\n",
    "ds_out = xr.concat(res, dim='ensmod').mean(dim=['ensmod', 'ssp'])\n",
    "ds_out.to_netcdf(out_path + 'hs09_iav/' + metric +'_iav.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bfb575-dbdd-4611-8778-1aed2a926d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "################################\n",
    "# UC on forced response\n",
    "################################\n",
    "delayed_res = []\n",
    "for year in range(2020, 2097):\n",
    "    # Read all ensembles and compute UC\n",
    "    tmp_res = dask.delayed(uc_hs09_forced)(nex_in, nex_models, \n",
    "                                           cil_in, cil_models, \n",
    "                                           isi_in, isi_models, \n",
    "                                           cbp_in, cbp_gard_precip_models, cbp_deep_models,\n",
    "                                           metric, False,\n",
    "                                           year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'hs09_iav/' + metric +'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfa8f6f-b1ef-44e0-a31d-0abb5721a631",
   "metadata": {},
   "source": [
    "## Hot days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c39e2-0f7a-4db3-8315-af9f89ce7e2b",
   "metadata": {},
   "source": [
    "### Tasmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a40955ae-268a-40ea-b712-de0c1c5e167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'hot'\n",
    "submetric = ['tasmax_rp5gmfd_count', 'tasmax_rp5gmfd_streak']\n",
    "submetric_var = 'tasmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd6e7f-731c-4ee7-b13f-c135d378c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "################################\n",
    "# Interannual variability\n",
    "################################\n",
    "delayed_res = make_delayed_list_iav(metric, submetric, submetric_var)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and average over ensemble + model (ensmod) and ssp\n",
    "ds_out = xr.concat(res, dim='ensmod').mean(dim=['ensmod', 'ssp'])\n",
    "ds_out.to_netcdf(out_path + 'hs09_iav/' + metric + '_' + submetric_var + '_iav.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53692b49-afbb-4ac7-a078-876d36607f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "################################\n",
    "# UC on forced response\n",
    "################################\n",
    "delayed_res = []\n",
    "for year in range(2020, 2097):\n",
    "    # Read all ensembles and compute UC\n",
    "    tmp_res = dask.delayed(uc_hs09_forced)(nex_in, [model + '_' + submetric_var for model in nex_models], \n",
    "                                           cil_in, cil_models, \n",
    "                                           isi_in, [model + '_' + submetric_var for model in isi_models], \n",
    "                                           cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                           metric, submetric,\n",
    "                                           year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'hs09_iav/' + metric + '_' + submetric_var +  '.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
