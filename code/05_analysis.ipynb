{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57743a7e-8b68-4544-8378-ac3e453f96bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f3946-bb89-4535-a1e8-ae9b6ccaed61",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662dfbf5-61c7-48f6-884d-ce058d6a7512",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Set paths\n",
    "# UPDATE THIS FOR REPRODUCTION\n",
    "###############################\n",
    "nex_in = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/nex-gddp/'\n",
    "cil_in = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/cil-gdpcir/'\n",
    "isi_in = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/isimip3b/regridded/conservative/'\n",
    "cbp_in = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/carbonplan/regridded/conservative/'\n",
    "\n",
    "out_path = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/uc_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c50de8a-ea1e-4a9d-ad37-c522111ff76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Models\n",
    "###################\n",
    "from utils import nex_ssp_dict, cil_ssp_dict, isimip_ssp_dict, gardsv_ssp_dict, deepsdbc_dict\n",
    "\n",
    "nex_models = list(nex_ssp_dict.keys())\n",
    "cil_models = list(cil_ssp_dict.keys())\n",
    "isi_models = list(isimip_ssp_dict.keys())\n",
    "cbp_gard_models = list(gardsv_ssp_dict.keys())\n",
    "cbp_deep_models = list(deepsdbc_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f9f2833-ddc0-4341-8a40-f7f142f4e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(nex_in + 'avg' + '/' + nex_models[0] + '.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1c47d5-5fb5-4648-aac2-a86cbb62fdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-8f3992cd-5ec6-11ed-addd-34e6d79eac50</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_jobqueue.PBSCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">PBSCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">63534e7b</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-d1a0e735-8eac-42de-a526-95c3755deebd</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://10.102.201.236:32914\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.102.201.236:32914' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############\n",
    "# Dask\n",
    "############\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "cluster = PBSCluster(cores=1, memory='40GB', resource_spec='pmem=40GB',\n",
    "                     account='open',\n",
    "                     worker_extra_args=['#PBS -l feature=rhel7'], \n",
    "                     walltime='00:10:00')\n",
    "\n",
    "cluster.scale(jobs=30)  # ask for jobs\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f20699-35c7-4dbf-a8ad-ad73115785df",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Total uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a2c6626-3157-4eee-9847-5df9e74f459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Total uncertainty: variance across all models, scenarios, ensembles \n",
    "#######################################################################\n",
    "def uc_total(nex_in, nex_models, \n",
    "             cil_in, cil_models, \n",
    "             isi_in, isi_models, \n",
    "             cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "             metric, year):\n",
    "    ######################\n",
    "    # Read all ensembles\n",
    "    ######################\n",
    "    # NEX-GDDP \n",
    "    ds_out = []\n",
    "    for model in nex_models:\n",
    "        ds = xr.open_dataset(nex_in + metric + '/' + model + '.nc')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        ds['lon'] = np.where(ds['lon'] > 180, ds['lon'] - 360, ds['lon'])\n",
    "        ds = ds.sortby('lon')\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = 'NEX')\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(nex_in, '').split('/')[-1][:-3])\n",
    "        ds_out.append(ds)\n",
    "    ds_nex = xr.concat(ds_out, dim='model', compat='identical')\n",
    "\n",
    "    # CIL-GDPCIR\n",
    "    ds_out = []\n",
    "    for model in cil_models:\n",
    "        ds = xr.open_dataset(cil_in + metric + '/' + model, engine='zarr')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "        ds = ds.assign_coords(ensemble = 'CIL')\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(cil_in, '').split('/')[-1])\n",
    "        ds_out.append(ds)\n",
    "    ds_cil = xr.concat(ds_out, dim='model', compat='identical')\n",
    "\n",
    "    # ISIMIP\n",
    "    ds_out = []\n",
    "    for model in isi_models:\n",
    "        ds = xr.open_dataset(isi_in + metric + '/' + model + '.nc')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        ds['lon'] = np.where(ds['lon'] > 180, ds['lon'] - 360, ds['lon'])\n",
    "        ds = ds.sortby('lon')\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = 'ISIMIP')\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(isi_in, '').split('/')[-1][:-3])\n",
    "        ds_out.append(ds)\n",
    "    ds_isi = xr.concat(ds_out, dim='model', compat='identical')\n",
    "\n",
    "    # carbonplan: GARD-SV\n",
    "    ds_out = []\n",
    "    for model in cbp_gard_models:\n",
    "        ds = xr.open_dataset(cbp_in + 'GARD-SV/' + metric + '/' + model + '.nc')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = 'GARD-SV')\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(cbp_in, '').split('/')[-1][:-3])\n",
    "        # for some models/methods we are missing \n",
    "        # precip so need to fill with NaNs\n",
    "        if 'pr' not in ds.data_vars:\n",
    "            ds['pr'] = xr.full_like(ds['tas'], np.NaN)\n",
    "        ds_out.append(ds)\n",
    "    ds_cbp_gard = xr.concat(ds_out, dim='model', compat='identical')\n",
    "    \n",
    "    # carbonplan: DeepSD-BC\n",
    "    ds_out = []\n",
    "    for model in cbp_deep_models:\n",
    "        ds = xr.open_dataset(cbp_in + 'DeepSD-BC/' + metric + '/' + model + '.nc')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = 'DeepSD-BC')\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(cbp_in, '').split('/')[-1][:-3])\n",
    "        # for some models/methods we are missing \n",
    "        # precip so need to fill with NaNs\n",
    "        if 'pr' not in ds.data_vars:\n",
    "            ds['pr'] = xr.full_like(ds['tas'], np.NaN)\n",
    "        ds_out.append(ds)\n",
    "    ds_cbp_deep = xr.concat(ds_out, dim='model', compat='identical')\n",
    "\n",
    "    ###########################\n",
    "    # Merge all and mask ocean\n",
    "    ###########################\n",
    "    ds = xr.concat([ds_nex, ds_cil, ds_isi, ds_cbp_gard, ds_cbp_deep],\n",
    "                       dim='ensemble', fill_value=np.nan)\n",
    "    \n",
    "    # mask out ocean points (NEX is only available over land)\n",
    "    ds_mask = ds.sel(ensemble='NEX').isel(ssp=0, model=0)[list(ds.keys())[0]].isnull()\n",
    "    ds = xr.where(ds_mask, np.nan, ds)\n",
    "    \n",
    "    ##########################\n",
    "    # Uncertainty calculation\n",
    "    ##########################\n",
    "    ## Total uncertainty\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        U_total_true = ds.var(dim=['ensemble', 'ssp', 'model']) # throws warning when all NaNs\n",
    "\n",
    "    U_total_true = U_total_true.assign_coords(uncertainty = 'total_true')\n",
    "    \n",
    "    return U_total_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae5b42a-fc3e-49d4-8763-7a7eb1840181",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Annual averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "772ccfcb-6e14-4a7b-8556-3c586a71d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'annual_avgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04bf260-eb27-486e-b914-20d479e2b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "################################\n",
    "# UC on raw outputs (no iav)\n",
    "################################\n",
    "delayed_res = []\n",
    "for year in range(2015, 2100):\n",
    "    # Read all ensembles and compute UC\n",
    "    tmp_res = dask.delayed(uc_total)(nex_in, nex_models, \n",
    "                                     cil_in, cil_models, \n",
    "                                     isi_in, isi_models, \n",
    "                                     cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                     metric, year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'total_uncertainty/' + metric +'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e3cdc-5cc0-4784-8f7e-afd5f64d8f3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Annual maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47d78034-3f02-4420-bd64-6535a9695f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'annual_maxs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a42be14-76c5-45c6-af3e-b3c3ac35d50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 5.73 s, total: 1min 13s\n",
      "Wall time: 8min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################################\n",
    "# UC on raw outputs (no iav)\n",
    "################################\n",
    "delayed_res = []\n",
    "for year in range(2015, 2100):\n",
    "    # Read all ensembles and compute UC\n",
    "    tmp_res = dask.delayed(uc_total)(nex_in, nex_models, \n",
    "                                     cil_in, cil_models, \n",
    "                                     isi_in, isi_models, \n",
    "                                     cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                     metric, year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'total_uncertainty/' + metric +'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb443a5-6422-4597-b3b7-3d83e4e857a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# No interannual variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "830e1b1a-ee79-417b-b048-a08c0cc43ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Uncertainty characterization following Hawkins & Sutton 2009 \n",
    "# No consideration of internal variability!\n",
    "################################################################\n",
    "def uc_hs09(nex_in, nex_models, \n",
    "            cil_in, cil_models, \n",
    "            isi_in, isi_models, \n",
    "            cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "            metric, year):\n",
    "    ##################################\n",
    "    # Read and format all ensembles\n",
    "    ##################################\n",
    "    # NEX-GDDP \n",
    "    ds_out = []\n",
    "    for model in nex_models:\n",
    "        ds = xr.open_dataset(nex_in + metric + '/' + model + '.nc')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        ds['lon'] = np.where(ds['lon'] > 180, ds['lon'] - 360, ds['lon'])\n",
    "        ds = ds.sortby('lon')\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = 'NEX')\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(nex_in, '').split('/')[-1][:-3])\n",
    "        ds_out.append(ds)\n",
    "    ds_nex = xr.concat(ds_out, dim='model', compat='identical')\n",
    "\n",
    "    # CIL-GDPCIR\n",
    "    ds_out = []\n",
    "    for model in cil_models:\n",
    "        ds = xr.open_dataset(cil_in + metric + '/' + model, engine='zarr')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "        ds = ds.assign_coords(ensemble = 'CIL')\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(cil_in, '').split('/')[-1])\n",
    "        ds_out.append(ds)\n",
    "    ds_cil = xr.concat(ds_out, dim='model', compat='identical')\n",
    "\n",
    "    # ISIMIP\n",
    "    ds_out = []\n",
    "    for model in isi_models:\n",
    "        ds = xr.open_dataset(isi_in + metric + '/' + model + '.nc')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        ds['lon'] = np.where(ds['lon'] > 180, ds['lon'] - 360, ds['lon'])\n",
    "        ds = ds.sortby('lon')\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = 'ISIMIP')\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(isi_in, '').split('/')[-1][:-3])\n",
    "        ds_out.append(ds)\n",
    "    ds_isi = xr.concat(ds_out, dim='model', compat='identical')\n",
    "\n",
    "    # carbonplan: GARD-SV\n",
    "    ds_out = []\n",
    "    for model in cbp_gard_models:\n",
    "        ds = xr.open_dataset(cbp_in + 'GARD-SV/' + metric + '/' + model + '.nc')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = 'GARD-SV')\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(cbp_in, '').split('/')[-1][:-3])\n",
    "        # for some models/methods we are missing \n",
    "        # precip so need to fill with NaNs\n",
    "        if 'pr' not in ds.data_vars:\n",
    "            ds['pr'] = xr.full_like(ds['tas'], np.NaN)\n",
    "        ds_out.append(ds)\n",
    "    ds_cbp_gard = xr.concat(ds_out, dim='model', compat='identical')\n",
    "    \n",
    "    # carbonplan: DeepSD-BC\n",
    "    ds_out = []\n",
    "    for model in cbp_deep_models:\n",
    "        ds = xr.open_dataset(cbp_in + 'DeepSD-BC/' + metric + '/' + model + '.nc')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = 'DeepSD-BC')\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(cbp_in, '').split('/')[-1][:-3])\n",
    "        # for some models/methods we are missing \n",
    "        # precip so need to fill with NaNs\n",
    "        if 'pr' not in ds.data_vars:\n",
    "            ds['pr'] = xr.full_like(ds['tas'], np.NaN)\n",
    "        ds_out.append(ds)\n",
    "    ds_cbp_deep = xr.concat(ds_out, dim='model', compat='identical')\n",
    "\n",
    "    ###########################\n",
    "    # Merge all and mask ocean\n",
    "    ###########################\n",
    "    ds = xr.concat([ds_nex, ds_cil, ds_isi, ds_cbp_gard, ds_cbp_deep],\n",
    "                       dim='ensemble', fill_value=np.nan)\n",
    "    \n",
    "    # mask out ocean points (NEX is only available over land)\n",
    "    ds_mask = ds.sel(ensemble='NEX').isel(ssp=0, model=0)[list(ds.keys())[0]].isnull()\n",
    "    ds = xr.where(ds_mask, np.nan, ds)\n",
    "    \n",
    "    ##########################\n",
    "    # Uncertainty calculation\n",
    "    ##########################\n",
    "    ##  Model uncertainty\n",
    "    # Variance across models, averaged over scenarios and ensembles\n",
    "    U_model = ds.var(dim='model')\n",
    "    weights = ds.isel(lat=300, lon=800)[list(ds.data_vars)[0]].count(dim='model').rename('weights')     # weights (choose point over land)\n",
    "    weights = xr.where(weights == 1, 0, weights) # remove combinations where variance was calculated over 1 entry\n",
    "    U_model = U_model.weighted(weights).mean(dim=['ssp', 'ensemble']) # weighted average\n",
    "\n",
    "    ## Scenario uncertainty\n",
    "    # Variance across multi-model means (HS09 approach)\n",
    "    U_scen = ds.mean(dim=['model', 'ensemble']).var(dim='ssp')\n",
    "\n",
    "    ## Downscaling uncertainy\n",
    "    # Variance across ensembles, averaged over models and scenarios\n",
    "    U_ens = ds.var(dim='ensemble')\n",
    "    weights = ds.isel(lat=300, lon=800)[list(ds.data_vars)[0]].count(dim='ensemble').rename('weights') # weights\n",
    "    weights = xr.where(weights == 1, 0, weights) # remove combinations where variance was calculated over 1 entry\n",
    "    U_ens = U_ens.weighted(weights).mean(dim=['ssp', 'model'])\n",
    "\n",
    "    ## Total uncertainty    \n",
    "    # Our 'simulated' total uncertainty\n",
    "    # This will in general not equal true total\n",
    "    U_total = U_model + U_scen + U_ens\n",
    "\n",
    "    ## Merge and return\n",
    "    U_model = U_model.assign_coords(uncertainty = 'model')\n",
    "    U_scen = U_scen.assign_coords(uncertainty = 'scenario')\n",
    "    U_ens = U_ens.assign_coords(uncertainty = 'ensemble')\n",
    "    U_total = U_total_sim.assign_coords(uncertainty = 'total')\n",
    "    \n",
    "    return xr.concat([U_model, U_scen, U_ens, U_total, U_total], dim='uncertainty')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b61a306-665b-40c4-8024-57c20a3364ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Annual averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbeead59-5a74-4f7b-89c3-f0e819715d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'annual_avgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84b78327-13ae-472f-93a7-4b2c28ec4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "################################\n",
    "# UC on raw outputs (no iav)\n",
    "################################\n",
    "delayed_res = []\n",
    "for year in range(2015, 2101):\n",
    "    # Read all ensembles and compute UC\n",
    "    tmp_res = dask.delayed(uc_hs09)(nex_in, nex_models, \n",
    "                                    cil_in, cil_models, \n",
    "                                    isi_in, isi_models, \n",
    "                                    cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                    metric, year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'hs09_no_iav/' + metric +'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25831023-fd72-43ee-b37e-004bcf01683f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Annual maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56c04bef-2ae7-480f-abcd-b82482b2d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'annual_maxs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff0ebc7a-482d-482f-8ff4-fbd96058a869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 s, sys: 9.44 s, total: 24.8 s\n",
      "Wall time: 7min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################################\n",
    "# UC on raw outputs (no iav)\n",
    "################################\n",
    "delayed_res = []\n",
    "for year in range(2015, 2101):\n",
    "    # Read all ensembles and compute UC\n",
    "    tmp_res = dask.delayed(uc_hs09)(nex_in, nex_models, \n",
    "                                    cil_in, cil_models, \n",
    "                                    isi_in, isi_models, \n",
    "                                    cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                    metric, year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'hs09_no_iav/' + metric +'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0791ed2a-a415-44a6-ba57-6ec6589e454c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Interannual variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f612e0f3-8e82-4d6d-847b-19eae4bac017",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Uncertainty characterization following Hawkins & Sutton 2009 \n",
    "# 'Forced response' = 10 year rolling mean\n",
    "################################################################\n",
    "def uc_hs09_forced(nex_in, nex_models, \n",
    "                   cil_in, cil_models, \n",
    "                   isi_in, isi_models, \n",
    "                   cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                   metric, year):\n",
    "    ######################\n",
    "    # Read all ensembles\n",
    "    ######################\n",
    "    # NEX-GDDP \n",
    "    ds_out = []\n",
    "    for model in nex_models:\n",
    "        ds = xr.open_dataset(nex_in + metric + '/' + model + '.nc')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=slice(year-6, year+6)) # faster rolling mean\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(nex_in, '').split('/')[-1][:-3])        \n",
    "        ds = ds.rolling(time=10, center=True).mean().sel(time=year)\n",
    "        ds['lon'] = np.where(ds['lon'] > 180, ds['lon'] - 360, ds['lon'])\n",
    "        ds = ds.sortby('lon')\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = 'NEX')\n",
    "        ds_out.append(ds)\n",
    "    ds_nex = xr.concat(ds_out, dim='model', compat='identical')\n",
    "\n",
    "    # CIL-GDPCIR\n",
    "    ds_out = []\n",
    "    for model in cil_models:\n",
    "        ds = xr.open_dataset(cil_in + metric + '/' + model, engine='zarr')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=slice(year-6, year+6)) # faster rolling mean\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(cil_in, '').split('/')[-1])        \n",
    "        ds = ds.rolling(time=10, center=True).mean().sel(time=year)\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "        ds = ds.assign_coords(ensemble = 'CIL')\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds_out.append(ds)\n",
    "    ds_cil = xr.concat(ds_out, dim='model', compat='identical')\n",
    "\n",
    "    # ISIMIP\n",
    "    ds_out = []\n",
    "    for model in isi_models:\n",
    "        ds = xr.open_dataset(isi_in + metric + '/' + model + '.nc')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=slice(year-6, year+6)) # faster rolling mean\n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(isi_in, '').split('/')[-1][:-3])        \n",
    "        ds = ds.rolling(time=10, center=True).mean().sel(time=year)\n",
    "        ds['lon'] = np.where(ds['lon'] > 180, ds['lon'] - 360, ds['lon'])\n",
    "        ds = ds.sortby('lon')\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = 'ISIMIP')\n",
    "        ds_out.append(ds)\n",
    "    ds_isi = xr.concat(ds_out, dim='model', compat='identical')\n",
    "\n",
    "    # carbonplan: GARD-SV\n",
    "    ds_out = []\n",
    "    for model in cbp_gard_models:\n",
    "        ds = xr.open_dataset(cbp_in + 'GARD-SV/' + metric + '/' + model + '.nc')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=slice(year-6, year+6)) # faster rolling mean        \n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(cbp_in, '').split('/')[-1][:-3])        \n",
    "        ds = ds.rolling(time=10, center=True).mean().sel(time=year)\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = 'GARD-SV')\n",
    "        # for some models/methods we are missing \n",
    "        # precip so need to fill with NaNs\n",
    "        if 'pr' not in ds.data_vars:\n",
    "            ds['pr'] = xr.full_like(ds['tas'], np.NaN)\n",
    "        ds_out.append(ds)\n",
    "    ds_cbp_gard = xr.concat(ds_out, dim='model', compat='identical')\n",
    "    \n",
    "    # carbonplan: DeepSD-BC\n",
    "    ds_out = []\n",
    "    for model in cbp_deep_models:\n",
    "        ds = xr.open_dataset(cbp_in + 'DeepSD-BC/' + metric + '/' + model + '.nc')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=slice(year-6, year+6)) # faster rolling mean        \n",
    "        ds = ds.assign_coords(model = ds.encoding['source'].replace(cbp_in, '').split('/')[-1][:-3])        \n",
    "        ds = ds.rolling(time=10, center=True).mean().sel(time=year)\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = 'DeepSD-BC')\n",
    "        # for some models/methods we are missing \n",
    "        # precip so need to fill with NaNs\n",
    "        if 'pr' not in ds.data_vars:\n",
    "            ds['pr'] = xr.full_like(ds['tas'], np.NaN)\n",
    "        ds_out.append(ds)\n",
    "    ds_cbp_deep = xr.concat(ds_out, dim='model', compat='identical')\n",
    "\n",
    "    ###########################\n",
    "    # Merge all and mask ocean\n",
    "    ###########################\n",
    "    ds = xr.concat([ds_nex, ds_cil, ds_isi, ds_cbp_gard, ds_cbp_deep],\n",
    "                       dim='ensemble', fill_value=np.nan)\n",
    "    \n",
    "    # mask out ocean points (NEX is only available over land)\n",
    "    ds_mask = ds.sel(ensemble='NEX').isel(ssp=0, model=0)[list(ds.keys())[0]].isnull()\n",
    "    ds = xr.where(ds_mask, np.nan, ds)\n",
    "    \n",
    "    ##########################\n",
    "    # Uncertainty calculation\n",
    "    ##########################\n",
    "    ##  Model uncertainty\n",
    "    # Variance across models, averaged over scenarios and ensembles\n",
    "    U_model = ds.var(dim='model')\n",
    "    weights = ds.isel(lat=300, lon=800)[list(ds.data_vars)[0]].count(dim='model').rename('weights') # weights (choose point over land)\n",
    "    weights = xr.where(weights == 1, 0, weights) # remove combinations where variance was calculated over 1 entry\n",
    "    U_model = U_model.weighted(weights).mean(dim=['ssp', 'ensemble']) # weighted average\n",
    "\n",
    "    ## Scenario uncertainty\n",
    "    # Variance across multi-model means (HS09 approach)\n",
    "    U_scen = ds.mean(dim=['model', 'ensemble']).var(dim='ssp')\n",
    "\n",
    "    ## Downscaling uncertainy\n",
    "    # Variance across ensembles, averaged over models and scenarios\n",
    "    U_ens = ds.var(dim='ensemble')\n",
    "    weights = ds.isel(lat=300, lon=800)[list(ds.data_vars)[0]].count(dim='ensemble').rename('weights') # weights\n",
    "    weights = xr.where(weights == 1, 0, weights) # remove combinations where variance was calculated over 1 entry\n",
    "    U_ens = U_ens.weighted(weights).mean(dim=['ssp', 'model'])\n",
    "\n",
    "    ## Merge and return\n",
    "    U_model = U_model.assign_coords(uncertainty = 'model')\n",
    "    U_scen = U_scen.assign_coords(uncertainty = 'scenario')\n",
    "    U_ens = U_ens.assign_coords(uncertainty = 'ensemble')\n",
    "    \n",
    "    return xr.concat([U_model, U_scen, U_ens], dim='uncertainty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b210d705-e5cf-4ff4-9a47-fcc171bc699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Uncertainty characterization following Hawkins & Sutton 2009 \n",
    "# Interannual variability (single value for all years)\n",
    "################################################################\n",
    "\n",
    "# This function calculates the internal variability (variance\n",
    "# over all years of residuals from rolling mean) for a given \n",
    "# model-ssp-ensemble\n",
    "\n",
    "def uc_hs09_iav(path_in, ensemble, model, metric):\n",
    "    \n",
    "    # read CIL (needs zarr)\n",
    "    if ensemble == 'CIL':\n",
    "        ds = xr.open_dataset(path_in + metric + '/' + model, engine='zarr')\n",
    "    \n",
    "    # read carbon (different path ordering)\n",
    "    elif ensemble == 'DeepSD-BC' or ensemble == 'GARD-SV':\n",
    "        ds = xr.open_dataset(path_in + ensemble + '/' + metric + '/' + model + '.nc')\n",
    "    \n",
    "    # read NEX and ISIMIP\n",
    "    else:\n",
    "        ds = xr.open_dataset(path_in + metric + '/' + model + '.nc')\n",
    "\n",
    "    ## Format same for merge\n",
    "    \n",
    "    # Year index only\n",
    "    ds['time'] = ds.indexes['time'].year\n",
    "\n",
    "    # Add model and ensemble dims\n",
    "    ds = ds.assign_coords(ensmod = ensemble + '__' + model)\n",
    "    ds = ds.sortby('ssp')\n",
    "    \n",
    "    # Transform longitudes if necessary\n",
    "    if ds['lon'].max() > 180:\n",
    "        ds['lon'] = np.where(ds['lon'] > 180, ds['lon'] - 360, ds['lon'])\n",
    "    ds = ds.sortby('lon')\n",
    "    \n",
    "    # Remove Antarctica\n",
    "    if ds['lat'].min() < -60:\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "    \n",
    "    # Missing precip for one carbonplan model\n",
    "    if 'pr' not in ds.data_vars:\n",
    "            ds['pr'] = xr.full_like(ds['tas'], np.NaN)\n",
    "            \n",
    "    ## Get IAV estimate: variance of rolling mean residuals\n",
    "    ds_rolling = ds.rolling(time=10, center=True).mean().sel(time=slice(2020,2096))\n",
    "    return (ds - ds_rolling).var(dim='time')\n",
    "\n",
    "\n",
    "\n",
    "# Make a delayed list with IAV of all models-ssps-ensembles which \n",
    "# can then be combined into one dataset and averaged for best estimate\n",
    "\n",
    "def make_delayed_list_iav(metric):\n",
    "    # Parallelize with dask over models\n",
    "    delayed_res = []\n",
    "    \n",
    "    # NEX\n",
    "    for model in nex_models:\n",
    "        tmp_res = dask.delayed(uc_hs09_iav)(nex_in, 'NEX', model, metric)\n",
    "        delayed_res.append(tmp_res)\n",
    "        \n",
    "    # CIL\n",
    "    for model in cil_models:\n",
    "        tmp_res = dask.delayed(uc_hs09_iav)(cil_in, 'CIL', model, metric)\n",
    "        delayed_res.append(tmp_res)\n",
    "        \n",
    "    # ISIMIP\n",
    "    for model in isi_models:\n",
    "        tmp_res = dask.delayed(uc_hs09_iav)(isi_in, 'ISIMIP', model, metric)\n",
    "        delayed_res.append(tmp_res)\n",
    "        \n",
    "    # carbonplan GARD-SV\n",
    "    for model in cbp_gard_models:\n",
    "        tmp_res = dask.delayed(uc_hs09_iav)(cbp_in, 'GARD-SV', model, metric)\n",
    "        delayed_res.append(tmp_res)\n",
    "        \n",
    "    # carbonplan DeepSD-BC\n",
    "    for model in cbp_deep_models:\n",
    "        tmp_res = dask.delayed(uc_hs09_iav)(cbp_in, 'DeepSD-BC', model, metric)\n",
    "        delayed_res.append(tmp_res)\n",
    "        \n",
    "    # return\n",
    "    return delayed_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a15c7-7b8c-4005-8855-a771f4649fc3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Annual averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79c75f14-322b-4e1d-b642-e429e7fbdaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'annual_avgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ec7c989-315a-42b8-8d2d-62fba57b7202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 s, sys: 9.44 s, total: 24.8 s\n",
      "Wall time: 7min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################################\n",
    "# Interannual variability\n",
    "################################\n",
    "delayed_res = make_delayed_list_iav(metric)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and average over ensemble + model (ensmod) and ssp\n",
    "ds_out = xr.concat(res, dim='ensmod').mean(dim=['ensmod', 'ssp'])\n",
    "ds_out.to_netcdf(out_path + 'hs09_iav/' + metric +'_iav.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b67b3fd8-62a1-4f8c-936c-50e8387fe0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 24s, sys: 27.5 s, total: 3min 51s\n",
      "Wall time: 27min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################################\n",
    "# UC on forced response\n",
    "################################\n",
    "delayed_res = []\n",
    "for year in range(2020, 2097):\n",
    "    # Read all ensembles and compute UC\n",
    "    tmp_res = dask.delayed(uc_hs09_forced)(nex_in, nex_models, \n",
    "                                          cil_in, cil_models, \n",
    "                                          isi_in, isi_models, \n",
    "                                          cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                          metric, year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'hs09_iav/' + metric +'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175dd321-2169-466e-8cef-ed20d0ede997",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Annual maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a260c81-0ea8-4510-84cc-8fd7f0490d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'annual_maxs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05c78b45-049e-4665-becf-f294c8a96bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.1 s, sys: 15 s, total: 1min 4s\n",
      "Wall time: 4min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################################\n",
    "# Interannual variability\n",
    "################################\n",
    "delayed_res = make_delayed_list_iav(metric)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and average over ensemble + model (ensmod) and ssp\n",
    "ds_out = xr.concat(res, dim='ensmod').mean(dim=['ensmod', 'ssp'])\n",
    "ds_out.to_netcdf(out_path + 'hs09_iav/' + metric +'_iav.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "702d89a9-23cd-4d42-84e8-fd039d6e2c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 43s, sys: 26.2 s, total: 3min 9s\n",
      "Wall time: 17min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################################\n",
    "# UC on forced response\n",
    "################################\n",
    "delayed_res = []\n",
    "for year in range(2020, 2097):\n",
    "    # Read all ensembles and compute UC\n",
    "    tmp_res = dask.delayed(uc_hs09_forced)(nex_in, nex_models, \n",
    "                                          cil_in, cil_models, \n",
    "                                          isi_in, isi_models, \n",
    "                                          cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                          metric, year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'hs09_iav/' + metric +'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73118140-bdf9-43d1-a743-7f5c4716cefd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
