{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57743a7e-8b68-4544-8378-ac3e453f96bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import functools\n",
    "import warnings\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f3946-bb89-4535-a1e8-ae9b6ccaed61",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "662dfbf5-61c7-48f6-884d-ce058d6a7512",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Set paths\n",
    "# UPDATE THIS FOR REPRODUCTION\n",
    "###############################\n",
    "nex_in = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/nex-gddp/'\n",
    "cil_in = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/cil-gdpcir/'\n",
    "isi_in = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/isimip3b/regridded/conservative/'\n",
    "cbp_in = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/carbonplan/'\n",
    "\n",
    "out_path = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/results/'\n",
    "poly_path = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/forced_response/'\n",
    "iav_path = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/interannual_variability/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c50de8a-ea1e-4a9d-ad37-c522111ff76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Models\n",
    "###################\n",
    "from utils import nex_ssp_dict, cil_ssp_dict, isimip_ssp_dict, gardsv_ssp_dict, gardsv_var_dict, deepsdbc_dict\n",
    "\n",
    "nex_models = list(nex_ssp_dict.keys())\n",
    "cil_models = list(cil_ssp_dict.keys())\n",
    "isi_models = list(isimip_ssp_dict.keys())\n",
    "cbp_gard_models = list(gardsv_ssp_dict.keys())\n",
    "cbp_gard_precip_models = [model for model in cbp_gard_models if 'pr' in gardsv_var_dict[model]]\n",
    "cbp_deep_models = list(deepsdbc_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ad43d2c-5706-40fa-a2b4-9e1f24782a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Land mask (from NEX)\n",
    "#######################\n",
    "land_mask = xr.open_dataset('/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/nex-gddp/avg/CanESM5.nc')\n",
    "land_mask = land_mask.isel(ssp=0, time=0).tas.isnull()\n",
    "land_mask['lon'] = np.where(land_mask['lon'] > 180, land_mask['lon'] - 360, land_mask['lon'])\n",
    "land_mask = land_mask.sortby('lon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1c47d5-5fb5-4648-aac2-a86cbb62fdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-82fdb08f-af3d-11ed-b90e-34e6d79eac77</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_jobqueue.PBSCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <button style=\"margin-bottom: 12px;\" data-commandlinker-command=\"dask:populate-and-launch-layout\" data-commandlinker-args='{\"url\": \"/proxy/8787/status\" }'>\n",
       "                Launch dashboard in JupyterLab\n",
       "            </button>\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">PBSCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">a58bb6f4</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-65f31f58-9d4e-4c27-97ad-78d5f936e845</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://10.102.201.239:33242\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.102.201.239:33242' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############\n",
    "# Dask\n",
    "############\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "cluster = PBSCluster(cores=1, memory='60GB', resource_spec='pmem=60GB',\n",
    "                     # account='open',\n",
    "                     worker_extra_args=['#PBS -l feature=rhel7'], \n",
    "                     walltime='20:00:00')\n",
    "\n",
    "cluster.scale(jobs=55)  # ask for jobs\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f20699-35c7-4dbf-a8ad-ad73115785df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Total uncertainty: gridded metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6099faaa-43cd-4e3b-a541-8cf3dfb66379",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c6626-3157-4eee-9847-5df9e74f459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Total uncertainty: variance across all models, scenarios, ensembles \n",
    "#######################################################################\n",
    "def uc_total(nex_in, nex_models, \n",
    "             cil_in, cil_models, \n",
    "             isi_in, isi_models, \n",
    "             cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "             land_mask,\n",
    "             metric, submetric,\n",
    "             year):\n",
    "    \"\"\"\n",
    "    Reads in all models, ssps, and calculates the total uncertainty (variance across\n",
    "    all model, ssp, ensemble dimensions) for a given year (and possibly DataArray).\n",
    "    For metrics like 'hot' where there are several sub-metrics based on different \n",
    "    thresholds and/or observational data, we need to select a specific DataArray\n",
    "    to keep the memory manageable.\n",
    "    \"\"\"\n",
    "    # Subfunction for general preprocessing of each model/ensemble\n",
    "    def read_and_process(ensemble, path_in, model, year, metric, submetric):\n",
    "        # Read netcdf or zarr\n",
    "        if ensemble in ['NEX', 'ISIMIP', 'GARD-SV']:\n",
    "            ds = xr.open_dataset(path_in + metric + '/' + model + '.nc')\n",
    "        elif ensemble in ['CIL', 'DeepSD-BC']:\n",
    "            ds = xr.open_dataset(path_in + metric + '/' + model, engine='zarr')\n",
    "            \n",
    "        # Select submetric if chosen\n",
    "        if submetric:\n",
    "            ds = ds[submetric]\n",
    "    \n",
    "        # Common preprocessing\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = ensemble)\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "        \n",
    "        # Add model dimension\n",
    "        if model[-6:] in ['tasmin', 'tasmax']:\n",
    "            model_str = model[:-7]\n",
    "        else:\n",
    "            model_str = model\n",
    "        ds = ds.assign_coords(model = model_str)\n",
    "        \n",
    "        # Fix lon to [-180,180]\n",
    "        if ds.lon.max() > 180:\n",
    "            ds['lon'] = np.where(ds['lon'] > 180, ds['lon'] - 360, ds['lon'])\n",
    "            ds = ds.sortby('lon')\n",
    "    \n",
    "        # Some models/methods are missing precip so fill with NaNs\n",
    "        if (metric in ['max', 'avg']) and ('pr' not in ds.data_vars):\n",
    "            ds['pr'] = xr.full_like(ds[list(ds.data_vars)[0]], np.nan)\n",
    "        if (metric == 'max5d') and ('RX5day' not in ds.data_vars):\n",
    "            ds['RX5day'] = xr.full_like(ds[list(ds.data_vars)[0]], np.nan)\n",
    "            \n",
    "        # Drop member_id\n",
    "        if 'member_id' in list(ds.coords):\n",
    "            ds = ds.isel(member_id=0).drop('member_id')\n",
    "    \n",
    "        # Return\n",
    "        return ds\n",
    "\n",
    "    ######################\n",
    "    # Read all ensembles\n",
    "    ######################\n",
    "    # NEX-GDDP \n",
    "    ds_out = []\n",
    "    for model in nex_models:\n",
    "        ds_out.append(read_and_process('NEX', nex_in, model, year, metric, submetric))\n",
    "    ds_nex = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "\n",
    "    # CIL-GDPCIR\n",
    "    ds_out = []\n",
    "    for model in cil_models:\n",
    "        ds_out.append(read_and_process('CIL', cil_in, model, year, metric, submetric))\n",
    "    ds_cil = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "\n",
    "    # ISIMIP\n",
    "    ds_out = []\n",
    "    for model in isi_models:\n",
    "        ds_out.append(read_and_process('ISIMIP', isi_in, model, year, metric, submetric))\n",
    "    ds_isi = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "\n",
    "    # carbonplan: GARD-SV\n",
    "    ds_out = []\n",
    "    for model in cbp_gard_models:\n",
    "        ds_out.append(read_and_process('GARD-SV', cbp_in + '/regridded/conservative/GARD-SV/', model, year, metric, submetric))\n",
    "    ds_cbp_gard = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "    \n",
    "    # carbonplan: DeepSD-BC\n",
    "    ds_out = []\n",
    "    for model in cbp_deep_models:\n",
    "        ds_out.append(read_and_process('DeepSD-BC', cbp_in + 'native_grid/DeepSD-BC/', model, year, metric, submetric))\n",
    "    ds_cbp_deep = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "\n",
    "    ###########################\n",
    "    # Merge all and mask ocean\n",
    "    ###########################\n",
    "    ds = xr.concat([ds_nex, ds_cil, ds_isi, ds_cbp_gard, ds_cbp_deep],\n",
    "                       dim='ensemble', fill_value=np.nan)\n",
    "\n",
    "    # Mask out ocean points\n",
    "    ds = xr.where(land_mask, np.nan, ds)\n",
    "    \n",
    "    ##########################\n",
    "    # Uncertainty calculation\n",
    "    ##########################\n",
    "    ## Total uncertainty\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        U_total_true = ds.var(dim=['ensemble', 'ssp', 'model'], skipna=True) # throws warning when all NaNs\n",
    "\n",
    "    U_total_true = U_total_true.assign_coords(uncertainty = 'total_true')\n",
    "    \n",
    "    return U_total_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ab2c4-937d-4182-bbb9-28cce3d0db86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fb9a5cf-fef0-4b13-a893-c3abb8f8869f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 6s, sys: 19.6 s, total: 7min 25s\n",
      "Wall time: 53min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "metric = 'avg'\n",
    "\n",
    "# Dask delayed over years\n",
    "delayed_res = []\n",
    "for year in range(2015, 2100):\n",
    "    # Read all ensembles and compute total uncertainty\n",
    "    tmp_res = dask.delayed(uc_total)(nex_in, nex_models, \n",
    "                                     cil_in, cil_models, \n",
    "                                     isi_in, isi_models, \n",
    "                                     cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                     land_mask,\n",
    "                                     metric, False,\n",
    "                                     year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'total_uncertainty/' + metric + '.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a42be14-76c5-45c6-af3e-b3c3ac35d50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 15s, sys: 26.9 s, total: 10min 42s\n",
      "Wall time: 1h 19min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "metric = 'max'\n",
    "\n",
    "# Dask delayed over years\n",
    "delayed_res = []\n",
    "for year in range(2015, 2100):\n",
    "    # Read all ensembles and compute total uncertainty\n",
    "    tmp_res = dask.delayed(uc_total)(nex_in, nex_models, \n",
    "                                     cil_in, cil_models, \n",
    "                                     isi_in, isi_models, \n",
    "                                     cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                     land_mask,\n",
    "                                     metric, False,\n",
    "                                     year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'total_uncertainty/' + metric +'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3fad6-da49-4d22-8683-e48f26f2b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "metric = 'max5d'\n",
    "\n",
    "# Dask delayed over years\n",
    "delayed_res = []\n",
    "for year in range(2015, 2100):\n",
    "    # Read all ensembles and compute total uncertainty\n",
    "    tmp_res = dask.delayed(uc_total)(nex_in, nex_models, \n",
    "                                     cil_in, cil_models, \n",
    "                                     isi_in, isi_models, \n",
    "                                     cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                     land_mask,\n",
    "                                     metric, False,\n",
    "                                     year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'total_uncertainty/' + metric +'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39d8af44-d0b6-4149-9fae-54bde897d272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 21s, sys: 10.3 s, total: 3min 32s\n",
      "Wall time: 26min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "metric = 'dry'\n",
    "\n",
    "# Dask delayed over years\n",
    "delayed_res = []\n",
    "for year in range(2015, 2100):\n",
    "    # Read all ensembles and compute total uncertainty\n",
    "    tmp_res = dask.delayed(uc_total)(nex_in, nex_models, \n",
    "                                     cil_in, cil_models, \n",
    "                                     isi_in, isi_models, \n",
    "                                     cbp_in, cbp_gard_precip_models, cbp_deep_models,\n",
    "                                     land_mask,\n",
    "                                     metric, False,\n",
    "                                     year)\n",
    "    \n",
    "    # Append\n",
    "    delayed_res.append(tmp_res)\n",
    "    \n",
    "# Compute\n",
    "res = dask.compute(*delayed_res)\n",
    "\n",
    "# Merge and store\n",
    "ds_out = xr.concat(res, dim='time')\n",
    "ds_out.to_netcdf(out_path + 'total_uncertainty/' + metric +'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dcf8a90-fa74-49a4-972a-2fd9e395645e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.5 s, sys: 9.57 s, total: 1min 3s\n",
      "Wall time: 10min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hot + dry days\n",
    "metric = 'hotdry'\n",
    "for thresh in ['q99', 'rp10']:\n",
    "    for obs in ['gmfd', 'era5']:\n",
    "        submetric_str = thresh + obs\n",
    "        submetric = ['hotdry_' + submetric_str + '_count', 'hotdry_' + submetric_str + '_streak']\n",
    "\n",
    "        # Dask delayed over years\n",
    "        delayed_res = []\n",
    "        for year in range(2015, 2100):\n",
    "            # Read all ensembles and compute total uncertainty\n",
    "            tmp_res = dask.delayed(uc_total)(nex_in, nex_models,\n",
    "                                             cil_in, cil_models, \n",
    "                                             isi_in, isi_models,\n",
    "                                             cbp_in, cbp_gard_precip_models, cbp_deep_models,\n",
    "                                             land_mask,\n",
    "                                             metric, submetric,\n",
    "                                             year)\n",
    "    \n",
    "            # Append\n",
    "            delayed_res.append(tmp_res)\n",
    "    \n",
    "        # Compute\n",
    "        res = dask.compute(*delayed_res)\n",
    "\n",
    "        # Merge and store\n",
    "        ds_out = xr.concat(res, dim='time')\n",
    "        ds_out.to_netcdf(out_path + 'total_uncertainty/' + metric + '_' + submetric_str + '.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0095b6b-8090-4cff-88f9-3e7e8f44587b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58 s, sys: 9.06 s, total: 1min 7s\n",
      "Wall time: 11min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Wet days\n",
    "metric = 'wet'\n",
    "for thresh in ['q99', 'rp10']:\n",
    "    for obs in ['gmfd', 'era5']:\n",
    "        submetric_str = thresh + obs\n",
    "        submetric = ['pr_' + submetric_str + '_count', 'pr_' + submetric_str + '_streak']\n",
    "\n",
    "        # Dask delayed over years\n",
    "        delayed_res = []\n",
    "        for year in range(2015, 2100):\n",
    "            # Read all ensembles and compute total uncertainty\n",
    "            tmp_res = dask.delayed(uc_total)(nex_in, nex_models,\n",
    "                                             cil_in, cil_models, \n",
    "                                             isi_in, isi_models,\n",
    "                                             cbp_in, cbp_gard_precip_models, cbp_deep_models,\n",
    "                                             land_mask,\n",
    "                                             metric, submetric,\n",
    "                                             year)\n",
    "    \n",
    "            # Append\n",
    "            delayed_res.append(tmp_res)\n",
    "    \n",
    "        # Compute\n",
    "        res = dask.compute(*delayed_res)\n",
    "\n",
    "        # Merge and store\n",
    "        ds_out = xr.concat(res, dim='time')\n",
    "        ds_out.to_netcdf(out_path + 'total_uncertainty/' + metric + '_' + submetric_str + '.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6258a0e0-a464-4630-8f81-b527e24aacc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 42s, sys: 33.1 s, total: 8min 15s\n",
      "Wall time: 58min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hot days\n",
    "metric = 'hot'\n",
    "for thresh in ['q99', 'rp10']:\n",
    "    for obs in ['gmfd', 'era5']:\n",
    "        for submetric_var in ['tas', 'tasmin', 'tasmax']:\n",
    "            submetric_str = submetric_var + '_' + thresh + obs\n",
    "            submetric = [submetric_str + '_count', submetric_str + '_streak']\n",
    "\n",
    "            # Dask delayed over years\n",
    "            delayed_res = []\n",
    "            for year in range(2015, 2100):\n",
    "                # Read all ensembles and compute total uncertainty\n",
    "                tmp_res = dask.delayed(uc_total)(nex_in, [model + '_' + submetric_var for model in nex_models], \n",
    "                                                 cil_in, cil_models, \n",
    "                                                 isi_in, [model + '_' + submetric_var for model in isi_models], \n",
    "                                                 cbp_in, [model + '_' + submetric_var for model in cbp_gard_models], cbp_deep_models,\n",
    "                                                 land_mask,\n",
    "                                                 metric, submetric,\n",
    "                                                 year)\n",
    "                \n",
    "                # Append\n",
    "                delayed_res.append(tmp_res)\n",
    "    \n",
    "            # Compute\n",
    "            res = dask.compute(*delayed_res)\n",
    "\n",
    "            # Merge and store\n",
    "            ds_out = xr.concat(res, dim='time')\n",
    "            ds_out.to_netcdf(out_path + 'total_uncertainty/' + metric + '_' + submetric_str + '.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846d2ef-2b66-4b48-8f15-7cd721208f46",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Uncertainty partitioning: gridded metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1c9fa-de1b-4b88-a734-dcefddb4be0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extracting the forced response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af24ede7-0f8b-4172-b9e8-7d0c8a137385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forced_poly(nex_in, nex_models,\n",
    "                    cil_in, cil_models, \n",
    "                    isi_in, isi_models, \n",
    "                    cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                    land_mask,\n",
    "                    metric, submetric, submetric_var,\n",
    "                    poly_path,\n",
    "                    deg):\n",
    "    \"\"\"\n",
    "    Reads in all models, ssps, and calculates the 'forced response' as a deg-th order\n",
    "    polynomial.\n",
    "    For metrics like 'hot' where there are several sub-metrics based on different \n",
    "    thresholds and/or observational data, we need to select a specific DataArray\n",
    "    to keep the memory manageable.\n",
    "    \"\"\"\n",
    "    # Subfunction to get polynomial for each model/ensemble\n",
    "    def get_poly_coeffs(ensemble, path_in, model, metric, submetric, submetric_var, deg):\n",
    "        # Read netcdf or zarr\n",
    "        if ensemble in ['NEX', 'ISIMIP', 'GARD-SV']:\n",
    "            if submetric_var:\n",
    "                ds = xr.open_dataset(path_in + metric + '/' + model + '_' + submetric_var + '.nc')\n",
    "            else:\n",
    "                ds = xr.open_dataset(path_in + metric + '/' + model + '.nc')\n",
    "        elif ensemble in ['CIL', 'DeepSD-BC']:\n",
    "            ds = xr.open_dataset(path_in + metric + '/' + model, engine='zarr')    \n",
    "        \n",
    "        # Select submetric if chosen\n",
    "        if submetric:\n",
    "            ds = ds[submetric]\n",
    "    \n",
    "        # Common preprocessing\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "        ds = ds.sortby('ssp')\n",
    "        if ds.lon.max() > 180:\n",
    "            ds['lon'] = np.where(ds['lon'] > 180, ds['lon'] - 360, ds['lon'])\n",
    "            ds = ds.sortby('lon')\n",
    "\n",
    "        # Forced response via polynomial\n",
    "        ds = xr.polyval(coord = ds['time'],\n",
    "                        coeffs = ds.polyfit(dim='time', deg=deg))\n",
    "        ds = ds.rename({name:name.replace('_polyfit_coefficients', '') for name in list(ds.data_vars)})\n",
    "            \n",
    "        # Construct output name: assumes submetrics are of the form ['X_count', 'X_streak']\n",
    "        out_str = poly_path + metric + '/'\n",
    "\n",
    "        if submetric:\n",
    "            submetric_str = submetric[0].replace('_count', '').replace('_streak', '')\n",
    "            out_str = out_str + submetric_str + '_'\n",
    "            \n",
    "        # Drop member_id\n",
    "        if 'member_id' in list(ds.coords):\n",
    "            ds = ds.isel(member_id=0).drop('member_id')\n",
    "        \n",
    "        ds.to_netcdf(out_str + ensemble + '_' + model + '_deg' + str(deg) + '.nc')\n",
    "\n",
    "    # For checking if file exists\n",
    "    def construct_out_str(poly_path, ensemble, model, metric, submetric, submetric_var, deg):\n",
    "        # Construct output name: assumes submetrics are of the form ['X_count', 'X_streak']\n",
    "        out_str = poly_path + metric + '/'\n",
    "\n",
    "        if submetric:\n",
    "            submetric_str = submetric[0].replace('_count', '').replace('_streak', '')\n",
    "            out_str = out_str + submetric_str + '_'\n",
    "        \n",
    "        return out_str + ensemble + '_' + model + '_deg' + str(deg) + '.nc'\n",
    "    \n",
    "    #######################\n",
    "    # Apply to all ensembles\n",
    "    #######################\n",
    "    # Dask delayed\n",
    "    res = []\n",
    "    \n",
    "    # NEX-GDDP \n",
    "    for model in nex_models:\n",
    "        if not os.path.isfile(construct_out_str(poly_path, 'NEX', model, metric, submetric, submetric_var, deg)):\n",
    "            res.append(dask.delayed(get_poly_coeffs)('NEX', nex_in, model, metric, submetric, submetric_var, deg))\n",
    "\n",
    "    # CIL-GDPCIR\n",
    "    for model in cil_models:\n",
    "        if not os.path.isfile(construct_out_str(poly_path, 'CIL', model, metric, submetric, submetric_var, deg)):\n",
    "            res.append(dask.delayed(get_poly_coeffs)('CIL', cil_in, model, metric, submetric, submetric_var, deg))\n",
    "\n",
    "    # ISIMIP\n",
    "    for model in isi_models:\n",
    "        if not os.path.isfile(construct_out_str(poly_path, 'ISIMIP', model, metric, submetric, submetric_var, deg)):\n",
    "            res.append(dask.delayed(get_poly_coeffs)('ISIMIP', isi_in, model, metric, submetric, submetric_var, deg))\n",
    "\n",
    "    # carbonplan: GARD-SV\n",
    "    for model in cbp_gard_models:\n",
    "        if not os.path.isfile(construct_out_str(poly_path, 'GARD-SV', model, metric, submetric, submetric_var, deg)):\n",
    "            res.append(dask.delayed(get_poly_coeffs)('GARD-SV', cbp_in + '/regridded/conservative/GARD-SV/', model, metric, submetric, submetric_var, deg))\n",
    "    \n",
    "    # carbonplan: DeepSD-BC\n",
    "    for model in cbp_deep_models:\n",
    "        if not os.path.isfile(construct_out_str(poly_path, 'DeepSD-BC', model, metric, submetric, submetric_var, deg)):\n",
    "            res.append(dask.delayed(get_poly_coeffs)('DeepSD-BC', cbp_in + 'native_grid/DeepSD-BC/', model, metric, submetric, submetric_var, deg))\n",
    "    \n",
    "    # Compute\n",
    "    if len(res) > 0:\n",
    "        dask.compute(*res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc653be-cb3f-4a81-9965-c701734e1c10",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Uncertainty characterization of forced response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2451366-7734-4d83-bb2c-c16d11a91062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uc_forced(nex_in, nex_models,\n",
    "              cil_in, cil_models, \n",
    "              isi_in, isi_models, \n",
    "              cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "              land_mask,\n",
    "              metric, submetric, submetric_var,\n",
    "              year,\n",
    "              poly_path, deg):\n",
    "    \"\"\"\n",
    "    Reads in all models, ssps, and calculates the uncertainty in the 'forced response'\n",
    "    along each dimension (ssp, model, ens) for a given year (and possibly DataArray).\n",
    "    For metrics like 'hot' where there are several sub-metrics based on different \n",
    "    thresholds and/or observational data, we need to select a specific DataArray\n",
    "    to keep the memory manageable.\n",
    "    \"\"\"\n",
    "    # Subfunction for general preprocessing of each model/ensemble\n",
    "    def read_and_process(ensemble, path_in, model, year, metric, submetric, submetric_var, deg):\n",
    "        # Polynomial responses should have already been calculated\n",
    "        poly_str = poly_path + metric + '/'\n",
    "        if submetric:\n",
    "            submetric_str = submetric[0].replace('_count', '').replace('_streak', '')\n",
    "            poly_str = poly_str + submetric_str + '_'\n",
    "        ds = xr.open_dataset(poly_str + ensemble + '_' + model + '_deg' + str(deg) + '.nc')\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sel(time=year)\n",
    "        \n",
    "        # Select submetric if chosen\n",
    "        if submetric:\n",
    "            ds = ds[submetric]\n",
    "    \n",
    "        # Common preprocessing\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensemble = ensemble)\n",
    "        ds = ds.assign_coords(model = model)\n",
    "    \n",
    "        # Fix lon to [-180,180]\n",
    "        if ds.lon.max() > 180:\n",
    "            ds['lon'] = np.where(ds['lon'] > 180, ds['lon'] - 360, ds['lon'])\n",
    "            ds = ds.sortby('lon')\n",
    "    \n",
    "        # Some models/methods are missing precip so fill with NaNs\n",
    "        if (metric in ['max', 'avg']) and ('pr' not in ds.data_vars):\n",
    "            ds['pr'] = xr.full_like(ds[list(ds.data_vars)[0]], np.nan)\n",
    "        if (metric == 'max5d') and ('RX5day' not in ds.data_vars):\n",
    "            ds['RX5day'] = xr.full_like(ds[list(ds.data_vars)[0]], np.nan)\n",
    "            \n",
    "        # Drop member_id\n",
    "        if 'member_id' in list(ds.coords):\n",
    "            ds = ds.isel(member_id=0).drop('member_id')\n",
    "            \n",
    "        # Forced response should always be >= 0\n",
    "        if metric in ['hot', 'wet', 'dry', 'hotdry']:\n",
    "            ds = xr.where(ds >= 0, ds, 0)\n",
    "    \n",
    "        # Return\n",
    "        return ds\n",
    "\n",
    "    ######################\n",
    "    # Read all ensembles\n",
    "    ######################\n",
    "    # NEX-GDDP \n",
    "    ds_out = []\n",
    "    for model in nex_models:\n",
    "        ds_out.append(read_and_process('NEX', nex_in, model, year, metric, submetric, submetric_var, deg))\n",
    "    ds_nex = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "\n",
    "    # CIL-GDPCIR\n",
    "    ds_out = []\n",
    "    for model in cil_models:\n",
    "        ds_out.append(read_and_process('CIL', cil_in, model, year, metric, submetric, submetric_var, deg))\n",
    "    ds_cil = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "\n",
    "    # ISIMIP\n",
    "    ds_out = []\n",
    "    for model in isi_models:\n",
    "        ds_out.append(read_and_process('ISIMIP', isi_in, model, year, metric, submetric, submetric_var, deg))\n",
    "    ds_isi = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "\n",
    "    # carbonplan: GARD-SV\n",
    "    ds_out = []\n",
    "    for model in cbp_gard_models:\n",
    "        ds_out.append(read_and_process('GARD-SV', cbp_in + '/regridded/conservative/GARD-SV/', model, year, metric, submetric, submetric_var, deg))\n",
    "    ds_cbp_gard = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "    \n",
    "    # carbonplan: DeepSD-BC\n",
    "    ds_out = []\n",
    "    for model in cbp_deep_models:\n",
    "        ds_out.append(read_and_process('DeepSD-BC', cbp_in + 'native_grid/DeepSD-BC/', model, year, metric, submetric, submetric_var, deg))\n",
    "    ds_cbp_deep = xr.concat(ds_out, dim='model', fill_value=np.nan)\n",
    "\n",
    "    ###########################\n",
    "    # Merge all and mask ocean\n",
    "    ###########################\n",
    "    ds = xr.concat([ds_nex, ds_cil, ds_isi, ds_cbp_gard, ds_cbp_deep],\n",
    "                       dim='ensemble', fill_value=np.nan)\n",
    "    ds = xr.where(land_mask, np.nan, ds)\n",
    "    \n",
    "    ##########################\n",
    "    # Uncertainty calculation\n",
    "    ##########################\n",
    "    \n",
    "    ## Scenario uncertainty\n",
    "    # HS09 approach: variance across multi-model means\n",
    "    U_scen_hs09 = ds.mean(dim=['model', 'ensemble']).var(dim='ssp')\n",
    "    # BB13 approach: variance across scenarios, averaged over models and ensembles (no weighting)\n",
    "    U_scen_bb13 = ds.var(dim='ssp').mean(dim=['model', 'ensemble'])\n",
    "    \n",
    "    ##  Model uncertainty\n",
    "    # Variance across models, averaged over scenarios and ensembles\n",
    "    U_model = ds.var(dim='model')\n",
    "    weights = ds.isel(lat=300, lon=800)[list(ds.data_vars)[0]].count(dim='model').rename('weights') # weights (choose point over land)\n",
    "    weights = xr.where(weights == 1, 0, weights) # remove combinations where variance was calculated over 1 entry\n",
    "    U_model = U_model.weighted(weights).mean(dim=['ssp', 'ensemble']) # weighted average\n",
    "\n",
    "    ## Downscaling uncertainy\n",
    "    # Variance across ensembles, averaged over models and scenarios\n",
    "    U_ens = ds.var(dim='ensemble')\n",
    "    weights = ds.isel(lat=300, lon=800)[list(ds.data_vars)[0]].count(dim='ensemble').rename('weights') # weights (choose point over land)\n",
    "    weights = xr.where(weights == 1, 0, weights) # remove combinations where variance was calculated over 1 entry\n",
    "    U_ens = U_ens.weighted(weights).mean(dim=['ssp', 'model']) # weighted average\n",
    "\n",
    "    ## Merge and return\n",
    "    U_model = U_model.assign_coords(uncertainty = 'model')\n",
    "    U_ens = U_ens.assign_coords(uncertainty = 'ensemble')\n",
    "    U_scen_hs09 = U_scen_hs09.assign_coords(uncertainty = 'scenario_hs09')\n",
    "    U_scen_bb13 = U_scen_bb13.assign_coords(uncertainty = 'scenario_bb13')\n",
    "    \n",
    "    U_out = xr.concat([U_scen_hs09, U_scen_bb13, U_model, U_ens],\n",
    "                      dim='uncertainty')\n",
    "        \n",
    "    U_out = U_out.assign_coords(time=year)\n",
    "        \n",
    "    return U_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be05b1c7-1d46-4c73-aa33-0213ed4e80af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_forced_uc(metric, submetric, submetric_var, poly_path, deg, save_str):\n",
    "    \"\"\"\n",
    "    Calculate the uncertainty paritition of the forced response\n",
    "    for a given selection of parameters/settings\n",
    "    \"\"\"\n",
    "    if os.path.isfile(out_path + 'uncertainty_partitioning/' + metric + '_' + save_str + '.nc'):\n",
    "        return None\n",
    "    \n",
    "    delayed_res = []\n",
    "    \n",
    "    # carbonplan GARD-SV precip models\n",
    "    if metric in ['wet', 'dry', 'hotdry']:\n",
    "        cbp_gard_models_in = cbp_gard_precip_models\n",
    "    else:\n",
    "        cbp_gard_models_in = cbp_gard_models\n",
    "    \n",
    "    for year in range(2015, 2100):\n",
    "        # Read all ensembles and compute UC\n",
    "        tmp_res = dask.delayed(uc_forced)(nex_in, nex_models, \n",
    "                                          cil_in, cil_models, \n",
    "                                          isi_in, isi_models, \n",
    "                                          cbp_in, cbp_gard_models_in, cbp_deep_models,\n",
    "                                          land_mask,\n",
    "                                          metric, submetric, submetric_var,\n",
    "                                          year,\n",
    "                                          poly_path, deg)\n",
    "    \n",
    "        # Append\n",
    "        delayed_res.append(tmp_res)\n",
    "    \n",
    "    # Compute\n",
    "    res = dask.compute(*delayed_res)\n",
    "\n",
    "    # Merge and store\n",
    "    ds_out = xr.concat(res, dim='time')\n",
    "    ds_out.to_netcdf(out_path + 'uncertainty_partitioning/' + metric + '_' + save_str + '.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3d2b7-b917-4378-a8a2-f0fd337868c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Measure of interannual variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aecba2ce-53f7-43c1-a490-f590f5043ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iav(path_in, ensemble, model, land_mask, metric, submetric, submetric_var, poly_path, deg, const_iav, iav_path):\n",
    "    \"\"\"\n",
    "    Calculates the internal variability (variance over all years\n",
    "    of residuals from forced response) for a given model-ssp-ensemble\n",
    "    \"\"\"\n",
    "    # Subfunction for general preprocessing of each model/ensemble\n",
    "    def read_and_process(ensemble, path_in, model, metric, submetric, submetric_var):\n",
    "        # Read netcdf or zarr\n",
    "        if ensemble in ['NEX', 'ISIMIP', 'GARD-SV']:\n",
    "            if submetric_var:\n",
    "                ds = xr.open_dataset(path_in + metric + '/' + model + '_' + submetric_var + '.nc')\n",
    "            else:\n",
    "                ds = xr.open_dataset(path_in + metric + '/' + model + '.nc')\n",
    "        elif ensemble in ['CIL', 'DeepSD-BC']:\n",
    "            ds = xr.open_dataset(path_in + metric + '/' + model, engine='zarr')\n",
    "            \n",
    "        # Some models/methods are missing precip so fill with NaNs\n",
    "        if (metric in ['max', 'avg']) and ('pr' not in ds.data_vars):\n",
    "            ds['pr'] = xr.full_like(ds[list(ds.data_vars)[0]], np.nan)\n",
    "        if (metric == 'max5d') and ('RX5day' not in ds.data_vars):\n",
    "            ds['RX5day'] = xr.full_like(ds[list(ds.data_vars)[0]], np.nan)\n",
    "        \n",
    "        # Select submetric if chosen\n",
    "        if submetric:\n",
    "            ds = ds[submetric]\n",
    "    \n",
    "        # Common preprocessing\n",
    "        ds['time'] = ds.indexes['time'].year\n",
    "        ds = ds.sortby('ssp')\n",
    "        ds = ds.assign_coords(ensmod = ensemble + '__' + model)\n",
    "        ds = ds.sel(lat=slice(-60, 90))\n",
    "    \n",
    "        # Fix lon to [-180,180]\n",
    "        if ds.lon.max() > 180:\n",
    "            ds['lon'] = np.where(ds['lon'] > 180, ds['lon'] - 360, ds['lon'])\n",
    "            ds = ds.sortby('lon')\n",
    "            \n",
    "        # Drop member_id\n",
    "        if 'member_id' in list(ds.coords):\n",
    "            ds = ds.isel(member_id=0).drop('member_id')\n",
    "    \n",
    "        # Return\n",
    "        return ds\n",
    "    \n",
    "    #########################\n",
    "    # Check if already done\n",
    "    # for non-const case \n",
    "    #########################\n",
    "    if not const_iav:\n",
    "        out_str = iav_path + metric + '/'\n",
    "\n",
    "        if submetric:\n",
    "            submetric_str = submetric[0].replace('_count', '').replace('_streak', '')\n",
    "            out_str = out_str + submetric_str + '_'\n",
    "        \n",
    "        if os.path.isfile(out_str + ensemble + '_' + model + '_deg' + str(deg) + '.nc'):\n",
    "            return None\n",
    "\n",
    "    ###################\n",
    "    # Read raw outputs\n",
    "    ###################\n",
    "    ds = read_and_process(ensemble, path_in, model, metric, submetric, submetric_var)\n",
    "    # Mask out ocean points\n",
    "    ds = xr.where(land_mask, np.nan, ds)\n",
    "\n",
    "    ########################\n",
    "    # Read forced response\n",
    "    ########################\n",
    "    poly_str = poly_path + metric + '/'\n",
    "    if submetric:\n",
    "        submetric_str = submetric[0].replace('_count', '').replace('_streak', '')\n",
    "        poly_str = poly_str + submetric_str + '_'\n",
    "        \n",
    "    ds_forced = xr.open_dataset(poly_str + ensemble + '_' + model + '_deg' + str(deg) + '.nc')    \n",
    "    ds_forced['time'] = ds_forced.indexes['time'].year\n",
    "    \n",
    "    # Forced response should always be >= 0\n",
    "    if metric in ['hot', 'wet', 'dry', 'hotdry']:\n",
    "        ds_forced = xr.where(ds_forced >= 0, ds_forced, 0)\n",
    "    \n",
    "    # Some models/methods are missing precip so fill with NaNs\n",
    "    if (metric in ['max', 'avg']) and ('pr' not in ds_forced.data_vars):\n",
    "        ds_forced['pr'] = xr.full_like(ds_forced[list(ds_forced.data_vars)[0]], np.nan)\n",
    "    if (metric == 'max5d') and ('RX5day' not in ds_forced.data_vars):\n",
    "        ds_forced['RX5day'] = xr.full_like(ds_forced[list(ds_forced.data_vars)[0]], np.nan)\n",
    "\n",
    "    ################################\n",
    "    # Get IAV estimate\n",
    "    # Variance of residuals\n",
    "    ################################\n",
    "    # IAV can be constant value or rolling\n",
    "    if const_iav:\n",
    "        iav = (ds - ds_forced).var(dim='time')\n",
    "        return iav\n",
    "    else:\n",
    "        iav = (ds - ds_forced).rolling(time=11, center=True).var()\n",
    "        \n",
    "        iav.to_netcdf(out_str + ensemble + '_' + model + '_deg' + str(deg) + '.nc')\n",
    "        \n",
    "        # #### NOTE: these time slices need to be the same as for the UC map plot!\n",
    "        # iav_early = (ds - ds_forced).sel(time=slice(2020,2039)).var(dim='time').assign_coords(time = 'early')\n",
    "        # iav_mid = (ds - ds_forced).sel(time=slice(2050,2069)).var(dim='time').assign_coords(time = 'mid')\n",
    "        # iav_late = (ds - ds_forced).sel(time=slice(2080,2099)).var(dim='time').assign_coords(time = 'late')\n",
    "        # iav = xr.concat([iav_early, iav_mid, iav_late], dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6bc00adc-483b-446c-be5d-6ac0914f745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_delayed_list_iav(metric, submetric, submetric_var, poly_path, deg, const_iav, iav_path):\n",
    "    \"\"\"\n",
    "    Make a delayed list with IAV of all models-ssps-ensembles which \n",
    "    can then be combined into one dataset and averaged for best estimate.\n",
    "    \"\"\"\n",
    "    # Parallelize with dask over models\n",
    "    delayed_res = []\n",
    "    \n",
    "    # NEX\n",
    "    for model in nex_models:\n",
    "        tmp_res = dask.delayed(calculate_iav)(nex_in, 'NEX', model, land_mask, metric, submetric, submetric_var, poly_path, deg, const_iav, iav_path)\n",
    "        delayed_res.append(tmp_res)\n",
    "        \n",
    "    # CIL\n",
    "    for model in cil_models:\n",
    "        tmp_res = dask.delayed(calculate_iav)(cil_in, 'CIL', model, land_mask, metric, submetric, submetric_var, poly_path, deg, const_iav, iav_path)\n",
    "        delayed_res.append(tmp_res)\n",
    "        \n",
    "    # ISIMIP\n",
    "    for model in isi_models:\n",
    "        tmp_res = dask.delayed(calculate_iav)(isi_in, 'ISIMIP', model, land_mask, metric, submetric, submetric_var, poly_path, deg, const_iav, iav_path)\n",
    "        delayed_res.append(tmp_res)\n",
    "        \n",
    "    # carbonplan GARD-SV\n",
    "    if metric in ['wet', 'dry', 'hotdry']:\n",
    "        models = cbp_gard_precip_models\n",
    "    else:\n",
    "        models = cbp_gard_models\n",
    "    for model in models:\n",
    "        tmp_res = dask.delayed(calculate_iav)(cbp_in + '/regridded/conservative/GARD-SV/', 'GARD-SV', model, land_mask, metric, submetric, submetric_var, poly_path, deg, const_iav, iav_path)\n",
    "        delayed_res.append(tmp_res)\n",
    "        \n",
    "    # carbonplan DeepSD-BC\n",
    "    for model in cbp_deep_models:\n",
    "        tmp_res = dask.delayed(calculate_iav)(cbp_in + 'native_grid/DeepSD-BC/', 'DeepSD-BC', model, land_mask, metric, submetric, submetric_var, poly_path, deg, const_iav, iav_path)\n",
    "        delayed_res.append(tmp_res)\n",
    "        \n",
    "    # return\n",
    "    return delayed_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0408c55b-f1b3-4839-8974-78be95a15924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_iav(metric, submetric, submetric_var, poly_path, deg, const_iav, iav_path, save_str):\n",
    "    \"\"\"\n",
    "    Calculate the internal variability uncertainty\n",
    "    for a given selection of parameters/settings\n",
    "    \"\"\"\n",
    "    # Check if already done \n",
    "    if os.path.isfile(out_path + 'uncertainty_partitioning/' + metric + '_' + save_str + '.nc'):\n",
    "        return None\n",
    "    \n",
    "    # Make delayed list\n",
    "    delayed_res = make_delayed_list_iav(metric = metric, \n",
    "                                        submetric = submetric,\n",
    "                                        submetric_var = submetric_var,\n",
    "                                        poly_path = poly_path,\n",
    "                                        deg = deg,\n",
    "                                        const_iav = const_iav,\n",
    "                                        iav_path = iav_path)\n",
    "    # Compute\n",
    "    res = dask.compute(*delayed_res)\n",
    "\n",
    "    if const_iav:\n",
    "        # Merge and average over ensemble + model (ensmod) and ssp\n",
    "        ds_out = xr.concat(res, dim='ensmod').mean(dim=['ensmod', 'ssp'])\n",
    "        ds_out.to_netcdf(out_path + 'uncertainty_partitioning/' + metric + '_' + save_str + '.nc')\n",
    "    else:\n",
    "        # If rolling IAV, each ensmod was saved individually so now read with dask\n",
    "        # and calculate average (otherwise would run out of memory)\n",
    "        out_str = iav_path + metric + '/'\n",
    "        \n",
    "        if submetric:\n",
    "            submetric_str = submetric[0].replace('_count', '').replace('_streak', '')\n",
    "            out_str = out_str + submetric_str + '_'\n",
    "            \n",
    "        with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "            ds_out = xr.open_mfdataset(out_str + '*_deg' + str(deg) + '.nc',\n",
    "                                       combine='nested',\n",
    "                                       concat_dim='ensmod',\n",
    "                                       parallel=True)\n",
    "            \n",
    "        ds_out = ds_out.mean(dim=['ensmod', 'ssp'])\n",
    "        ds_out.to_netcdf(out_path + 'uncertainty_partitioning/' + metric + '_' + save_str + '.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c897ff-ddc7-44df-aa4d-13f65dee67f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b69ab4c-263f-4ed8-ac43-c61a0ba02312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 23s, sys: 3.24 s, total: 1min 26s\n",
      "Wall time: 8min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "metric = 'avg'\n",
    "\n",
    "##############################\n",
    "# Get the forced response\n",
    "##############################\n",
    "for deg in [2,4]:\n",
    "    get_forced_poly(nex_in, nex_models,\n",
    "                    cil_in, cil_models, \n",
    "                    isi_in, isi_models, \n",
    "                    cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                    land_mask,\n",
    "                    metric, False, False,\n",
    "                    poly_path, deg)\n",
    "\n",
    "##############################\n",
    "# Interannual variability\n",
    "##############################\n",
    "for deg in [2,4]:\n",
    "    for const_iav in [True, False]:\n",
    "        calculate_all_iav(metric = metric,\n",
    "                          submetric = False,\n",
    "                          submetric_var = False,\n",
    "                          poly_path = poly_path,\n",
    "                          deg = deg,\n",
    "                          const_iav = const_iav,\n",
    "                          iav_path = iav_path,\n",
    "                          save_str = 'deg' + str(deg) + '_' + ('non' * (not const_iav)) + 'const_iav')\n",
    "\n",
    "#############################\n",
    "# UC on forced response\n",
    "#############################\n",
    "for deg in [2,4]:\n",
    "    calculate_forced_uc(metric = metric,\n",
    "                        submetric = False,\n",
    "                        submetric_var = False, \n",
    "                        poly_path = poly_path,\n",
    "                        deg = deg,\n",
    "                        save_str = 'deg' + str(deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "918e6809-7610-45c9-9dbe-b8f73379da90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min, sys: 7.66 s, total: 3min 7s\n",
      "Wall time: 19min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "metric = 'max'\n",
    "\n",
    "#######################################\n",
    "# Get the forced response (polynomial)\n",
    "#######################################\n",
    "for deg in [2,4]:\n",
    "    get_forced_poly(nex_in, nex_models,\n",
    "                    cil_in, cil_models, \n",
    "                    isi_in, isi_models, \n",
    "                    cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                    land_mask,\n",
    "                    metric, False, False,\n",
    "                    poly_path, deg)\n",
    "\n",
    "####################################\n",
    "# Interannual variability\n",
    "####################################\n",
    "for deg in [2,4]:\n",
    "    for const_iav in [True, False]:\n",
    "        calculate_all_iav(metric = metric,\n",
    "                          submetric = False,\n",
    "                          submetric_var = False,\n",
    "                          poly_path = poly_path,\n",
    "                          deg = deg,\n",
    "                          const_iav = const_iav,\n",
    "                          iav_path = iav_path,\n",
    "                          save_str = 'deg' + str(deg) + '_' + ('non' * (not const_iav)) + 'const_iav')\n",
    "\n",
    "################################\n",
    "# UC on forced response\n",
    "################################\n",
    "for deg in [2,4]:\n",
    "    calculate_forced_uc(metric = metric,\n",
    "                        submetric = False,\n",
    "                        submetric_var = False, \n",
    "                        poly_path = poly_path,\n",
    "                        deg = deg,\n",
    "                        save_str = 'deg' + str(deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "550cb11e-5caf-4227-aa34-8a0eab66b1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 48s, sys: 4.42 s, total: 1min 53s\n",
      "Wall time: 10min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "metric = 'max5d'\n",
    "\n",
    "#######################################\n",
    "# Get the forced response (polynomial)\n",
    "#######################################\n",
    "for deg in [2,4]:\n",
    "    get_forced_poly(nex_in, nex_models,\n",
    "                    cil_in, cil_models, \n",
    "                    isi_in, isi_models, \n",
    "                    cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                    land_mask,\n",
    "                    metric, False, False,\n",
    "                    poly_path, deg)\n",
    "\n",
    "####################################\n",
    "# Interannual variability\n",
    "####################################\n",
    "for deg in [2,4]:\n",
    "    for const_iav in [True, False]:\n",
    "        calculate_all_iav(metric = metric,\n",
    "                          submetric = False,\n",
    "                          submetric_var = False,\n",
    "                          poly_path = poly_path,\n",
    "                          deg = deg,\n",
    "                          const_iav = const_iav,\n",
    "                          iav_path = iav_path,\n",
    "                          save_str = 'deg' + str(deg) + '_' + ('non' * (not const_iav)) + 'const_iav')\n",
    "\n",
    "################################\n",
    "# UC on forced response\n",
    "################################\n",
    "for deg in [2,4]:\n",
    "    calculate_forced_uc(metric = metric,\n",
    "                        submetric = False,\n",
    "                        submetric_var = False, \n",
    "                        poly_path = poly_path,\n",
    "                        deg = deg,\n",
    "                        save_str = 'deg' + str(deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab8cfcf8-68dd-4a28-999d-83bf65aeea73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 ms, sys: 7.93 ms, total: 25.3 ms\n",
      "Wall time: 166 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "metric = 'dry'\n",
    "\n",
    "#######################################\n",
    "# Get the forced response (polynomial)\n",
    "#######################################\n",
    "for deg in [2,4]:\n",
    "    get_forced_poly(nex_in, nex_models,\n",
    "                    cil_in, cil_models, \n",
    "                    isi_in, isi_models, \n",
    "                    cbp_in, cbp_gard_precip_models, cbp_deep_models,\n",
    "                    land_mask,\n",
    "                    metric, False, False,\n",
    "                    poly_path, deg)\n",
    "\n",
    "####################################\n",
    "# Interannual variability\n",
    "####################################\n",
    "for deg in [2,4]:\n",
    "    for const_iav in [True, False]:\n",
    "        calculate_all_iav(metric = metric,\n",
    "                          submetric = False,\n",
    "                          submetric_var = False,\n",
    "                          poly_path = poly_path,\n",
    "                          deg = deg,\n",
    "                          const_iav = const_iav,\n",
    "                          iav_path = iav_path,\n",
    "                          save_str = 'deg' + str(deg) + '_' + ('non' * (not const_iav)) + 'const_iav')\n",
    "\n",
    "################################\n",
    "# UC on forced response\n",
    "################################\n",
    "for deg in [2,4]:\n",
    "    calculate_forced_uc(metric = metric,\n",
    "                        submetric = False,\n",
    "                        submetric_var = False, \n",
    "                        poly_path = poly_path,\n",
    "                        deg = deg,\n",
    "                        save_str = 'deg' + str(deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ab6d358-4c11-4d48-b85b-283f73aed10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 65.3 ms, sys: 31.1 ms, total: 96.4 ms\n",
      "Wall time: 629 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Wet days\n",
    "metric = 'wet'\n",
    "\n",
    "for thresh in ['q99', 'rp10']:\n",
    "    for obs in ['gmfd', 'era5']:\n",
    "        submetric_str = thresh + obs\n",
    "        submetric = ['pr_' + submetric_str + '_count', 'pr_' + submetric_str + '_streak']\n",
    "\n",
    "        #######################################\n",
    "        # Get the forced response (polynomial)\n",
    "        #######################################\n",
    "        for deg in [2,4]:\n",
    "            get_forced_poly(nex_in, nex_models,\n",
    "                            cil_in, cil_models, \n",
    "                            isi_in, isi_models, \n",
    "                            cbp_in, cbp_gard_precip_models, cbp_deep_models,\n",
    "                            land_mask,\n",
    "                            metric, submetric, False,\n",
    "                            poly_path, deg)\n",
    "        \n",
    "        ####################################\n",
    "        # Interannual variability\n",
    "        ####################################\n",
    "        for deg in [2,4]:\n",
    "            for const_iav in [True, False]:\n",
    "                calculate_all_iav(metric = metric,\n",
    "                                  submetric = submetric,\n",
    "                                  submetric_var = False,\n",
    "                                  poly_path = poly_path,\n",
    "                                  deg = deg,\n",
    "                                  const_iav = const_iav,\n",
    "                                  iav_path = iav_path,\n",
    "                                  save_str = submetric_str + '_deg' + str(deg) + '_' + ('non' * (not const_iav)) + 'const_iav')\n",
    "        \n",
    "        ################################\n",
    "        # UC on forced response\n",
    "        ################################\n",
    "        for deg in [2,4]:\n",
    "            calculate_forced_uc(metric = metric,\n",
    "                                submetric = submetric,\n",
    "                                submetric_var = False, \n",
    "                                poly_path = poly_path,\n",
    "                                deg = deg,\n",
    "                                save_str = submetric_str + '_deg' + str(deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b0e6718e-9c58-4d40-b842-a95c075f6008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 2.98 s, total: 1min 9s\n",
      "Wall time: 6min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hot days\n",
    "metric = 'hot'\n",
    "\n",
    "# Subselection of submetrics to analyze\n",
    "submetric_strs = ['tas_q99gmfd', 'tasmax_q99gmfd', 'tasmin_q99gmfd',\n",
    "                  'tas_q99era5', 'tasmax_q99era5', 'tasmin_q99era5',\n",
    "                  'tas_rp10gmfd', 'tasmax_rp10gmfd', 'tasmin_rp10gmfd']\n",
    "\n",
    "# for thresh in ['q99', 'rp10']:\n",
    "#     for obs in ['gmfd', 'era5']:\n",
    "#         for submetric_var in ['tas', 'tasmin', 'tasmax']:\n",
    "\n",
    "for submetric_str in submetric_strs:\n",
    "    if True:\n",
    "        if True:\n",
    "            # submetric_str = submetric_var + '_' + thresh + obs\n",
    "            submetric_var = submetric_str.split('_')[0]\n",
    "            submetric = [submetric_str + '_count', submetric_str + '_streak']\n",
    "            \n",
    "            #######################################\n",
    "            # Get the forced response (polynomial)\n",
    "            #######################################\n",
    "            for deg in [2,4]:\n",
    "                get_forced_poly(nex_in, nex_models,\n",
    "                                cil_in, cil_models, \n",
    "                                isi_in, isi_models, \n",
    "                                cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                                land_mask,\n",
    "                                metric, submetric, submetric_var,\n",
    "                                poly_path, deg)\n",
    "            \n",
    "            ####################################\n",
    "            # Interannual variability\n",
    "            ####################################\n",
    "            for deg in [2,4]:\n",
    "                for const_iav in [True, False]:\n",
    "                    calculate_all_iav(metric = metric,\n",
    "                                      submetric = submetric,\n",
    "                                      submetric_var = submetric_var,\n",
    "                                      poly_path = poly_path,\n",
    "                                      deg = deg,\n",
    "                                      const_iav = const_iav,\n",
    "                                      iav_path = iav_path,\n",
    "                                      save_str = submetric_str + '_deg' + str(deg) + '_' + ('non' * (not const_iav)) + 'const_iav')\n",
    "            \n",
    "            ################################\n",
    "            # UC on forced response\n",
    "            ################################\n",
    "            for deg in [2,4]:\n",
    "                calculate_forced_uc(metric = metric,\n",
    "                                    submetric = submetric,\n",
    "                                    submetric_var = submetric_var, \n",
    "                                    poly_path = poly_path,\n",
    "                                    deg = deg,\n",
    "                                    save_str = submetric_str + '_deg' + str(deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f141ef95-11f8-4cee-af38-ff90cba490e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.14 s, sys: 83.8 ms, total: 1.23 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hot + dry days\n",
    "metric = 'hotdry'\n",
    "\n",
    "for thresh in ['q99', 'rp10']:\n",
    "    for obs in ['gmfd', 'era5']:\n",
    "        submetric_str = thresh + obs\n",
    "        submetric = ['hotdry_' + submetric_str + '_count', 'hotdry_' + submetric_str + '_streak']\n",
    "\n",
    "        #######################################\n",
    "        # Get the forced response (polynomial)\n",
    "        #######################################\n",
    "        for deg in [2,4]:\n",
    "            get_forced_poly(nex_in, nex_models,\n",
    "                            cil_in, cil_models, \n",
    "                            isi_in, isi_models, \n",
    "                            cbp_in, cbp_gard_precip_models, cbp_deep_models,\n",
    "                            land_mask,\n",
    "                            metric, submetric, False,\n",
    "                            poly_path, deg)\n",
    "        \n",
    "        ####################################\n",
    "        # Interannual variability\n",
    "        ####################################\n",
    "        for deg in [2,4]:\n",
    "            for const_iav in [True, False]:\n",
    "                calculate_all_iav(metric = metric,\n",
    "                                  submetric = submetric,\n",
    "                                  submetric_var = False,\n",
    "                                  poly_path = poly_path,\n",
    "                                  deg = deg,\n",
    "                                  const_iav = const_iav,\n",
    "                                  iav_path = iav_path, \n",
    "                                  save_str = submetric_str + '_deg' + str(deg) + '_' + ('non' * (not const_iav)) + 'const_iav')\n",
    "        \n",
    "        ################################\n",
    "        # UC on forced response\n",
    "        ################################\n",
    "        for deg in [2,4]:\n",
    "            calculate_forced_uc(metric = metric,\n",
    "                                submetric = submetric,\n",
    "                                submetric_var = False, \n",
    "                                poly_path = poly_path,\n",
    "                                deg = deg,\n",
    "                                save_str = submetric_str + '_deg' + str(deg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1928dd-e0c7-47c1-b684-ecf078ccdd7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Uncertainty partitioning: regional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82a90e05-bd9b-4e66-a7e2-6d29e0fafb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Set paths\n",
    "# UPDATE THIS FOR REPRODUCTION\n",
    "###############################\n",
    "nex_in = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/nex-gddp/'\n",
    "cil_in = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/cil-gdpcir/'\n",
    "isi_in = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/isimip3b/native_grid/'\n",
    "cbp_in = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/carbonplan/'\n",
    "\n",
    "out_path = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/results/'\n",
    "poly_path = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/forced_response/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08f056a-213d-406f-8bae-781a8c3f48ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Total uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f652a7-af82-4340-90ab-661d9802a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uc_total(nex_in, nex_models,\n",
    "             cil_in, cil_models, \n",
    "             isi_in, isi_models, \n",
    "             cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "             metric, submetric):\n",
    "    \"\"\"\n",
    "    Reads in all models, ssps, and calculates the uncertainty in the 'forced response'\n",
    "    along each dimension (ssp, model, ens) for a given year.\n",
    "    \"\"\"\n",
    "\n",
    "    ######################\n",
    "    # Read all ensembles\n",
    "    ######################\n",
    "    # NEX-GDDP \n",
    "    df_out = []\n",
    "    for model in nex_models:\n",
    "        df_tmp = pd.read_csv(nex_in + metric + '/' + model + '_' + submetric + '.csv')\n",
    "        df_tmp['ensemble'] = 'NEX'\n",
    "        df_tmp['model'] = model\n",
    "        df_out.append(df_tmp)\n",
    "    df_nex = pd.concat(df_out)\n",
    "    \n",
    "    # CIL-GDPCIR\n",
    "    df_out = []\n",
    "    for model in cil_models:\n",
    "        df_tmp = pd.read_csv(cil_in + metric + '/' + model + '_' + submetric + '.csv')\n",
    "        df_tmp['ensemble'] = 'CIL'\n",
    "        df_tmp['model'] = model\n",
    "        df_out.append(df_tmp)\n",
    "    df_cil = pd.concat(df_out)\n",
    "    \n",
    "    # ISIMIP\n",
    "    df_out = []\n",
    "    for model in isi_models:\n",
    "        df_tmp = pd.read_csv(isi_in + metric + '/' + model + '_' + submetric + '.csv')\n",
    "        df_tmp['ensemble'] = 'ISIMIP'\n",
    "        df_tmp['model'] = model\n",
    "        df_out.append(df_tmp)\n",
    "    df_isi = pd.concat(df_out)\n",
    "    \n",
    "    # carbonplan: GARD-SV\n",
    "    df_out = []\n",
    "    for model in cbp_gard_models:\n",
    "        df_tmp = pd.read_csv(cbp_in + 'native_grid/GARD-SV/' + metric + '/' + model + '_' + submetric + '.csv')\n",
    "        df_tmp['ensemble'] = 'GARD-SV'\n",
    "        df_tmp['model'] = model\n",
    "        df_out.append(df_tmp)\n",
    "    df_cbp_gard = pd.concat(df_out)\n",
    "    \n",
    "    # carbonplan: DeepSD-BC\n",
    "    df_out = []\n",
    "    for model in cbp_deep_models:\n",
    "        df_tmp = pd.read_csv(cbp_in + 'native_grid/DeepSD-BC/' + metric + '/' + model + '_' + submetric + '.csv')\n",
    "        df_tmp['ensemble'] = 'DeepSD-BC'\n",
    "        df_tmp['model'] = model\n",
    "        df_out.append(df_tmp)\n",
    "    df_cbp_deep = pd.concat(df_out)\n",
    "\n",
    "    ###############\n",
    "    # Merge all\n",
    "    ###############\n",
    "    df = pd.concat([df_nex, df_cil, df_isi, df_cbp_gard, df_cbp_deep])\n",
    "    df = df[df.year < 2100] # drop 2100 (carbonplan models missing)\n",
    "    \n",
    "    ##########################\n",
    "    # Uncertainty calculation\n",
    "    ##########################\n",
    "    ##  Total uncertainty\n",
    "    # Variance across models, scenarios, and ensembles\n",
    "    U_total = df.groupby(['region', 'year']).var(numeric_only = True)\n",
    "    U_total['uncertainty'] = 'total_true'\n",
    "\n",
    "    return U_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c7929a6-4bf0-4a8b-8d6c-6a3eb72da165",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'hot_spatial'\n",
    "submetric = 'tasmax'\n",
    "\n",
    "df_out = uc_total(nex_in, nex_models,\n",
    "              cil_in, cil_models, \n",
    "              isi_in, isi_models, \n",
    "              cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "              metric, submetric)\n",
    "df_out.to_csv(out_path + 'total_uncertainty/' + metric + '_' + submetric + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdaaceb-4923-405b-9d80-21707f94a8d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Extracting the forced response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "405dc7fe-7ccd-4f94-9456-daf88d576cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forced_poly(nex_in, nex_models,\n",
    "                    cil_in, cil_models, \n",
    "                    isi_in, isi_models, \n",
    "                    cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                    metric, submetric,\n",
    "                    poly_path,\n",
    "                    deg):\n",
    "    \"\"\"\n",
    "    Reads in all models, ssps, and calculates the 'forced response' as a deg-th order\n",
    "    polynomial. This function only works for 'hot_spatial' metric. \n",
    "    \"\"\"\n",
    "    # Subfunction to get polynomial for each model/ensemble\n",
    "    def get_poly_coeffs(ensemble, path_in, model, metric, submetric, deg):\n",
    "        # Read CSV\n",
    "        df_in = pd.read_csv(path_in + metric + '/' + model + '_' + submetric + '.csv')\n",
    "         \n",
    "        df_out = []\n",
    "        \n",
    "        # Loop through regions\n",
    "        for region in df_in.region.unique():\n",
    "            df_ssp_tmp = []\n",
    "            \n",
    "            # Loop through SSPs\n",
    "            for ssp in df_in.ssp.unique():\n",
    "                df_varid_tmp = []\n",
    "                \n",
    "                # Year index identical for all regions, ssps\n",
    "                years = df_in[(df_in.ssp == ssp) & (df_in.region == region)]['year']\n",
    "                \n",
    "                # Loop through metrics\n",
    "                for var_id in df_in.columns.drop(['region', 'ssp', 'year']):\n",
    "                    # Fit polynomial\n",
    "                    z = np.polyfit(years,\n",
    "                                   df_in[(df_in.ssp == ssp) & (df_in.region == region)][var_id],\n",
    "                                   deg)\n",
    "                    p = np.poly1d(z)(years)\n",
    "                    # Set negative values to zero\n",
    "                    p[p < 0] = 0\n",
    "                    # Append each metric\n",
    "                    df_varid_tmp.append(pd.DataFrame(data = {'year':years, 'region':region, 'ssp':ssp, var_id:p}))\n",
    "                    \n",
    "                # Merge metrics\n",
    "                df_ssp_tmp.append(functools.reduce(lambda a, b: pd.merge(a,b, on =['region', 'ssp', 'year']), df_varid_tmp))\n",
    "                \n",
    "            # Concatenate SSPs\n",
    "            df_out.append(pd.concat(df_ssp_tmp))\n",
    "            \n",
    "        # Concatenate regions\n",
    "        df_out = pd.concat(df_out)\n",
    "        \n",
    "        # Store\n",
    "        df_out.to_csv(poly_path + metric + '/' + ensemble + '_' + model + '_' + submetric + '_deg' + str(deg) + '.csv', index=False)\n",
    "    \n",
    "    #######################\n",
    "    # Apply to all ensembles\n",
    "    #######################\n",
    "    # Dask delayed\n",
    "    res = []\n",
    "    \n",
    "    # NEX-GDDP \n",
    "    for model in nex_models:\n",
    "        res.append(dask.delayed(get_poly_coeffs)('NEX', nex_in, model, metric, submetric, deg))\n",
    "\n",
    "    # CIL-GDPCIR\n",
    "    for model in cil_models:\n",
    "        res.append(dask.delayed(get_poly_coeffs)('CIL', cil_in, model, metric, submetric, deg))\n",
    "\n",
    "    # ISIMIP\n",
    "    for model in isi_models:\n",
    "        res.append(dask.delayed(get_poly_coeffs)('ISIMIP', isi_in, model, metric, submetric, deg))\n",
    "\n",
    "    # carbonplan: GARD-SV\n",
    "    for model in cbp_gard_models:\n",
    "        res.append(dask.delayed(get_poly_coeffs)('GARD-SV', cbp_in + 'native_grid/GARD-SV/', model, metric, submetric, deg))\n",
    "    \n",
    "    # carbonplan: DeepSD-BC\n",
    "    for model in cbp_deep_models:\n",
    "        res.append(dask.delayed(get_poly_coeffs)('DeepSD-BC', cbp_in + 'native_grid/DeepSD-BC/', model, metric, submetric, deg))\n",
    "    \n",
    "    # Compute\n",
    "    dask.compute(*res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f50e17d2-2a77-4411-bc98-a0fe6c9edbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute\n",
    "metric = 'hot_spatial'\n",
    "submetric = 'tasmax'\n",
    "\n",
    "for deg in [2,4]:\n",
    "    get_forced_poly(nex_in, nex_models,\n",
    "                    cil_in, cil_models, \n",
    "                    isi_in, isi_models, \n",
    "                    cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                    metric, submetric,\n",
    "                    poly_path,\n",
    "                    deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4020dea7-ed23-4ab7-b141-c10cc63d2926",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Uncertainty characterization of forced response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22cf4a17-acf5-46f3-855b-49765eaee703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uc_forced(nex_in, nex_models,\n",
    "              cil_in, cil_models, \n",
    "              isi_in, isi_models, \n",
    "              cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "              metric, submetric,\n",
    "              poly_path, deg):\n",
    "    \"\"\"\n",
    "    Reads in all models, ssps, and calculates the uncertainty in the 'forced response'\n",
    "    along each dimension (ssp, model, ens) for a given year.\n",
    "    \"\"\"\n",
    "\n",
    "    ######################\n",
    "    # Read all ensembles\n",
    "    ######################\n",
    "    # NEX-GDDP \n",
    "    df_out = []\n",
    "    for model in nex_models:\n",
    "        df_tmp = pd.read_csv(poly_path + metric + '/NEX_' + model + '_' + submetric + '_deg' + str(deg) + '.csv')\n",
    "        df_tmp['ensemble'] = 'NEX'\n",
    "        df_tmp['model'] = model\n",
    "        df_out.append(df_tmp)\n",
    "    df_nex = pd.concat(df_out)\n",
    "    \n",
    "    # CIL-GDPCIR\n",
    "    df_out = []\n",
    "    for model in cil_models:\n",
    "        df_tmp = pd.read_csv(poly_path + metric + '/CIL_' + model + '_' + submetric + '_deg' + str(deg) + '.csv')\n",
    "        df_tmp['ensemble'] = 'CIL'\n",
    "        df_tmp['model'] = model\n",
    "        df_out.append(df_tmp)\n",
    "    df_cil = pd.concat(df_out)\n",
    "    \n",
    "    # ISIMIP\n",
    "    df_out = []\n",
    "    if len(isi_models) > 0:\n",
    "        for model in isi_models:\n",
    "            df_tmp = pd.read_csv(poly_path + metric + '/ISIMIP_' + model + '_' + submetric + '_deg' + str(deg) + '.csv')\n",
    "            df_tmp['ensemble'] = 'ISIMIP'\n",
    "            df_tmp['model'] = model\n",
    "            df_out.append(df_tmp)\n",
    "        df_isi = pd.concat(df_out)\n",
    "    \n",
    "    # carbonplan: GARD-SV\n",
    "    df_out = []\n",
    "    for model in cbp_gard_models:\n",
    "        df_tmp = pd.read_csv(poly_path + metric + '/GARD-SV_' + model + '_' + submetric + '_deg' + str(deg) + '.csv')\n",
    "        df_tmp['ensemble'] = 'GARD-SV'\n",
    "        df_tmp['model'] = model\n",
    "        df_out.append(df_tmp)\n",
    "    df_cbp_gard = pd.concat(df_out)\n",
    "    \n",
    "    # carbonplan: DeepSD-BC\n",
    "    df_out = []\n",
    "    for model in cbp_deep_models:\n",
    "        df_tmp = pd.read_csv(poly_path + metric + '/DeepSD-BC_' + model + '_' + submetric + '_deg' + str(deg) + '.csv')\n",
    "        df_tmp['ensemble'] = 'DeepSD-BC'\n",
    "        df_tmp['model'] = model\n",
    "        df_out.append(df_tmp)\n",
    "    df_cbp_deep = pd.concat(df_out)\n",
    "\n",
    "    ###############\n",
    "    # Merge all\n",
    "    ###############\n",
    "    df = pd.concat([df_nex, df_cil, df_cbp_gard, df_cbp_deep]) #df_isi\n",
    "    df = df[df.year < 2100] # drop 2100 (carbonplan models missing)\n",
    "    \n",
    "    ##########################\n",
    "    # Uncertainty calculation\n",
    "    ##########################\n",
    "    ##  Model uncertainty\n",
    "    # Variance across models, averaged over scenarios and ensembles\n",
    "    U_model = df.groupby(['region', 'year', 'ssp', 'ensemble']).var(numeric_only = True)\n",
    "    weights = df.groupby(['region', 'year', 'ssp', 'ensemble']).count().drop(columns='model') # weights\n",
    "    \n",
    "    norm = weights.groupby(['region', 'year']).sum().iloc[0][0] # normalization constant same for all years, regions\n",
    "    U_model = (U_model * weights / norm).groupby(['region','year']).sum() # manually perform weighted average\n",
    "    \n",
    "    ## Scenario uncertainty\n",
    "    # HS09 approach: variance across multi-model means\n",
    "    U_scen_hs09 = df.groupby(['region', 'year', 'ssp']).mean(numeric_only = True).groupby(['region', 'year']).var()\n",
    "    # BB13 approach: variance across scenarios, averaged over models and ensembles (no weighting)\n",
    "    U_scen_bb13 = df.groupby(['region', 'year', 'model', 'ensemble']).var(numeric_only = True).groupby(['region', 'year']).mean()\n",
    "    \n",
    "    ## Downscaling uncertainy\n",
    "    # Variance across ensembles, averaged over models and scenarios\n",
    "    U_ens = df.groupby(['region', 'year', 'ssp', 'model']).var(numeric_only = True)\n",
    "    weights = df.groupby(['region', 'year', 'ssp', 'model']).count().drop(columns='ensemble') # weights\n",
    "    \n",
    "    norm = weights.groupby(['region', 'year']).sum().iloc[0][0] # normalization constant same for all years, regions\n",
    "    U_ens = (U_ens * weights / norm).groupby(['region','year']).sum() # manually perform weighted average\n",
    "    \n",
    "    ## Merge and return\n",
    "    U_model['uncertainty'] = 'model'\n",
    "    U_scen_bb13['uncertainty'] = 'scenario_bb13'\n",
    "    U_scen_hs09['uncertainty'] = 'scenario_hs09'\n",
    "    U_ens['uncertainty'] = 'ensemble'\n",
    "    \n",
    "    U_out = pd.concat([U_model, U_scen_hs09, U_scen_bb13, U_ens]).reset_index().set_index(['region', 'uncertainty', 'year'])\n",
    "    \n",
    "    return U_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1769cb4-d1fd-480e-adf0-b5d76e6841f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute\n",
    "metric = 'hot_spatial'\n",
    "submetric = 'tasmax'\n",
    "\n",
    "for deg in [2,4]:\n",
    "    df_out = uc_forced(nex_in, nex_models,\n",
    "                       cil_in, cil_models, \n",
    "                       isi_in, isi_models, \n",
    "                       cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                       metric, submetric,\n",
    "                       poly_path, deg)\n",
    "    df_out.to_csv(out_path + 'uncertainty_partitioning/' + metric + '_' + submetric + '_deg' + str(deg) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52642c3d-083e-4c3f-b630-2f25cfc51c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute\n",
    "metric = 'hot_spatial'\n",
    "submetric = 'tasmax'\n",
    "\n",
    "for deg in [2,4]:\n",
    "    df_out = uc_forced(nex_in, nex_models,\n",
    "                       cil_in, cil_models, \n",
    "                       isi_in, [], \n",
    "                       cbp_in, cbp_gard_models, cbp_deep_models,\n",
    "                       metric, submetric,\n",
    "                       poly_path, deg)\n",
    "    df_out.to_csv(out_path + 'uncertainty_partitioning/' + metric + '_' + submetric + '_deg' + str(deg) + '_noISIMIP.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecef95f-f597-48b0-a661-53505a75ced9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Measure of interannual variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4fa99e9-31d2-44c0-a41f-66b3f3b4d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iav(metric, submetric, poly_path, deg, const_iav, include_isi):\n",
    "    \"\"\"\n",
    "    Calculates the internal variability (variance over all years\n",
    "    of residuals from forced response) for a given model-ssp-ensemble\n",
    "    \"\"\"\n",
    "    def read_and_process(df_raw, df_forced, ensemble, model):\n",
    "        df_raw = df_raw.set_index(['region', 'year', 'ssp'])\n",
    "        df_forced = df_forced.set_index(['region', 'year', 'ssp'])\n",
    "\n",
    "        # Get residuals \n",
    "        df_resid = df_raw - df_forced\n",
    "        df_resid['ensemble'] = ensemble\n",
    "        df_resid['model'] = model\n",
    "        return(df_resid.reset_index())\n",
    "    \n",
    "    ######################\n",
    "    # Read all ensembles\n",
    "    ######################\n",
    "    # NEX-GDDP \n",
    "    df_out = []\n",
    "    for model in nex_models:\n",
    "        # Read raw and forced\n",
    "        df_raw = pd.read_csv(nex_in + metric + '/' + model + '_' + submetric + '.csv')\n",
    "        df_forced = pd.read_csv(poly_path + metric + '/NEX_' + model + '_' + submetric + '_deg' + str(deg) + '.csv')\n",
    "        \n",
    "        df_out.append(read_and_process(df_raw, df_forced, 'NEX', model))\n",
    "    df_nex = pd.concat(df_out)\n",
    "    \n",
    "    # CIL-GDPCIR\n",
    "    df_out = []\n",
    "    for model in cil_models:\n",
    "        # Read raw and forced\n",
    "        df_raw = pd.read_csv(cil_in + metric + '/' + model + '_' + submetric + '.csv')\n",
    "        df_forced = pd.read_csv(poly_path + metric + '/CIL_' + model + '_' + submetric + '_deg' + str(deg) + '.csv')\n",
    "        \n",
    "        df_out.append(read_and_process(df_raw, df_forced, 'CIL', model))\n",
    "    df_cil = pd.concat(df_out)\n",
    "    \n",
    "    # ISIMIP\n",
    "    df_out = []\n",
    "    if include_isi:\n",
    "        for model in isi_models:\n",
    "            # Read raw and forced\n",
    "            df_raw = pd.read_csv(isi_in + metric + '/' + model + '_' + submetric + '.csv')\n",
    "            df_forced = pd.read_csv(poly_path + metric + '/ISIMIP_' + model + '_' + submetric + '_deg' + str(deg) + '.csv')\n",
    "        \n",
    "            df_out.append(read_and_process(df_raw, df_forced, 'ISIMIP', model))\n",
    "        df_isi = pd.concat(df_out)\n",
    "    \n",
    "    # carbonplan: GARD-SV\n",
    "    df_out = []\n",
    "    for model in cbp_gard_models:\n",
    "        # Read raw and forced\n",
    "        df_raw = pd.read_csv(cbp_in + 'native_grid/GARD-SV/' + metric + '/' + model + '_' + submetric + '.csv')\n",
    "        df_forced = pd.read_csv(poly_path + metric + '/GARD-SV_' + model + '_' + submetric + '_deg' + str(deg) + '.csv')\n",
    "        \n",
    "        df_out.append(read_and_process(df_raw, df_forced, 'GARD-SV', model))\n",
    "    df_cbp_gard = pd.concat(df_out)\n",
    "    \n",
    "    # carbonplan: DeepSD-BC\n",
    "    df_out = []\n",
    "    for model in cbp_deep_models:\n",
    "        # Read raw and forced\n",
    "        df_raw = pd.read_csv(cbp_in + 'native_grid/DeepSD-BC/' + metric + '/' + model + '_' + submetric + '.csv')\n",
    "        df_forced = pd.read_csv(poly_path + metric + '/DeepSD-BC_' + model + '_' + submetric + '_deg' + str(deg) + '.csv')\n",
    "        \n",
    "        df_out.append(read_and_process(df_raw, df_forced, 'DeepSD-BC', model))\n",
    "    df_cbp_deep = pd.concat(df_out)\n",
    "\n",
    "    ###############\n",
    "    # Merge all\n",
    "    ###############\n",
    "    df = pd.concat([df_nex, df_cil, df_cbp_gard, df_cbp_deep]) # df_isi, \n",
    "    df = df[df.year < 2100] # drop 2100 (carbonplan models missing)\n",
    "    \n",
    "    #####################################\n",
    "    # Get IAV estimate\n",
    "    # Variance of residuals\n",
    "    #####################################\n",
    "    # IAV can be constant value or early/mid/late\n",
    "    if const_iav:\n",
    "        iav = df.groupby('region').var(numeric_only=True).drop(columns='year')\n",
    "    else:\n",
    "        #### NOTE: these time slices need to be the same as for the UC map plot!\n",
    "        iav_early = df.query('year >= 2020 and year <= 2039').groupby('region').var(numeric_only=True).drop(columns='year')\n",
    "        iav_early['time'] = 'early'\n",
    "        \n",
    "        iav_mid = df.query('year >= 2050 and year <= 2069').groupby('region').var(numeric_only=True).drop(columns='year')\n",
    "        iav_early['time'] = 'mid'\n",
    "        \n",
    "        iav_late = df.query('year >= 2080 and year <= 2099').groupby('region').var(numeric_only=True).drop(columns='year')\n",
    "        iav_early['time'] = 'late'\n",
    "        \n",
    "        iav = pd.concat([iav_early, iav_mid, iav_late])\n",
    "        \n",
    "    return iav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0765b7-fd58-4206-96c5-020727342e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute\n",
    "metric = 'hot_spatial'\n",
    "submetric = 'tasmax'\n",
    "\n",
    "for const_iav in [True, False]:\n",
    "    for deg in [2,4]:\n",
    "        df_out = calculate_iav(metric, submetric, poly_path, deg, const_iav, True)\n",
    "        df_out.to_csv(out_path + 'uncertainty_partitioning/' + metric + '_' + submetric + '_deg' + str(deg) + '_' + ('non' * (not const_iav)) + 'const_iav.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30b765d9-965a-4ad3-b928-82382e9bf097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute\n",
    "metric = 'hot_spatial'\n",
    "submetric = 'tasmax'\n",
    "\n",
    "for const_iav in [True, False]:\n",
    "    for deg in [2,4]:\n",
    "        df_out = calculate_iav(metric, submetric, poly_path, deg, const_iav, False)\n",
    "        df_out.to_csv(out_path + 'uncertainty_partitioning/' + metric + '_' + submetric + '_deg' + str(deg) + '_' + ('non' * (not const_iav)) + 'const_iav_noISIMIP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544fa459-8c06-482f-a8cf-f7a923c31372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
