{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "010dc88c-5348-4f20-b016-f5de6aa3121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8527e008-f356-43f0-bd69-e949bbc5afed",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dccce93-c392-4a92-b317-d9a37641eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Set paths\n",
    "# UPDATE THIS FOR REPRODUCTION\n",
    "###############################\n",
    "in_path = '/gpfs/group/kaf26/default/public/NEX-GDDP-CMIP6/models/'\n",
    "out_path = '/gpfs/group/kaf26/default/dcl5300/lafferty-sriver_inprep_tbh_DATA/metrics/nex-gddp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b6d653-0e6d-4d29-9d58-cf4e1f64334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Models\n",
    "###################\n",
    "from utils import nex_ssp_dict\n",
    "\n",
    "models = list(nex_ssp_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13b1446d-4c58-433d-9472-033c67ee4087",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Model details\n",
    "###################\n",
    "model_info = {}\n",
    "for model in models:\n",
    "    tmp = glob(in_path + model + '/ssp126/tasmax/*_2015.nc')\n",
    "    tmp = tmp[0].replace(in_path + model, '').replace('/ssp126/tasmax/tasmax_day_' + model + '_ssp126', '').replace('2015.nc', '')\n",
    "    model_info.update({model: tmp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f418bb08-2ee8-4156-89cb-4f459c5e9d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-8190c25f-5edd-11ed-b860-34e6d79eabe8</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_jobqueue.PBSCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">PBSCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">3d34e795</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-941d185c-d04f-4f53-996a-ade64b74110e</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://10.102.201.228:36971\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.102.201.228:36971' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############\n",
    "# Dask\n",
    "############\n",
    "from dask_jobqueue import PBSCluster\n",
    "cluster = PBSCluster(cores=1, resource_spec = 'pmem=20GB', memory='20GB',\n",
    "                     worker_extra_args= ['#PBS -l feature=rhel7'],\n",
    "                     walltime = '00:30:00')\n",
    "\n",
    "cluster.scale(jobs=30)  # ask for jobs\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c78ddaf-5cf0-4b86-8e49-c7d156f01721",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simple metrics (no historical quantiles required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2226eb6d-aa5d-44ee-b2b5-d2399ff66bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Calculate the metric for a \n",
    "# single model-year, including all SSPs and variables\n",
    "########################################################\n",
    "def model_year_metric(path, model, model_vers, ssps, var_ids, year, metric):\n",
    "    # Function for longest consecutive spell if needed\n",
    "    def n_longest_consecutive(ds, dim='time'):\n",
    "        ds = ds.cumsum(dim=dim) - ds.cumsum(dim=dim).where(ds == 0).ffill(dim=dim).fillna(0)\n",
    "        return ds.max(dim=dim)\n",
    "\n",
    "    # Set up dictionary for all results\n",
    "    ds_all = {}\n",
    "    # Loop through SSPs\n",
    "    for ssp in ssps:\n",
    "        # Temporary list for each SSP\n",
    "        ds_list = []\n",
    "        # Loop through variables\n",
    "        for var in var_ids:\n",
    "            ## Temporary file for each variable\n",
    "            ds_tmp = xr.open_dataset(path + model + '/' + ssp + '/' +\n",
    "                                     var + '/' + var + '_day_' + model + \n",
    "                                     '_' + ssp + model_vers + str(year) + '.nc')\n",
    "            \n",
    "            ## Convert units\n",
    "            # temperature: K -> C\n",
    "            if var == 'tas' and ds_tmp.tas.attrs['units'] == 'K':\n",
    "                ds_tmp['tas'] = ds_tmp['tas'] - 273.15\n",
    "            if var == 'tasmax' and ds_tmp.tasmax.attrs['units'] == 'K':\n",
    "                ds_tmp['tasmax'] = ds_tmp['tasmax'] - 273.15\n",
    "            if var == 'tasmin' and ds_tmp.tasmin.attrs['units'] == 'K':\n",
    "                ds_tmp['tasmin'] = ds_tmp['tasmin'] - 273.15\n",
    "\n",
    "            # precip: kg m-2 s-1 -> mm day-1\n",
    "            if var == 'pr' and ds_tmp.pr.attrs['units'] == 'kg m-2 s-1':\n",
    "                ds_tmp['pr'] = ds_tmp['pr'] * 86400\n",
    "\n",
    "            # Calculate metric\n",
    "            if metric == 'avg':\n",
    "                ds_tmp = ds_tmp.resample(time='1Y').mean()\n",
    "            elif metric == 'max':\n",
    "                ds_tmp = ds_tmp.resample(time='1Y').max()\n",
    "            elif metric == 'dry':\n",
    "                # Dry days\n",
    "                ds_tmp_0 = (ds_tmp == 0.).resample(time='1Y').sum() # 0mm\n",
    "                ds_tmp_1 = (ds_tmp < 1.).resample(time='1Y').sum() # less than 1mm\n",
    "                # Longest sonsecutive dry day streak\n",
    "                ds_tmp_0c = (ds_tmp == 0.).resample(time='1Y').apply(n_longest_consecutive) # 0mm longest consecutive\n",
    "                ds_tmp_1c = (ds_tmp < 1.).resample(time='1Y').apply(n_longest_consecutive) # less than 1mm longest consecutive\n",
    "                # Merge\n",
    "                ds_tmp = xr.merge([ds_tmp_0.rename({'pr':'count_eq_0'}),\n",
    "                                   ds_tmp_0c.rename({'pr':'streak_eq_0'}),\n",
    "                                   ds_tmp_1.rename({'pr':'count_lt_1'}),\n",
    "                                   ds_tmp_1c.rename({'pr':'streak_lt_1'})])\n",
    "                \n",
    "            # Append to list\n",
    "            ds_list.append(ds_tmp)\n",
    "            \n",
    "        # Append to dict\n",
    "        ds_all.update({ssp: ds_list})\n",
    "\n",
    "    # Merge and concat along ssp dimension\n",
    "    for ssp in ssps:\n",
    "        ds_all[ssp] = xr.merge(ds_all[ssp])\n",
    "        ds_all[ssp] = ds_all[ssp].assign_coords(ssp = ssp)\n",
    "    \n",
    "    # Return\n",
    "    ds_out = xr.concat([ds_all[ssp] for ssp in ssps], dim='ssp')\n",
    "    return ds_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f3e158-fad8-4e8b-ac18-1a74aaa26fbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Annual averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14278fa3-3162-4bde-b617-893996a3eb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-ESM1-5 already done\n",
      "BCC-CSM2-MR already done\n",
      "CanESM5 already done\n",
      "CMCC-ESM2 already done\n",
      "CNRM-CM6-1 already done\n",
      "CNRM-ESM2-1 already done\n",
      "EC-Earth3 already done\n",
      "EC-Earth3-Veg-LR already done\n",
      "GFDL-ESM4 already done\n",
      "HadGEM3-GC31-LL already done\n",
      "INM-CM4-8 already done\n",
      "INM-CM5-0 already done\n",
      "IPSL-CM6A-LR already done\n",
      "MIROC-ES2L already done\n",
      "MIROC6 already done\n",
      "MPI-ESM1-2-HR\n",
      "MPI-ESM1-2-LR already done\n",
      "MRI-ESM2-0\n",
      "NESM3\n",
      "NorESM2-LM already done\n",
      "NorESM2-MM already done\n",
      "UKESM1-0-LL already done\n"
     ]
    }
   ],
   "source": [
    "# Loop through models: RUNTIME IS ~15 MINS PER MODEL WITH 30 DASK WORKERS\n",
    "metric = 'avg'\n",
    "\n",
    "# All variables\n",
    "var_ids = ['tas', 'tasmin', 'tasmax', 'pr']\n",
    "\n",
    "for model in models:\n",
    "    # Check if already exists\n",
    "    if os.path.isfile(out_path + metric + '/' + model + '.nc'):\n",
    "        print(model + ' already done')\n",
    "        continue\n",
    "    \n",
    "    # Parallelize with dask over years\n",
    "    delayed_res = []\n",
    "\n",
    "    for year in range(2015, 2101):\n",
    "        tmp_res = dask.delayed(model_year_metric)(path = in_path,\n",
    "                                                  model = model,\n",
    "                                                  model_vers = model_info[model],\n",
    "                                                  ssps = nex_ssp_dict[model],\n",
    "                                                  var_ids = var_ids,\n",
    "                                                  year = year, \n",
    "                                                  metric = metric)\n",
    "        delayed_res.append(tmp_res)\n",
    "            \n",
    "    # Compute\n",
    "    res = dask.compute(*delayed_res)\n",
    "\n",
    "    # Store\n",
    "    df_final = xr.combine_by_coords(res)\n",
    "    df_final.to_netcdf(out_path + metric + '/' + model + '.nc')\n",
    "\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eecd2f0-6a74-46a1-a7cd-25a4afc7956d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Annual maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0793d900-bea0-4afd-bd50-dfc6cc1ec933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-ESM1-5 already done\n",
      "BCC-CSM2-MR already done\n",
      "CanESM5 already done\n",
      "CMCC-ESM2 already done\n",
      "CNRM-CM6-1 already done\n",
      "CNRM-ESM2-1 already done\n",
      "EC-Earth3 already done\n",
      "EC-Earth3-Veg-LR already done\n",
      "GFDL-ESM4 already done\n",
      "HadGEM3-GC31-LL already done\n",
      "INM-CM4-8 already done\n",
      "INM-CM5-0 already done\n",
      "IPSL-CM6A-LR already done\n",
      "MIROC-ES2L already done\n",
      "MIROC6 already done\n",
      "MPI-ESM1-2-HR\n",
      "MPI-ESM1-2-LR already done\n",
      "MRI-ESM2-0\n",
      "NESM3\n",
      "NorESM2-LM already done\n",
      "NorESM2-MM already done\n",
      "UKESM1-0-LL already done\n"
     ]
    }
   ],
   "source": [
    "# Loop through models: RUNTIME IS ~10 MINS PER MODEL WITH 30 DASK WORKERS\n",
    "metric = 'max'\n",
    "\n",
    "# All variables\n",
    "var_ids = ['tas', 'tasmin', 'tasmax', 'pr']\n",
    "\n",
    "for model in models:\n",
    "    # Check if already exists\n",
    "    if os.path.isfile(out_path + metric + '/' + model + '.nc'):\n",
    "        print(model + ' already done')\n",
    "        continue\n",
    "    \n",
    "    # Parallelize with dask over years\n",
    "    delayed_res = []\n",
    "\n",
    "    for year in range(2015, 2101):\n",
    "        tmp_res = dask.delayed(model_year_metric)(path = in_path,\n",
    "                                                  model = model,\n",
    "                                                  model_vers = model_info[model],\n",
    "                                                  ssps = nex_ssp_dict[model],\n",
    "                                                  var_ids = var_ids,\n",
    "                                                  year = year, \n",
    "                                                  metric = metric)\n",
    "        delayed_res.append(tmp_res)\n",
    "            \n",
    "    # Compute\n",
    "    res = dask.compute(*delayed_res)\n",
    "\n",
    "    # Store\n",
    "    df_final = xr.combine_by_coords(res)\n",
    "    df_final.to_netcdf(out_path + metric + '/' + model + '.nc')\n",
    "\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014def66-53d0-4bfa-bbe8-df28be225f49",
   "metadata": {},
   "source": [
    "### Dry days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8af26328-ffa6-4e1a-88d2-1b095e5c37f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "combine_attrs='no_conflicts', but some values are not the same. Merging {'cmip6_source_id': 'ACCESS-ESM1-5', 'cmip6_institution_id': 'CSIRO', 'cmip6_license': 'CC-BY-SA 4.0', 'activity': 'NEX-GDDP-CMIP6', 'contact': 'Dr. Rama Nemani: rama.nemani@nasa.gov, Dr. Bridget Thrasher: bridget@climateanalyticsgroup.org', 'Conventions': 'CF-1.7', 'creation_date': '2022-02-22T11:07:26.220685+00:00', 'frequency': 'day', 'institution': 'NASA Earth Exchange, NASA Ames Research Center, Moffett Field, CA 94035', 'variant_label': 'r1i1p1f1', 'product': 'output', 'realm': 'atmos', 'source': 'BCSD', 'scenario': 'ssp126', 'references': 'BCSD method: Thrasher et al., 2012, Hydrol. Earth Syst. Sci.,16, 3309-3314. Ref period obs: latest version of the Princeton Global Meteorological Forcings (http://hydrology.princeton.edu/data.php), based on Sheffield et al., 2006, J. Climate, 19 (13), 3088-3111.', 'version': '1.0', 'tracking_id': '678c0a3c-7982-420c-a52e-968a47ae3ea9', 'title': 'ACCESS-ESM1-5, r1i1p1f1, ssp126, global downscaled CMIP6 climate projection data', 'resolution_id': '0.25 degree', 'history': '2022-02-22T11:07:26.220685+00:00: install global attributes', 'doi': 'https://doi.org/10.7917/OFSG3345', 'disclaimer': 'This data is considered provisional and subject to change. This data is provided as is without any warranty of any kind, either express or implied, arising by law or otherwise, including but not limited to warranties of completeness, non-infringement, accuracy, merchantability, or fitness for a particular purpose. The user assumes all risk associated with the use of, or inability to use, this data.', 'external_variables': 'areacella'} with {'cmip6_source_id': 'ACCESS-ESM1-5', 'cmip6_institution_id': 'CSIRO', 'cmip6_license': 'CC-BY-SA 4.0', 'activity': 'NEX-GDDP-CMIP6', 'contact': 'Dr. Rama Nemani: rama.nemani@nasa.gov, Dr. Bridget Thrasher: bridget@climateanalyticsgroup.org', 'Conventions': 'CF-1.7', 'creation_date': '2022-02-22T11:07:26.535384+00:00', 'frequency': 'day', 'institution': 'NASA Earth Exchange, NASA Ames Research Center, Moffett Field, CA 94035', 'variant_label': 'r1i1p1f1', 'product': 'output', 'realm': 'atmos', 'source': 'BCSD', 'scenario': 'ssp126', 'references': 'BCSD method: Thrasher et al., 2012, Hydrol. Earth Syst. Sci.,16, 3309-3314. Ref period obs: latest version of the Princeton Global Meteorological Forcings (http://hydrology.princeton.edu/data.php), based on Sheffield et al., 2006, J. Climate, 19 (13), 3088-3111.', 'version': '1.0', 'tracking_id': 'c0d6a1bc-b89a-4cf8-9154-511b15f7a9b6', 'title': 'ACCESS-ESM1-5, r1i1p1f1, ssp126, global downscaled CMIP6 climate projection data', 'resolution_id': '0.25 degree', 'history': '2022-02-22T11:07:26.535384+00:00: install global attributes', 'doi': 'https://doi.org/10.7917/OFSG3345', 'disclaimer': 'This data is considered provisional and subject to change. This data is provided as is without any warranty of any kind, either express or implied, arising by law or otherwise, including but not limited to warranties of completeness, non-infringement, accuracy, merchantability, or fitness for a particular purpose. The user assumes all risk associated with the use of, or inability to use, this data.', 'external_variables': 'areacella'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/xarray/core/merge.py:650\u001b[0m, in \u001b[0;36mmerge_attrs\u001b[0;34m(variable_attrs, combine_attrs, context)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcompat_dict_union\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/xarray/core/utils.py:398\u001b[0m, in \u001b[0;36mcompat_dict_union\u001b[0;34m(first_dict, second_dict, compat)\u001b[0m\n\u001b[1;32m    397\u001b[0m new_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(first_dict)\n\u001b[0;32m--> 398\u001b[0m \u001b[43mupdate_safety_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecond_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m new_dict\u001b[38;5;241m.\u001b[39mupdate(second_dict)\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/xarray/core/utils.py:174\u001b[0m, in \u001b[0;36mupdate_safety_check\u001b[0;34m(first_dict, second_dict, compat)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m first_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compat(v, first_dict[k]):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe to merge dictionaries without \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverriding values; conflicting key \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: unsafe to merge dictionaries without overriding values; conflicting key 'creation_date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m res \u001b[38;5;241m=\u001b[39m dask\u001b[38;5;241m.\u001b[39mcompute(\u001b[38;5;241m*\u001b[39mdelayed_res)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Store\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m df_final \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_by_coords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m df_final\u001b[38;5;241m.\u001b[39mto_netcdf(out_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdry/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m model \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/xarray/core/combine.py:982\u001b[0m, in \u001b[0;36mcombine_by_coords\u001b[0;34m(data_objects, compat, data_vars, coords, fill_value, join, combine_attrs, datasets)\u001b[0m\n\u001b[1;32m    980\u001b[0m     concatenated_grouped_by_data_vars \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mvars\u001b[39m, datasets_with_same_vars \u001b[38;5;129;01min\u001b[39;00m grouped_by_vars:\n\u001b[0;32m--> 982\u001b[0m         concatenated \u001b[38;5;241m=\u001b[39m \u001b[43m_combine_single_variable_hypercube\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatasets_with_same_vars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m         concatenated_grouped_by_data_vars\u001b[38;5;241m.\u001b[39mappend(concatenated)\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[1;32m    994\u001b[0m     concatenated_grouped_by_data_vars,\n\u001b[1;32m    995\u001b[0m     compat\u001b[38;5;241m=\u001b[39mcompat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m     combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs,\n\u001b[1;32m    999\u001b[0m )\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/xarray/core/combine.py:640\u001b[0m, in \u001b[0;36m_combine_single_variable_hypercube\u001b[0;34m(datasets, fill_value, data_vars, coords, compat, join, combine_attrs)\u001b[0m\n\u001b[1;32m    637\u001b[0m     _check_dimension_depth_tile_ids(combined_ids)\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# Concatenate along all of concat_dims one by one to create single ds\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m concatenated \u001b[38;5;241m=\u001b[39m \u001b[43m_combine_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombined_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcat_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;66;03m# Check the overall coordinates are monotonically increasing\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m concat_dims:\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/xarray/core/combine.py:239\u001b[0m, in \u001b[0;36m_combine_nd\u001b[0;34m(combined_ids, concat_dims, data_vars, coords, compat, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Each iteration of this loop reduces the length of the tile_ids tuples\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# by one. It always combines along the first dimension, removing the first\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# element of the tuple\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m concat_dim \u001b[38;5;129;01min\u001b[39;00m concat_dims:\n\u001b[0;32m--> 239\u001b[0m     combined_ids \u001b[38;5;241m=\u001b[39m \u001b[43m_combine_all_along_first_dim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombined_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m (combined_ds,) \u001b[38;5;241m=\u001b[39m combined_ids\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m combined_ds\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/xarray/core/combine.py:275\u001b[0m, in \u001b[0;36m_combine_all_along_first_dim\u001b[0;34m(combined_ids, dim, data_vars, coords, compat, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    273\u001b[0m     combined_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28msorted\u001b[39m(group))\n\u001b[1;32m    274\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m combined_ids\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m--> 275\u001b[0m     new_combined_ids[new_id] \u001b[38;5;241m=\u001b[39m \u001b[43m_combine_1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombine_attrs\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_combined_ids\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/xarray/core/combine.py:298\u001b[0m, in \u001b[0;36m_combine_1d\u001b[0;34m(datasets, concat_dim, compat, data_vars, coords, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concat_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m         combined \u001b[38;5;241m=\u001b[39m \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencountered unexpected variable\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/xarray/core/concat.py:243\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _dataarray_concat(\n\u001b[1;32m    232\u001b[0m         objs,\n\u001b[1;32m    233\u001b[0m         dim\u001b[38;5;241m=\u001b[39mdim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m         combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs,\n\u001b[1;32m    241\u001b[0m     )\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_obj, Dataset):\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dataset_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan only concatenate xarray Dataset and DataArray \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(first_obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m     )\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/xarray/core/concat.py:513\u001b[0m, in \u001b[0;36m_dataset_concat\u001b[0;34m(datasets, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    510\u001b[0m result_vars\u001b[38;5;241m.\u001b[39mupdate(dim_coords)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# assign attrs and encoding from first dataset\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m result_attrs \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_attrs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m result_encoding \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mencoding\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# check that global attributes are fixed across all datasets if necessary\u001b[39;00m\n",
      "File \u001b[0;32m/storage/work/d/dcl5300/ENVS/micromamba/envs/climate-stack-mamba-2022-11/lib/python3.10/site-packages/xarray/core/merge.py:652\u001b[0m, in \u001b[0;36mmerge_attrs\u001b[0;34m(variable_attrs, combine_attrs, context)\u001b[0m\n\u001b[1;32m    650\u001b[0m             result \u001b[38;5;241m=\u001b[39m compat_dict_union(result, attrs)\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 652\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombine_attrs=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_conflicts\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, but some values are not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe same. Merging \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(attrs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    655\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m combine_attrs \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_conflicts\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mMergeError\u001b[0m: combine_attrs='no_conflicts', but some values are not the same. Merging {'cmip6_source_id': 'ACCESS-ESM1-5', 'cmip6_institution_id': 'CSIRO', 'cmip6_license': 'CC-BY-SA 4.0', 'activity': 'NEX-GDDP-CMIP6', 'contact': 'Dr. Rama Nemani: rama.nemani@nasa.gov, Dr. Bridget Thrasher: bridget@climateanalyticsgroup.org', 'Conventions': 'CF-1.7', 'creation_date': '2022-02-22T11:07:26.220685+00:00', 'frequency': 'day', 'institution': 'NASA Earth Exchange, NASA Ames Research Center, Moffett Field, CA 94035', 'variant_label': 'r1i1p1f1', 'product': 'output', 'realm': 'atmos', 'source': 'BCSD', 'scenario': 'ssp126', 'references': 'BCSD method: Thrasher et al., 2012, Hydrol. Earth Syst. Sci.,16, 3309-3314. Ref period obs: latest version of the Princeton Global Meteorological Forcings (http://hydrology.princeton.edu/data.php), based on Sheffield et al., 2006, J. Climate, 19 (13), 3088-3111.', 'version': '1.0', 'tracking_id': '678c0a3c-7982-420c-a52e-968a47ae3ea9', 'title': 'ACCESS-ESM1-5, r1i1p1f1, ssp126, global downscaled CMIP6 climate projection data', 'resolution_id': '0.25 degree', 'history': '2022-02-22T11:07:26.220685+00:00: install global attributes', 'doi': 'https://doi.org/10.7917/OFSG3345', 'disclaimer': 'This data is considered provisional and subject to change. This data is provided as is without any warranty of any kind, either express or implied, arising by law or otherwise, including but not limited to warranties of completeness, non-infringement, accuracy, merchantability, or fitness for a particular purpose. The user assumes all risk associated with the use of, or inability to use, this data.', 'external_variables': 'areacella'} with {'cmip6_source_id': 'ACCESS-ESM1-5', 'cmip6_institution_id': 'CSIRO', 'cmip6_license': 'CC-BY-SA 4.0', 'activity': 'NEX-GDDP-CMIP6', 'contact': 'Dr. Rama Nemani: rama.nemani@nasa.gov, Dr. Bridget Thrasher: bridget@climateanalyticsgroup.org', 'Conventions': 'CF-1.7', 'creation_date': '2022-02-22T11:07:26.535384+00:00', 'frequency': 'day', 'institution': 'NASA Earth Exchange, NASA Ames Research Center, Moffett Field, CA 94035', 'variant_label': 'r1i1p1f1', 'product': 'output', 'realm': 'atmos', 'source': 'BCSD', 'scenario': 'ssp126', 'references': 'BCSD method: Thrasher et al., 2012, Hydrol. Earth Syst. Sci.,16, 3309-3314. Ref period obs: latest version of the Princeton Global Meteorological Forcings (http://hydrology.princeton.edu/data.php), based on Sheffield et al., 2006, J. Climate, 19 (13), 3088-3111.', 'version': '1.0', 'tracking_id': 'c0d6a1bc-b89a-4cf8-9154-511b15f7a9b6', 'title': 'ACCESS-ESM1-5, r1i1p1f1, ssp126, global downscaled CMIP6 climate projection data', 'resolution_id': '0.25 degree', 'history': '2022-02-22T11:07:26.535384+00:00: install global attributes', 'doi': 'https://doi.org/10.7917/OFSG3345', 'disclaimer': 'This data is considered provisional and subject to change. This data is provided as is without any warranty of any kind, either express or implied, arising by law or otherwise, including but not limited to warranties of completeness, non-infringement, accuracy, merchantability, or fitness for a particular purpose. The user assumes all risk associated with the use of, or inability to use, this data.', 'external_variables': 'areacella'}"
     ]
    }
   ],
   "source": [
    "# Loop through models: RUNTIME IS ~12 MINS PER MODEL WITH 30 DASK WORKERS\n",
    "metric = 'dry'\n",
    "\n",
    "# Precip only\n",
    "var_ids = ['pr']\n",
    "\n",
    "for model in models:\n",
    "    # Check if already exists\n",
    "    if os.path.isfile(out_path + metric + '/' + model + '.nc'):\n",
    "        print(model + ' already done')\n",
    "        continue\n",
    "    \n",
    "    # Parallelize with dask over years\n",
    "    delayed_res = []\n",
    "\n",
    "    for year in range(2015, 2101):\n",
    "        tmp_res = dask.delayed(model_year_metric)(path = in_path,\n",
    "                                                  model = model,\n",
    "                                                  model_vers = model_info[model],\n",
    "                                                  ssps = nex_ssp_dict[model],\n",
    "                                                  var_ids = var_ids,\n",
    "                                                  year = year, \n",
    "                                                  metric = metric)\n",
    "        delayed_res.append(tmp_res)\n",
    "            \n",
    "    # Compute\n",
    "    res = dask.compute(*delayed_res)\n",
    "\n",
    "    # Store\n",
    "    df_final = xr.concat(res, dim='time')\n",
    "    df_final.to_netcdf(out_path + metric + '/' + model + '.nc')\n",
    "\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850701a9-65d8-4b14-9cce-2ffe068a41f2",
   "metadata": {},
   "source": [
    "## Less simple metrics (historical quantiles required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5dd2e1-c8fc-4254-bf71-68e3a7ede76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Calculate the metric for a \n",
    "# single model-year, including all SSPs and variables\n",
    "########################################################\n",
    "def model_year_metric(path, model, model_vers, ssps, var_ids, year, metric):\n",
    "    # Function for longest consecutive spell if needed\n",
    "    def n_longest_consecutive(ds, dim='time'):\n",
    "        ds = ds.cumsum(dim=dim) - ds.cumsum(dim=dim).where(ds == 0).ffill(dim=dim).fillna(0)\n",
    "        return ds.max(dim=dim)\n",
    "    \n",
    "    # Read historical quantiles\n",
    "    if 'tas' in var_ids:\n",
    "        ds_q_era5 = xr.open_dataset('../data/ta/era5_temperature_quantiles.nc')\n",
    "        ds_q_gmfd = xr.open_dataset('../data/gmfd_temperature_quantiles.nc')\n",
    "    elif 'pr' in var_ids:\n",
    "        ds_q_era5 = xr.open_dataset('../data/era5_precip_quantiles.nc')\n",
    "        ds_q_gmfd = xr.open_dataset('../data/gmfd_precip_quantiles.nc')\n",
    "\n",
    "    # Set up dictionary for all results\n",
    "    ds_all = {}\n",
    "    # Loop through SSPs\n",
    "    for ssp in ssps:\n",
    "        # Temporary list for each SSP\n",
    "        ds_list = []\n",
    "        # Loop through variables\n",
    "        for var in var_ids:\n",
    "            ## Temporary file for each variable\n",
    "            ds_tmp = xr.open_dataset(path + model + '/' + ssp + '/' +\n",
    "                                     var + '/' + var + '_day_' + model + \n",
    "                                     '_' + ssp + model_vers + str(year) + '.nc')\n",
    "            \n",
    "            ## Convert units\n",
    "            # temperature: K -> C\n",
    "            if var == 'tas' and ds_tmp.tas.attrs['units'] == 'K':\n",
    "                ds_tmp['tas'] = ds_tmp['tas'] - 273.15\n",
    "            if var == 'tasmax' and ds_tmp.tasmax.attrs['units'] == 'K':\n",
    "                ds_tmp['tasmax'] = ds_tmp['tasmax'] - 273.15\n",
    "            if var == 'tasmin' and ds_tmp.tasmin.attrs['units'] == 'K':\n",
    "                ds_tmp['tasmin'] = ds_tmp['tasmin'] - 273.15\n",
    "\n",
    "            # precip: kg m-2 s-1 -> mm day-1\n",
    "            if var == 'pr' and ds_tmp.pr.attrs['units'] == 'kg m-2 s-1':\n",
    "                ds_tmp['pr'] = ds_tmp['pr'] * 86400\n",
    "\n",
    "            ## Calculate metric\n",
    "            # Get above/below binary\n",
    "            if metric == 'hot':\n",
    "                ds_tmp_q95_era5 = ds_tmp > ds_q_era5[var_id + '_q95']\n",
    "                ds_tmp_q99_era5 = ds_tmp > ds_q_era5[var_id + '_q99']\n",
    "                ds_tmp_q95_gmfd = ds_tmp > ds_q_gmfd[var_id + '_q95']\n",
    "                ds_tmp_q99_gmfd = ds_tmp > ds_q_gmfd[var_id + '_q99']\n",
    "            elif metric == 'wet':\n",
    "                ds_tmp_q95_era5 = ds_tmp > ds_q_era5[var_id + '_q95_wet']\n",
    "                ds_tmp_q99_era5 = ds_tmp > ds_q_era5[var_id + '_q99_wet']\n",
    "                ds_tmp_q95_gmfd = ds_tmp > ds_q_gmfd[var_id + '_q95_wet']\n",
    "                ds_tmp_q99_gmfd = ds_tmp > ds_q_gmfd[var_id + '_q99_wet']\n",
    "                \n",
    "            # Count of hot/wet days\n",
    "            ds_tmp_q95_era5_count = ds_tmp_q95_era5.resample(time='1Y').count()\n",
    "            ds_tmp_q99_era5_count = ds_tmp_q99_era5.resample(time='1Y').count()\n",
    "            ds_tmp_q95_gmfd_count = ds_tmp_q95_gmfd.resample(time='1Y').count()\n",
    "            ds_tmp_q99_gmfd_count = ds_tmp_q99_gmfd.resample(time='1Y').count()\n",
    "            # Longest consecutive hot/wet day streak\n",
    "            ds_tmp_q95_era5_streak = ds_tmp_q95_era5.resample(time='1Y').apply(n_longest_consecutive)\n",
    "            ds_tmp_q99_era5_streak = ds_tmp_q99_era5.resample(time='1Y').apply(n_longest_consecutive)\n",
    "            ds_tmp_q95_gmfd_streak = ds_tmp_q95_gmfd.resample(time='1Y').apply(n_longest_consecutive)\n",
    "            ds_tmp_q99_gmfd_streak = ds_tmp_q99_gmfd.resample(time='1Y').apply(n_longest_consecutive)\n",
    "            # Merge\n",
    "            ds_tmp = xr.merge([ds_tmp_q95_era5_count.rename({var_id: var_id + '_q95_era5_count'}),\n",
    "                               ds_tmp_q99_era5_count.rename({var_id: var_id + '_q99_era5_count'}),\n",
    "                               ds_tmp_q95_gmfd_count.rename({var_id: var_id + '_q95_gmfd_count'}),\n",
    "                               ds_tmp_q99_gmfd_count.rename({var_id: var_id + '_q99_gmfd_count'}),\n",
    "                               ds_tmp_q95_era5_streak.rename({var_id: var_id + '_q95_era5_streak'}),\n",
    "                               ds_tmp_q99_era5_streak.rename({var_id: var_id + '_q99_era5_streak'}),\n",
    "                               ds_tmp_q95_gmfd_streak.rename({var_id: var_id + '_q95_gmfd_streak'}),\n",
    "                               ds_tmp_q99_gmfd_streak.rename({var_id: var_id + '_q99_gmfd_streak'})])\n",
    "                \n",
    "            # Append to list\n",
    "            ds_list.append(ds_tmp)\n",
    "            \n",
    "        # Append to dict\n",
    "        ds_all.update({ssp: ds_list})\n",
    "\n",
    "    # Merge and concat along ssp dimension\n",
    "    for ssp in ssps:\n",
    "        ds_all[ssp] = xr.merge(ds_all[ssp])\n",
    "        ds_all[ssp] = ds_all[ssp].assign_coords(ssp = ssp)\n",
    "    \n",
    "    # Return\n",
    "    ds_out = xr.concat([ds_all[ssp] for ssp in ssps], dim='ssp')\n",
    "    return ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0903a90e-0f06-4790-b644-8dabeaef04d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Calculate the metric for a \n",
    "# single model-year, including all SSPs and variables\n",
    "########################################################\n",
    "def model_year_metric(path, model, model_vers, ssps, var_ids, year, metric):\n",
    "    # Function for longest consecutive spell if needed\n",
    "    def n_longest_consecutive(ds, dim='time'):\n",
    "        ds = ds.cumsum(dim=dim) - ds.cumsum(dim=dim).where(ds == 0).ffill(dim=dim).fillna(0)\n",
    "        return ds.max(dim=dim)\n",
    "    \n",
    "    # Read historical quantiles\n",
    "    if 'tas' in var_ids:\n",
    "        ds_q_era5 = xr.open_dataset('../data/era5_temperature_quantiles.nc')\n",
    "        ds_q_gmfd = xr.open_dataset('../data/gmfd_temperature_quantiles.nc')\n",
    "    elif 'pr' in var_ids:\n",
    "        ds_q_era5 = xr.open_dataset('../data/era5_precip_quantiles.nc')\n",
    "        ds_q_gmfd = xr.open_dataset('../data/gmfd_precip_quantiles.nc')\n",
    "\n",
    "    # Set up dictionary for all results\n",
    "    ds_all = {}\n",
    "    # Loop through SSPs\n",
    "    for ssp in ssps:\n",
    "        # Temporary list for each SSP\n",
    "        ds_list = []\n",
    "        \n",
    "        # Read file\n",
    "        ds_tmp = xr.open_dataset(path + model + '/' + ssp + '/' +\n",
    "                                 var + '/' + var + '_day_' + model + \n",
    "                                 '_' + ssp + model_vers + str(year) + '.nc')\n",
    "            \n",
    "        ## Convert units\n",
    "        # temperature: K -> C\n",
    "        if var == 'tas' and ds_tmp.tas.attrs['units'] == 'K':\n",
    "            ds_tmp['tas'] = ds_tmp['tas'] - 273.15\n",
    "        if var == 'tasmax' and ds_tmp.tasmax.attrs['units'] == 'K':\n",
    "            ds_tmp['tasmax'] = ds_tmp['tasmax'] - 273.15\n",
    "        if var == 'tasmin' and ds_tmp.tasmin.attrs['units'] == 'K':\n",
    "            ds_tmp['tasmin'] = ds_tmp['tasmin'] - 273.15\n",
    "\n",
    "        # precip: kg m-2 s-1 -> mm day-1\n",
    "        if var == 'pr' and ds_tmp.pr.attrs['units'] == 'kg m-2 s-1':\n",
    "            ds_tmp['pr'] = ds_tmp['pr'] * 86400\n",
    "\n",
    "        # Calculate metric\n",
    "        if metric == 'hot':\n",
    "            ds_tmp_q95 = (ds_tmp > 0.).resample(time='1Y').apply(n_longest_consecutive) # 0mm\n",
    "            ds_tmp_q99 = (ds_tmp < 1.).resample(time='1Y').apply(n_longest_consecutive) # less than 1mm\n",
    "            # merge\n",
    "            ds_tmp = xr.merge([ds_tmp_0.rename({'pr':'consec_eq_0'}),\n",
    "                                ds_tmp_1.rename({'pr':'consec_lt_1'})])\n",
    "                \n",
    "            # Append to list\n",
    "            ds_list.append(ds_tmp)\n",
    "            \n",
    "        # Append to dict\n",
    "        ds_all.update({ssp: ds_list})\n",
    "\n",
    "    # Merge and concat along ssp dimension\n",
    "    for ssp in ssps:\n",
    "        ds_all[ssp] = xr.merge(ds_all[ssp])\n",
    "        ds_all[ssp] = ds_all[ssp].assign_coords(ssp = ssp)\n",
    "    \n",
    "    # Return\n",
    "    ds_out = xr.concat([ds_all[ssp] for ssp in ssps], dim='ssp')\n",
    "    return ds_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
